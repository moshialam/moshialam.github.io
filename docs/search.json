[
  {
    "objectID": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#introduction",
    "href": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#introduction",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Introduction",
    "text": "Introduction\n\nData scaling\nStandardized coefficients\nNonlinearities in X\nInteraction terms\nAverage partial effects\nAdjusted R-squared\nOver fitting"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#data-scaling",
    "href": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#data-scaling",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Data Scaling",
    "text": "Data Scaling\n\nlibrary(wooldridge); library(stargazer);\nmodel1 &lt;- lm(bwght ~ cigs + faminc, bwght) # bwght in grams\nmodel2 &lt;- lm(I(bwght/16) ~ cigs + faminc, bwght) # bwght in pounds\nbwght$packs &lt;- bwght$cigs/20 # 1 pack =  20 cigs\nmodel3 &lt;- lm(bwght ~ packs + faminc, bwght) # use packs instead of cigs\nstargazer(model1, model2, model3, type = \"text\")\n\n\n=================================================================\n                                       Dependent variable:       \n                                ---------------------------------\n                                  bwght    I(bwght/16)   bwght   \n                                   (1)         (2)        (3)    \n-----------------------------------------------------------------\ncigs                            -0.463***   -0.029***            \n                                 (0.092)     (0.006)             \n                                                                 \npacks                                                  -9.268*** \n                                                        (1.832)  \n                                                                 \nfaminc                           0.093***   0.006***    0.093*** \n                                 (0.029)     (0.002)    (0.029)  \n                                                                 \nConstant                        116.974***  7.311***   116.974***\n                                 (1.049)     (0.066)    (1.049)  \n                                                                 \n-----------------------------------------------------------------\nObservations                      1,388       1,388      1,388   \nR2                                0.030       0.030      0.030   \nAdjusted R2                       0.028       0.028      0.028   \nResidual Std. Error (df = 1385)   20.063      1.254      20.063  \nF Statistic (df = 2; 1385)      21.274***   21.274***  21.274*** \n=================================================================\nNote:                                 *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#data-scaling-1",
    "href": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#data-scaling-1",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Data Scaling",
    "text": "Data Scaling\n\nCoefficients in a regression model are sensitive to the units of measurement\n\n\n\nThe population model: \\(bwght_i = \\beta_0 + \\beta_1 cigs_i + \\beta_2 faminc + u_i\\) follows MLR 1-5\n\\(bwght_i/16 = \\beta_0/16 + \\beta_1/16 cigs_i + \\beta_2/16 faminc + u_i\\)\n\nCoefficients scaled down by factor with which dependent variable is scaled\n\n\\(bwhgt_i = \\beta_0 + 20\\beta_1 cigs_i/20 + \\beta_2 faminc + u_i\\)\n\nwhich simplifies to \\(bwhgt_i = \\beta_0 + 20\\beta_1 packs_i + \\beta_2 faminc + u_i\\)\nCoefficients scaled up by factor with which independent variable is scaled\n\nWhat about SE? R^2?"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#standardized-coefficients",
    "href": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#standardized-coefficients",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Standardized Coefficients",
    "text": "Standardized Coefficients\nInstead of asking what happens to \\(y\\) when \\(x\\) changes by 1 unit, we ask what happens to \\(y\\) when \\(x\\) changes by 1 standard deviation\n\nThis is useful because:\n\n\\(x\\) may be measured in different units in different datasets\n\\(x\\) may be a composite index of several variables\n\\(x\\) could be scaled up or down differently in different datasets. E.g. GPA\n\nBut s.d. effects are only useful when interpreted relative to the mean of \\(x\\)\n\nSo the standardized \\(x_i\\) is \\(x^* = (x_i - \\bar{x})/s_x\\)\n\nThus from the population model: \\(y = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k + u\\)\n\nThe standardized model is: \\(y = \\beta_0^* + \\beta_1^* x_1^* + \\ldots + \\beta_k^* x_k^* + u^*\\)\n\nwhere \\(\\beta_j^* =  \\frac{s_{x_j}}{s_y}\\beta_j\\) for \\(j = 1, \\ldots, k\\)\n\n\nHow would you derive this?\nThis is easy to implement in R using scale() function"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#standardized-coefficients-1",
    "href": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#standardized-coefficients-1",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Standardized Coefficients",
    "text": "Standardized Coefficients\n\n\nScaled estimation:\n\nmodel4 &lt;- lm(scale(bwght) ~ scale(cigs) + scale(faminc), bwght)\nstargazer(model1, model4, type = \"text\", omit.stat = c(\"f\", \"ser\"))\n\n\n==========================================\n                  Dependent variable:     \n              ----------------------------\n                  bwght      scale(bwght) \n                   (1)            (2)     \n------------------------------------------\ncigs            -0.463***                 \n                 (0.092)                  \n                                          \nfaminc           0.093***                 \n                 (0.029)                  \n                                          \nscale(cigs)                    -0.136***  \n                                (0.027)   \n                                          \nscale(faminc)                  0.085***   \n                                (0.027)   \n                                          \nConstant        116.974***      -0.000    \n                 (1.049)        (0.026)   \n                                          \n------------------------------------------\nObservations      1,388          1,388    \nR2                0.030          0.030    \nAdjusted R2       0.028          0.028    \n==========================================\nNote:          *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\n\nCompute manually:\n\nmodel1$coef[\"cigs\"]* sd(bwght$cigs)/sd(bwght$bwght)\n\n      cigs \n-0.1359828 \n\nmodel1$coef[\"faminc\"]* sd(bwght$faminc)/sd(bwght$bwght)\n\n    faminc \n0.08540571"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#more-examples",
    "href": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#more-examples",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "More examples",
    "text": "More examples\nprice \\(=\\beta_0+\\beta_1\\) nox \\(+\\beta_2\\) crime \\(+\\beta_3\\) rooms \\(+\\beta_4\\) dist \\(+\\beta_5\\) stratio \\(+u\\)\n\n\n\nlibrary(wooldridge)\nsummary(lm(price ~ nox + crime + rooms + dist + stratio, hprice2))\n\n\nCall:\nlm(formula = price ~ nox + crime + rooms + dist + stratio, data = hprice2)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13914  -3201   -662   2110  38064 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 20871.13    5054.60   4.129 4.27e-05 ***\nnox         -2706.43     354.09  -7.643 1.09e-13 ***\ncrime        -153.60      32.93  -4.665 3.97e-06 ***\nrooms        6735.50     393.60  17.112  &lt; 2e-16 ***\ndist        -1026.81     188.11  -5.459 7.57e-08 ***\nstratio     -1149.20     127.43  -9.018  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5586 on 500 degrees of freedom\nMultiple R-squared:  0.6357,    Adjusted R-squared:  0.632 \nF-statistic: 174.5 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nsummary(lm(scale(price) ~ scale(nox) + scale(crime) + scale(rooms) + scale(dist) + scale(stratio), hprice2))\n\n\nCall:\nlm(formula = scale(price) ~ scale(nox) + scale(crime) + scale(rooms) + \n    scale(dist) + scale(stratio), data = hprice2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5110 -0.3476 -0.0719  0.2291  4.1334 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     2.327e-16  2.697e-02   0.000        1    \nscale(nox)     -3.404e-01  4.454e-02  -7.643 1.09e-13 ***\nscale(crime)   -1.433e-01  3.072e-02  -4.665 3.97e-06 ***\nscale(rooms)    5.139e-01  3.003e-02  17.112  &lt; 2e-16 ***\nscale(dist)    -2.348e-01  4.302e-02  -5.459 7.57e-08 ***\nscale(stratio) -2.703e-01  2.997e-02  -9.018  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6066 on 500 degrees of freedom\nMultiple R-squared:  0.6357,    Adjusted R-squared:  0.632 \nF-statistic: 174.5 on 5 and 500 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#functional-forms",
    "href": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#functional-forms",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Functional forms",
    "text": "Functional forms\nRecall from lecture 1\n\n\n\nSpecification\nChange in x\nEffect on y\n\n\n\n\nLevel-level\n+1 unit\n+\\(b_1\\) units\n\n\nLevel-log\n+1%\n+\\(\\frac{b_1}{100}\\) units\n\n\nLog-level\n+1 unit\n+\\((100 \\times b_1)\\%\\)\n\n\nLog-log\n+1%\n+\\(b_1\\%\\)"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#nonlinearities-in-x",
    "href": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#nonlinearities-in-x",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Nonlinearities in X",
    "text": "Nonlinearities in X\n\\[y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{1i}^2 + \\beta_3 x_{2i} + u_i\\]\n\\[\\frac{\\partial y}{\\partial x_{1i}} = \\beta_1 + 2\\beta_2 x_{1i}\\]\nDiscuss the nature of price change of houses with respect to number of rooms:\n\nlm(log(price) ~ log(nox) + log(dist) + rooms + I(rooms^2), hprice2)\n\n\nCall:\nlm(formula = log(price) ~ log(nox) + log(dist) + rooms + I(rooms^2), \n    data = hprice2)\n\nCoefficients:\n(Intercept)     log(nox)    log(dist)        rooms   I(rooms^2)  \n   12.87147     -0.88558     -0.04421     -0.72860      0.08004  \n\n\n\n\nBased on estimates how does price change as # of rooms go up?\nIncreasing or decreasing rates?\nAre there tipping points?\n\n\\(|\\beta_1/2\\beta_2|\\) is the tipping point for \\(y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{1i}^2 + \\beta_3 x_{3i} + u_i\\) when \\(x_{1i}\\) changes"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#interaction-terms",
    "href": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#interaction-terms",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Interaction terms",
    "text": "Interaction terms\n\n\n\\(p =  \\beta_0 + \\beta_1 sqrft + \\beta_2 bdrms + \\beta_3 sqrft \\times bdrms + \\beta_4 bthrms +u\\)\nWe can formalize it with conditional expecations:\n\\(E(p|sqrft, bdrms, bthrms) = \\beta_0 + \\beta_1 sqrft + \\beta_2 bdrms + \\beta_3 sqrft \\times bdrms + \\beta_4 bthrms\\)\n\\[\\frac{\\partial E(p|sqrft, bdrms, bthrms)}{\\partial bdrms} = \\beta_2 + \\beta_3 sqrft\\]\nSo at sqrft = \\(\\bar{sqrft}\\), the effect of bdrms on price is \\(\\beta_2 + \\beta_3 \\bar{sqrft}\\)\nEasy to obtain given \\(\\hat{\\beta}_2\\) and \\(\\hat{\\beta}_3\\) and \\(\\bar{sqrft}\\)\nSimilarly in the previous slide \\(\\frac{\\partial E(y | x_1 = \\bar{x_1}, x_2)}{\\partial x_1} = \\beta_1 + 2\\beta_2 \\bar{x_1}\\)\n\n\n\nmodel &lt;-  lm(price ~ sqrft + bdrms + I(sqrft * bdrms), data = hprice1)\n model$coefficients[\"bdrms\"] + model$coefficients[\"I(sqrft * bdrms)\"] * mean(hprice1$sqrft)\n\n   bdrms \n11.26181"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#adjusted-r-squared",
    "href": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#adjusted-r-squared",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Adjusted R-squared",
    "text": "Adjusted R-squared\n\nRecall Econ 160\nAdjusted R-squared is a modified version of R-squared that adjusts for the number of predictors in the model.\nCompare goodness of fit of models with different numbers of X’s. \\[\\text{Adjusted-}R^2 = 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - k - 1} \\right)\\] \\(R^2\\) is the R-squared value, \\(n\\) is the # observations and \\(k\\) is the number of X’s\n\nPenalizes the addition of unnecessary predictors\n\nThe F-test could be related to the adjusted R-squared. (next topic)\n\nCan be written into \\(UR\\) and \\(R\\) models for the F-test\nF-test is useful when models are nested"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#overfittingovercontrolling",
    "href": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#overfittingovercontrolling",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Overfitting/overcontrolling",
    "text": "Overfitting/overcontrolling\n\nIncluding too many controls can distort causal interpretation.\n\nExample: Estimating the effect of beer tax on traffic fatalities ins tates:\n\n\\[\n\\text{fatalities}_s = \\beta_0 + \\beta_1 \\text{tax}_s + \\beta_2 \\text{miles}_s + \\beta_3 \\text{percmale}_s + \\beta_4 \\text{perc16_21}_s + \\dots\n\\]\n\nShould we control for beer consumption (beercons)?\n\nNo, it mediates the effect of tax on fatalities.\n\nControlling for it absorbs the policy impact."
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#the-problem-of-overcontrolling",
    "href": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#the-problem-of-overcontrolling",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "The Problem of Overcontrolling",
    "text": "The Problem of Overcontrolling\n\nOvercontrolling can remove meaningful variation in key variables.\n\nExample: Estimating the effect of pesticides on health costs:\n\nControlling for doctor visits blocks part of the effect.\n\nAnother case: School quality → earnings:\n\nIf quality raises education, controlling for education understates its impact.\n\nTakeaway: Control variables should reflect causal logic, not just maximize \\(R^2\\)."
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#binary-variables",
    "href": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#binary-variables",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Binary variables",
    "text": "Binary variables\n\n\nBinary variables are {0,1} variables. E.g. female, married, smoker\nExample: Estimating the effect of gender on wages:\n\\(wage_i = \\beta_0 + \\delta_0 female_i + \\beta_1 educ_i + u_i\\)\n\\(\\delta_0=\\mathrm{E}( wage_i \\mid female_i =1, educ_i )-\\mathrm{E}( wage_i \\mid female_i =0, educ_i )\\)\nDifference in wages between females and males, holding education constant.\nIf \\(\\delta_0 &lt; 0\\), women earn less than men on average, given education.\n\nMales are the base group (\\(\\beta_0\\) is their intercept)[draw]\n\nIncluding both male and female leads to perfect collinearity.\nAlternatively: \\(wage_i = \\alpha_0 + \\gamma_0 male_i + \\beta_1 educ_i + u_i\\)\n\nHere, females are the base group."
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#tests-for-wage-discrimination",
    "href": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#tests-for-wage-discrimination",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "tests for wage discrimination",
    "text": "tests for wage discrimination\nwage \\(=\\beta_0+\\delta_0\\) female \\(+\\beta_1\\) educ \\(+\\beta_2\\) exper \\(+\\beta_3\\) tenure \\(+u\\)\n\nlibrary(wooldridge)\nsummary( lm(wage ~ female + educ + exper + tenure, data = wage1))\n\n\nCall:\nlm(formula = wage ~ female + educ + exper + tenure, data = wage1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.7675 -1.8080 -0.4229  1.0467 14.0075 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.56794    0.72455  -2.164   0.0309 *  \nfemale      -1.81085    0.26483  -6.838 2.26e-11 ***\neduc         0.57150    0.04934  11.584  &lt; 2e-16 ***\nexper        0.02540    0.01157   2.195   0.0286 *  \ntenure       0.14101    0.02116   6.663 6.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.958 on 521 degrees of freedom\nMultiple R-squared:  0.3635,    Adjusted R-squared:  0.3587 \nF-statistic:  74.4 on 4 and 521 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#multiple-categories",
    "href": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#multiple-categories",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Multiple categories",
    "text": "Multiple categories\nMarital status and gender: \\[\nlog(wage_i) = \\beta_0 + \\beta_1 marriedmale_i + \\beta_2 marriedfemale_i + \\beta_3 unmarriedfemale_i \\\\\n+ \\beta_4 educ_i + \\beta_5 exper_i + \\beta_6 tenure_i + \\beta_7 exper^2_i + \\beta_8 tenure^2_i + u_i\\] What is the base group?\n\nsummary(lm(log(wage) ~ I(married*(1-female)) + I(married*female) + I((1-married)*female) + educ + exper + tenure + I(exper^2) + I(tenure^2), data = wage1) )\n\n\nCall:\nlm(formula = log(wage) ~ I(married * (1 - female)) + I(married * \n    female) + I((1 - married) * female) + educ + exper + tenure + \n    I(exper^2) + I(tenure^2), data = wage1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.89697 -0.24060 -0.02689  0.23144  1.09197 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                0.3213781  0.1000090   3.213 0.001393 ** \nI(married * (1 - female))  0.2126757  0.0553572   3.842 0.000137 ***\nI(married * female)       -0.1982676  0.0578355  -3.428 0.000656 ***\nI((1 - married) * female) -0.1103502  0.0557421  -1.980 0.048272 *  \neduc                       0.0789103  0.0066945  11.787  &lt; 2e-16 ***\nexper                      0.0268006  0.0052428   5.112 4.50e-07 ***\ntenure                     0.0290875  0.0067620   4.302 2.03e-05 ***\nI(exper^2)                -0.0005352  0.0001104  -4.847 1.66e-06 ***\nI(tenure^2)               -0.0005331  0.0002312  -2.306 0.021531 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3933 on 517 degrees of freedom\nMultiple R-squared:  0.4609,    Adjusted R-squared:  0.4525 \nF-statistic: 55.25 on 8 and 517 DF,  p-value: &lt; 2.2e-16\n\n\n\nWhat is the wage gap between married females and unmarried females?"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#interactions-with-dummy-variables",
    "href": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#interactions-with-dummy-variables",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Interactions with dummy variables",
    "text": "Interactions with dummy variables\nWhat is the point of interacting: Marital status X gender?\n\\[\nlog(wage_i) = \\beta_0 + \\beta_1  female_i + \\beta_2 married_i + \\beta_3 married_i * female_i \\\\\n+ \\beta_4 educ_i + \\beta_5 exper_i + \\beta_6 tenure_i + \\beta_7 exper^2_i + \\beta_8 tenure^2_i + u_i\\]\n\nsummary(lm(log(wage) ~ female + married + I((married)*female) + educ + exper + tenure + I(exper^2) + I(tenure^2), data = wage1))\n\n\nCall:\nlm(formula = log(wage) ~ female + married + I((married) * female) + \n    educ + exper + tenure + I(exper^2) + I(tenure^2), data = wage1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.89697 -0.24060 -0.02689  0.23144  1.09197 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            0.3213781  0.1000090   3.213 0.001393 ** \nfemale                -0.1103502  0.0557421  -1.980 0.048272 *  \nmarried                0.2126757  0.0553572   3.842 0.000137 ***\nI((married) * female) -0.3005931  0.0717669  -4.188 3.30e-05 ***\neduc                   0.0789103  0.0066945  11.787  &lt; 2e-16 ***\nexper                  0.0268006  0.0052428   5.112 4.50e-07 ***\ntenure                 0.0290875  0.0067620   4.302 2.03e-05 ***\nI(exper^2)            -0.0005352  0.0001104  -4.847 1.66e-06 ***\nI(tenure^2)           -0.0005331  0.0002312  -2.306 0.021531 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3933 on 517 degrees of freedom\nMultiple R-squared:  0.4609,    Adjusted R-squared:  0.4525 \nF-statistic: 55.25 on 8 and 517 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#different-slopes",
    "href": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#different-slopes",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Different slopes",
    "text": "Different slopes\n\nDo men and women have different returns to education?\n\\(wage_i = \\beta_0 + \\beta_1 educ_i + \\beta_2 female_i + \\beta_3 educ_i * female_i + u_i\\)\n\\(E(wage_i | educ_i, female_i = 1) - E(wage_i | educ_i, female_i = 0) = \\beta_3 educ_i\\)\n\\(\\uparrow\\) educ by 1 unit \\(\\uparrow\\) wage by \\(\\beta_1 + \\beta_3\\) for females and \\(\\beta_1\\) for males. \n\n\nsummary(lm(log(wage) ~ educ + female + I(educ*female) + exper + expersq + tenure + tenursq, data = wage1))\n\n\nCall:\nlm(formula = log(wage) ~ educ + female + I(educ * female) + exper + \n    expersq + tenure + tenursq, data = wage1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.83265 -0.25261 -0.02374  0.25396  1.13584 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       0.3888060  0.1186871   3.276  0.00112 ** \neduc              0.0823692  0.0084699   9.725  &lt; 2e-16 ***\nfemale           -0.2267886  0.1675394  -1.354  0.17644    \nI(educ * female) -0.0055645  0.0130618  -0.426  0.67028    \nexper             0.0293366  0.0049842   5.886 7.11e-09 ***\nexpersq          -0.0005804  0.0001075  -5.398 1.03e-07 ***\ntenure            0.0318967  0.0068640   4.647 4.28e-06 ***\ntenursq          -0.0005900  0.0002352  -2.509  0.01242 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4001 on 518 degrees of freedom\nMultiple R-squared:  0.441, Adjusted R-squared:  0.4334 \nF-statistic: 58.37 on 7 and 518 DF,  p-value: &lt; 2.2e-16\n\n\n\nWhat is the gender gap at the average education level?"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#different-models-for-diff-groups",
    "href": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#different-models-for-diff-groups",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Different models for diff groups",
    "text": "Different models for diff groups\n\\[\nGPA_i =\\beta_0+\\beta_1  SAT_i +\\beta_2  hsrankperc_i +\\beta_3  tothrs_i +u_i\n\\]\n\nFor any of the slopes to depend on gender, we simply interact it with \\(female_i\\), and include it\nTo test if the model is different between men and women, then we need a model where the intercept and all slopes can be different across the two groups\n\n\\[\n\\begin{aligned}\nGPA_i = & \\beta_0+\\beta_1  sat_i +\\beta_2  hsperc_i   +\\beta_3  tothrs_i \\\\\n& +\\delta_0  female_i  +\\delta_1  female_i  *  sat_i +\\delta_2  female_i  *  hsperc_i +\\delta_3  female_i  *  tothrs_i  \\\\\n& +u_i\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#section",
    "href": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#section",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "",
    "text": "summary(lm(cumgpa ~ sat + hsperc + tothrs, data = gpa3))\n\n\nCall:\nlm(formula = cumgpa ~ sat + hsperc + tothrs, data = gpa3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.01959 -0.42768  0.04212  0.45355  2.90131 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.9291105  0.2285515   4.065 5.32e-05 ***\nsat          0.0009028  0.0002079   4.343 1.60e-05 ***\nhsperc      -0.0063791  0.0015678  -4.069 5.24e-05 ***\ntothrs       0.0119779  0.0009314  12.860  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8671 on 728 degrees of freedom\nMultiple R-squared:  0.2354,    Adjusted R-squared:  0.2323 \nF-statistic: 74.72 on 3 and 728 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nsummary(lm(cumgpa ~ sat + hsperc + tothrs + female + I(sat*female) + I(hsperc*female) + I(tothrs*female), data = gpa3)) \n\n\nCall:\nlm(formula = cumgpa ~ sat + hsperc + tothrs + female + I(sat * \n    female) + I(hsperc * female) + I(tothrs * female), data = gpa3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.08519 -0.39944  0.05277  0.45862  2.73325 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         1.214e+00  2.648e-01   4.584 5.37e-06 ***\nsat                 6.113e-04  2.350e-04   2.601 0.009484 ** \nhsperc             -5.967e-03  1.776e-03  -3.359 0.000823 ***\ntothrs              1.030e-02  1.093e-03   9.425  &lt; 2e-16 ***\nfemale             -1.114e+00  5.285e-01  -2.107 0.035460 *  \nI(sat * female)     1.117e-03  5.000e-04   2.233 0.025832 *  \nI(hsperc * female)  5.076e-05  4.103e-03   0.012 0.990132    \nI(tothrs * female)  5.560e-03  2.070e-03   2.686 0.007386 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8591 on 724 degrees of freedom\nMultiple R-squared:  0.2537,    Adjusted R-squared:  0.2464 \nF-statistic: 35.15 on 7 and 724 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#heteroskedasticity-to-be-covered-by-zeyi-in-the-lab",
    "href": "teachingmaterials/UG_intro_metrics/06_mlrvariants.html#heteroskedasticity-to-be-covered-by-zeyi-in-the-lab",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Heteroskedasticity to be covered by Zeyi in the lab",
    "text": "Heteroskedasticity to be covered by Zeyi in the lab"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#population-vs.-sample",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#population-vs.-sample",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Population vs. Sample",
    "text": "Population vs. Sample\nSimple linear regression model (SLRM): studies how \\(y\\) changes with changes in \\(x\\)\n\n\nPopulation Model: \\[y_i = β₀ + β₁x_i + u_i\\]\n\nβ₀: Population intercept parameter\nβ₁: Population slope parameter\n\\(u_i\\): Error term (unobserved factors)\nExample: \\(wages_i = β₀ + β₁education_i + u_i\\)\n\n\nEstimation from a sample gives: \\[\\hat{y_i} = \\widehat{\\beta_0} + \\widehat{\\beta_1} x_i\\]\n\n\\(\\widehat{\\beta_0}\\): estimated intercept\n\\(\\widehat{\\beta_1}\\): estimated slope\n\\(\\hat{u_i}\\): Residuals = \\(\\hat{y_i} - y_i\\)\n\n\n\n\n\n\n\n\nNote\n\n\n\nparameters by defintion are model-defined population constants (not random variables)\n“hats” refer to estimates\nIf I write \\(y\\) instead of \\(y_i\\) that means I am referring to the entire vector of all \\(y_i\\)’s"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#section",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#section",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "",
    "text": "Population Model: \\[y_i = β₀ + β₁x_i + u_i\\]\n\n\nIf the other unobserved factors in u are held fixed,\n\nso that the changes in u is zero,\n\nthen x has a linear effect on y\nsimply: \\(\\Delta y_i = \\beta_1 \\Delta x_i\\) iff \\(\\Delta u_i = 0\\)\nFormally, \\(\\frac{\\partial y_i}{\\partial x_i} = \\beta_1\\) iff \\(\\frac{\\partial u_i}{\\partial x_i} = 0\\)\n\n\n\n\n\n\n\nNote\n\n\n\nWe are imposing an assumption on the relation between \\(x\\) and \\(u\\) to be able to interpret \\(\\beta_1\\).\nLets formalize."
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#key-assumptions-about-u",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#key-assumptions-about-u",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Key Assumptions About \\(u\\)",
    "text": "Key Assumptions About \\(u\\)\n\nAssumptionsWhy is \\(E(u)=0\\) just a normalization?\n\n\n\n\n\n1. Zero Mean: \\(E(u) = 0\\)\n\nSimple normalization of unobservables\nCan always redefine intercept to make this true\n\nWhy This Works\n\nRequires intercept in the regression\nRedefining intercept absorbs any non-zero mean\n\n\n2. Mean Independence: \\(E(u|x) = E(u)\\)\n\nAverage error same for all values of \\(x\\)\nStronger than zero correlation\nVery strong assumption\n\nImplications\n\nCombines with first assumption to give:\n\\(E(u|x) = 0\\) (Zero Conditional Mean)\nEssential for interpreting \\(\\beta_1\\) as causal effect\n\n\n\n\n\n\n\nOriginal Model\n\\(wage_i = \\beta_0 + \\beta_1educ_i + u_i\\)\nWhere \\(u\\) includes ability:\n\nAverage ability = 100 (IQ points)\nSo \\(E(u) = 100 \\neq 0\\)\n\n\n\nRewritten Model\n\\(wage_i = (\\beta_0 + 100) + \\beta_1educ_i + \\tilde{u_i}\\)\nWhere:\n\n\\(\\tilde{u} = u - 100\\) is deviation from mean\nNow \\(E(\\tilde{u}) = 0\\)\nNew intercept = \\(\\beta_0 + 100\\)\nSame model, different parameterization"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#what-about-eu-mid-x-0",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#what-about-eu-mid-x-0",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "What about \\(E(u \\mid x) = 0\\)?",
    "text": "What about \\(E(u \\mid x) = 0\\)?\n\nLet us go back to the wage and education example\n\n\\[ wage_i = \\beta_0 + \\beta_1 education_i + u_i \\]\n\nwhat does mean independence and thereby zero conditional mean, mean here?\nis it too strong an assumption?"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#population-regression-function",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#population-regression-function",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Population Regression Function",
    "text": "Population Regression Function\n\nPopulation Regression FunctionSample VisualizationComponents of y\n\n\n\n\nZero conditional mean implies:\n\\(E(y_i|x_i) = \\beta_0 + \\beta_1x_i\\)\nInterpretation\n\nLinear function of x\n1 unit \\(\\uparrow\\) in x changes \\(E(y)\\) by \\(\\beta_1\\)\nGiven x, distribution of y centered around \\(E(y|x)\\)\n\n\ngpa1 data\n\\(E(colGPA|hsGPA) = 1.5 + 0.5 \\, hsGPA\\)\nFor \\(hsGPA = 3.6\\):\n\nAverage \\(colGPA = 1.5 + 0.5(3.6) = 3.3\\)\nNot every student gets 3.3\nSome higher, some lower\nDepends on unobserved factors (u)\n\n\n\n\n\nlibrary(wooldridge)\n# gpa_model &lt;- lm(colGPA ~ hsGPA, data = gpa1)\n# summary(gpa_model)\nplot(gpa1$hsGPA, gpa1$colGPA,\n     xlab = \"High school  GPA\",\n     ylab = \"College GPA\",\n     col = \"red\")\nabline(lm(gpa1$colGPA ~ gpa1$hsGPA), col = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n\nSystematic Part\n\n\\(\\beta_0 + \\beta_1x = E(y|x)\\)\nExplained by \\(x\\)\nPopulation regression line\nBlue line shows \\(E(y|x)\\) assuming linear model\n\n\nUnsystematic Part\n\n\\(u\\) = Deviation from \\(E(y|x)\\)\nNot explained by \\(x\\)\nZero mean at each \\(x\\): \\(E(u \\mid x) = 0\\)"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#section-1",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#section-1",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "",
    "text": "set.seed(123)\nn &lt;- 1e5\nu1 &lt;- rnorm(n, mean=0.0, sd=0.1)\nx &lt;- sin(seq(-5, 5, length.out=n))\nu2 &lt;- x + rnorm(n, mean=0, sd=0.1)\ncat(\"Mean of u1:\", mean(u1), \"\\n\", \"Mean of u2:\", mean(u2), \"\\n\")\n\nMean of u1: 9.767488e-05 \n Mean of u2: 0.0005215476 \n\npar(mfrow=c(1,2))\np1 &lt;- plot(x, u1, main=\"E(u)=0 and E(u|x)=0\")\nabline(lm(u1~x), col=\"red\", lwd=2)\np2 &lt;- plot(u2, x, main=\"E(u)=0 but E(u|x)!=0\")\nabline(lm(u2~x), col=\"red\", lwd=2)"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#section-2",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#section-2",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "",
    "text": "Starting assumption to deriving OLS estimators\n\nWe start with two key assumptions:\n\n\\(E(u) = 0\\)\n\\(E(u \\mid x) = 0\\) (zero conditional mean)\n\nUsing this we can show that:\n\n\\(Cov(x,u) = E(xu) - E(x)E(u)\\)\nSince \\(E(u) = 0\\):\n\n\\(Cov(x,u) = E(xu) - E(x) \\cdot 0 = E(xu)\\)\n\nTherefore: \\(Cov(x,u) = 0 \\iff E(xu) = 0\\)"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#derivation",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#derivation",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Derivation",
    "text": "Derivation\nPopulation Model: \\[y_i = β₀ + β₁x_i + u_i\\]\n\n\n\nAssumptions on the population:\n\n\\(E(u) = 0\\)\nCovariance between \\(x\\) and \\(u\\) is zero: \\(\\text{Cov}(x,u) = 0\\)\n\nThese imply: \\(E(xu) = 0\\)\n\n\nImplications:\n\nIn terms of observable variables \\(x\\) and \\(y\\):\n\n\\(E(y - \\beta_0 - \\beta_1x) = 0\\)\n\\(E[x(y - \\beta_0 - \\beta_1x)] = 0\\)\n\nAssumptions give us these moment conditions\n\n\n\nSample counterparts\n\nGiven a sample of data\nestimates \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) solve the sample counterparts of the moment conditions,\n\nleading to equations:\n\n\n\\[\\frac{1}{n} \\sum_{i=1}^{n}(y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i) = 0\\]\n\\[\\frac{1}{n} \\sum_{i=1}^{n}x_i(y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i) = 0\\]\n\nThese equations can be solved for \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\)."
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#section-3",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#section-3",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "",
    "text": "Now, \\(\\frac{1}{n}  \\sum_{i=1}^{n}(y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i) = 0\\), simplifies to\n\\[\\bar{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}\\bar{x_i}\\]\nWhere \\(\\bar{y_i} = \\frac{1}{n}  \\sum_{i=1}^{n}y_i\\) is the sample average of the \\(y_i\\).\nThus,\n\\[\\hat{\\beta_0} = \\bar{y_i}  - \\hat{\\beta_1}\\bar{x_i}\\]\nPlug in \\(\\hat{\\beta_0}\\) into: \\(\\frac{1}{n}  \\sum_{i=1}^{n}x_i(y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i) = 0\\) and simplifying gives us the slope coefficient \\(\\hat{\\beta_1}\\):\n\\[\\hat{\\beta_1} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\\]\nprovided, \\(\\sum_{i=1}^{n}(x_i - \\bar{x})^2 &gt; 0\\). What’s this?"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#section-4",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#section-4",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "",
    "text": "Note that \\(\\hat{\\beta_1} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\\) is nothing but\nthe sample covariance divided by the sample variance,\nwhich can be written as,\n\\[\\hat{\\beta}_1=\\hat{\\rho}_{x y} \\cdot\\left(\\frac{\\hat{\\sigma_y}}{\\hat{\\sigma_x}}\\right)\\]\nsince,\n\nsample covariance \\(= \\hat{\\rho}_{x y} \\cdot \\hat{\\sigma_y}\\hat{\\sigma_x}\\)\ncorrelation coeff: \\(\\hat{\\rho}_{x y}\\)\nsample variance \\(= \\hat{\\sigma_x}^2\\)"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#interestingly",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#interestingly",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Interestingly",
    "text": "Interestingly\n\nthe line (characterized by an intercept and a slope) that minimizes: \\[\\sum_{i=1}^n \\hat{u}_i^2= \\underbrace{\\sum_{i=1}^n\\left(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_i\\right)^2}_{\\text{SSR: Squared Sum of Residuals}}\\]\nlead to FOC’s that are same as what we obtained as the sample counterparts of the moment conditions\n\n\\(\\frac{1}{n}  \\sum_{i=1}^{n}(y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i) = 0\\)\n\\(\\frac{1}{n}  \\sum_{i=1}^{n}x_i(y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i) = 0\\)\n\nthat we used to derive the OLS estimates\nOLS chooses \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) to minimize SSR\nLet us implement in R and test with the lm package"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#quick-recap",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#quick-recap",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Quick recap",
    "text": "Quick recap\n\nMain assumptions on the population: \\(E(u) = 0\\) and \\(E(u \\mid x) = 0\\)\nGives us moment conditions which we can convert to sample counterparts invoking LLN\nSolving the sample counterparts gives us the OLS estimates"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#fitted-values-and-residual",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#fitted-values-and-residual",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Fitted values and residual",
    "text": "Fitted values and residual\n\nThe estimates of \\(\\beta_0\\) and \\(\\beta_1\\) thus obtained are called OLS\nCan obtain “fitted values” of \\(y_i\\): \\[\\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1} x_i\\]\nObtain residuals: \\(\\hat{u_i} = y_i - \\hat{y_i} = y_i - \\hat{\\beta_0} - \\hat{\\beta_1} x_i\\)"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#properties",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#properties",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Properties",
    "text": "Properties\n\nSample mean of residuals is zero \\(\\frac{1}{n} \\sum_{i=1}^{n} \\hat{u}_i = 0\\)\nSample mean of x and y \\((\\bar{x}, \\bar{y})\\) lies on the regression line\nThe sample covariance between the regressors (\\(x\\)) and the OLS residuals is zero \\(\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})\\hat{u}_i = 0\\)\nPredictions and residuals are uncorrelated \\(\\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})\\hat{u}_i = 0\\)\n\nTry testing these out with this generated data:\n\nset.seed(123)\nx &lt;- seq(1, 100, length.out = 50)\ny &lt;- 2 + 3*x + rnorm(50, 0, 20) #DGP\nplot(x, y, main=\"Linear Relationship Example\")\nabline(lm(y ~ x), col=\"red\", lwd=2)"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#decomposition",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#decomposition",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Decomposition",
    "text": "Decomposition\n\n\n\nTotal Sum of Squares (SST): \\[\n\\text{SST} \\equiv \\sum_{i=1}^{n}(y_i - \\bar{y})^2\n\\]\nExplained Sum of Squares (SSE): \\[\n\\text{SSE} \\equiv \\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y})^2\n\\]\nResidual Sum of Squares (SSR): \\[\n\\text{SSR} \\equiv \\sum_{i=1}^{n}\\hat{u}_i^2\n\\]\n\n\nRelationship Between Sums of Squares\nTotal variation in \\(y\\) can be expressed as:\n\\[\n\\text{SST} = \\text{SSE} + \\text{SSR}\n\\]\nProof Outline To prove this relationship, we can write:\n\\[\n\\sum_{i=1}^{n}(y_i - \\bar{y})^2 = \\sum_{i=1}^{n}[(\\hat{y}_i - \\bar{y}) + (y_i - \\hat{y}_i)]^2\n\\] \\[\n= \\sum_{i=1}^{n}(\\hat{u}_i + (y_i - \\bar{y}))^2\n\\] \\[\n= \\text{SSR} + 2\\sum_{i=1}^{n}\\hat{u}_i(y_i - \\bar{y}) + \\text{SSE}\n\\]"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#r-squared",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#r-squared",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "R-squared",
    "text": "R-squared\n\nMeasures goodness of fit \\[R^2 = \\frac{SSE}{SST} = 1 - \\frac{SSR}{SST}\\]\nRanges from 0 to 1\nInterpretation: Proportion of variance explained by model\n\n\n# Calculate R-squared\nsummary(model)$r.squared\n\n[1] 0.6510794\n\n# Decomposition of variance\nanova(model)\n\nAnalysis of Variance Table\n\nResponse: dist\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nspeed      1  21186 21185.5  89.567 1.49e-12 ***\nResiduals 48  11354   236.5                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#overview",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#overview",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Overview",
    "text": "Overview\n\nFor any estimator we want to know two important things\n\nWhether we are getting unbiased estimates\nIf yes, how precisely can we obtain those estimates\n\nWill require assumptions\nFor OLS, we have 5 of them, which will constitute the Gauss-Markov assumptions"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#unbiasedness-of-an-estimator",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#unbiasedness-of-an-estimator",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Unbiasedness of an estimator",
    "text": "Unbiasedness of an estimator\nAn estimator is unbiased if, on average, it produces estimates that are equal to the true value of the parameter being estimated.\n\n\n\nDefinition\n\n\nAn estimator producing an estimate \\(\\hat{\\theta}\\) for a parameter \\(\\theta\\) is considered unbiased if: \\[\n\\mathbb{E}(\\widehat{\\theta}) = \\theta\n\\]\n\n\n\nBasically means that the sampling distribution of \\(\\hat{\\theta}\\) is centered around the true \\(\\theta\\) on average\nWe will now show that the OLS estimator for a SLR model’s parameters \\((\\beta_0, \\beta_1)\\) is unbiased"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#assumptions-to-prove-unbiasedness-of-ols",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#assumptions-to-prove-unbiasedness-of-ols",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Assumptions to prove unbiasedness of OLS",
    "text": "Assumptions to prove unbiasedness of OLS\n\nSLR 1 - 4SLR1SLR2SLR3SLR4\n\n\n\nLinear in parameters\nRandom sampling\nSample Variation in X\nZero Conditional mean\n\n\n\n\nLinear in parameters:\nIn the population model, the dependent variable, \\(y\\), is related to the independent variable, \\(x\\), and the error (or disturbance), \\(u\\), as: \\[\ny=\\beta_0+\\beta_1 x+u\n\\] where \\(\\beta_0\\) and \\(\\beta_1\\) are the population intercept and slope parameters, respectively.\n\n\n\n\nRandom Sampling\nWe have a random sample of size n,\\(\\left\\{\\left(x_i, y_i\\right): i=1,2, \\ldots, n\\right\\}\\) in the population model\n\n\n\n\nSample variation in X\nThe sample explanatory x variable, namely, \\(x_i\\) , \\(i \\in \\{1, \\cdots, n\\}\\) , are not all the same value \\[\\widehat{Var}(x_i) &gt; 0\\]\n\n\n\n\nZero conditional mean\n\nThe error \\(u\\) has an expected value of zero given any value of the explanatory variable. In other words, \\[E(u \\mid x)=0\\] ."
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#theorem",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#theorem",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Theorem",
    "text": "Theorem\n\n\n\nTheorem: Unbiasedness of the OLS estimator\n\n\nUsing Assumptions SLR. 1 through SLR.4,\n\\[\n\\mathrm{E}\\left(\\hat{\\beta}_0\\right)=\\beta_0 \\text { and } \\mathrm{E}\\left(\\hat{\\beta}_1\\right)=\\beta_1\n\\]\n\n\n\n\nIn other words, \\(\\hat{\\beta}_0\\) is unbiased for \\(\\beta_0\\), and \\(\\hat{\\beta}_1\\) is unbiased for \\(\\beta_1\\).\nLet us prove it using our assumptions!\nIn doing so, we will learn some tricks, and I want you to learn the tricks and observe the patterns in the art of going about an econoemtric proof."
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#key-patterns-in-the-proof",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#key-patterns-in-the-proof",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Key patterns in the proof",
    "text": "Key patterns in the proof\n\nStart with the OLS estimate \\(\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\\) and \\(\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}\\)\nWrite the OLS estimator in terms of the population parameters\n\nTo do this we have to bring in \\(y_i\\) (population model) and not \\(\\hat{y_i}\\)\n\nThis is what brings in the population parameters \\(\\beta_0\\) and \\(\\beta_1\\) on the RHS\n\nTricks to bring in \\(y_i\\) in the OLS estimator:\n\nWithout any assumptions we showed and used:\n\\(\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y}) = \\sum_{i=1}^n(x_i - \\bar{x})y_i\\)\n\nCould have written this as \\(\\sum_{i=1}^n(y_i - \\bar{y})x_i\\) but did not to bring in \\(y_i\\)\n\n\nOther tricks: Without any assumptions\n\n\\(\\sum_{i=1}^n(x_i - \\bar{x}) = 0\\)\n\\(\\sum_{i=1}^n(x_i - \\bar{x})x_i = \\sum_{i=1}^n(x_i - \\bar{x})^2\\)\n\n\nTake the expectation conditional on \\(x_i\\)\n\nimpose the SLR assumptions on zero mean and zero conditional mean\nuse LIE"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#sampling-distribution-of-the-ols-estimators",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#sampling-distribution-of-the-ols-estimators",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Sampling distribution of the OLS estimators",
    "text": "Sampling distribution of the OLS estimators\n\npopulation model: \\(y_i = \\beta_0 + \\beta_1 x_i + u_i\\)\nThe OLS estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are random variables because they depend on the random sample \\(\\left\\{\\left(x_i, y_i\\right): i=1,2, \\ldots, n\\right\\}\\)"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#slr-5-homoskedasticity-assumption",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#slr-5-homoskedasticity-assumption",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "SLR 5: Homoskedasticity assumption",
    "text": "SLR 5: Homoskedasticity assumption\nThe population error \\(u_i\\) has the same variance given any value of \\(x_i\\), i.e.,\n\\[\nVar(u \\mid x) = \\sigma^2\n\\]\nSLR 5 further implies that \\(\\sigma^2\\) is also the unconditional variance of \\(u\\).\n\\[\\operatorname{Var}(u \\mid x)=\\mathrm{E}\\left(u^2 \\mid x\\right)-[\\mathrm{E}(u \\mid x)]^2\\]\n\\[ \\text { and since } \\mathrm{E}(u \\mid x)=0, \\sigma^2=\\mathrm{E}\\left(u^2 \\mid x\\right)\\]"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#now-we-have-var-y-mid-x",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#now-we-have-var-y-mid-x",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Now we have \\(Var( y \\mid x)\\)",
    "text": "Now we have \\(Var( y \\mid x)\\)\n\n\nusing Assumptions SLR. 4 and SLR. 5 we can derive the conditional variance of \\(y\\) :\n\\[\n\\begin{gathered}\n\\mathrm{E}(y \\mid x)=\\beta_0+\\beta_1 x \\\\\n\\operatorname{Var}(y \\mid x)=\\sigma^2\n\\end{gathered}\n\\]\nFrom here we can get \\(Var(u \\mid x)\\)\n\n\n\n\nHomoskedasticity"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#theorem-1",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#theorem-1",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Theorem",
    "text": "Theorem\n\n\n\nSampling variance of OLS estimators\n\n\nUnder Assumptions SLR. 1 through SLR.5,\n\\[\n\\operatorname{Var}\\left(\\hat{\\beta}_1\\right)=\\frac{\\sigma^2}{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2} = \\frac{\\sigma^2}{\\mathrm{SST}_x},\n\\]\nand\n\\[\n\\operatorname{Var}\\left(\\hat{\\beta}_0\\right)=\\frac{\\sigma^2 \\frac{1}{n} \\sum_{i=1}^n x_i^2}{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2} = \\frac{\\sigma^2 \\frac{1}{n} \\sum_{i=1}^n x_i^2}{\\mathrm{SST}_x}\n\\]\nwhere these are conditional on the sample values \\(\\left\\{x_1, \\ldots, x_n\\right\\}\\).\n\n\n\n\nLet us prove this!"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#proof-of-operatornamevarlefthatbeta_1right",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#proof-of-operatornamevarlefthatbeta_1right",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Proof of \\(\\operatorname{Var}\\left(\\hat{\\beta}_1\\right)\\)",
    "text": "Proof of \\(\\operatorname{Var}\\left(\\hat{\\beta}_1\\right)\\)\n\nLet us work with \\(\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\\) \\(\\implies\\) \\(\\hat{\\beta}_1 = \\beta_1 + \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})u_i}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\\)\nDenote \\((x_i - \\bar{x})\\) as \\(d_i\\). So \\(\\hat{\\beta}_1 = \\beta_1 + \\frac{\\sum_{i=1}^{n} d_iu_i}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\\)\nNow take the variance of \\(\\hat{\\beta}_1\\) conditional on the sample values \\(x_i\\)\n\n\\[\\begin{align*}\n    \\operatorname{Var}\\left(\\hat{\\beta}_1\\right) & = \\operatorname{Var}\\left(\\beta_1 + \\frac{\\sum_{i=1}^{n} d_iu_i}{SST_x}\\right) \\\\\n    & = \\operatorname{Var}\\left(\\frac{\\sum_{i=1}^{n} d_iu_i}{SST_x}\\right) \\quad \\text{skipping 2 steps} \\\\\n    & = \\frac{1}{\\left(SST_x\\right)^2} \\sum_{i=1}^{n} d_i^2 \\operatorname{Var}(u_i) \\quad \\text{skipping 3 steps} \\\\\n    & = \\frac{\\sigma^2}{SST_x}\n\\end{align*}\\]"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#measure-of-precision-of-hatbeta_1",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#measure-of-precision-of-hatbeta_1",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Measure of precision of \\(\\hat{\\beta_1}\\)",
    "text": "Measure of precision of \\(\\hat{\\beta_1}\\)\n\n\n\\(sd(\\hat{\\beta}_1) = \\sqrt{\\operatorname{Var}\\left(\\hat{\\beta}_1\\right)}\\) gives a measure of the precision of \\(\\hat{\\beta}_1\\)\nBut we do not know \\(\\sigma^2\\) so we do not know \\(sd(\\hat{\\beta}_1)\\)\nSo we have to estimate \\(\\sigma^2\\) to get an estimate of \\(sd(\\hat{\\beta}_1)\\)\nThe estimate of the \\(sd(\\hat{\\beta}_1)\\) is called the standard error of \\(\\hat{\\beta}_1\\)"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#theorem-2",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#theorem-2",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Theorem",
    "text": "Theorem\n\n\n\nUnbiased estimation of \\(\\sigma^2\\)\n\n\nUnder Assumptions SLR. through SLR.5,\n\\[\\mathrm{E}\\left(\\hat{\\sigma}^2\\right)=\\sigma^2\\]\nwhere \\(\\hat{\\sigma}^2=\\frac{1}{n-2} \\sum_{i=1}^n \\hat{u}_i^2\\), where \\(\\hat{u}_i=y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_i\\)\n\n\n\n\nStudy the proof from 2-5c\nTry to observe the patterns in the proof similar to the unbiasedness proof of the OLS estimates"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#in-essence-we-have-shown",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#in-essence-we-have-shown",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "In essence we have shown",
    "text": "In essence we have shown\n\nAssuming SLR 1-5, and conditional on the sample values \\(\\left\\{x_1, \\ldots, x_n\\right\\}\\), we have shown that:\n\nThe OLS estimators are unbiased\n\n\\(\\mathrm{E}\\left(\\hat{\\beta}_0\\right)=\\beta_0\\) and \\(\\mathrm{E}\\left(\\hat{\\beta}_1\\right)=\\beta_1\\)\n\nWe can obtain the variance of the OLS estimators\n\n\\(\\operatorname{Var}\\left(\\hat{\\beta}_1\\right)=\\frac{\\sigma^2}{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2} = \\frac{\\sigma^2}{\\mathrm{SST}_x}\\)\n\\(\\operatorname{Var}\\left(\\hat{\\beta}_0\\right)=\\frac{\\sigma^2 \\frac{1}{n} \\sum_{i=1}^n x_i^2}{\\mathrm{SST}_x}\\)\n\nBut we do not know \\(\\sigma^2\\)\n\nThe residual variance \\(\\hat{\\sigma}^2\\) is unbiased for the variance of the population errror \\(\\sigma^2\\)\n\\(\\mathrm{E}\\left(\\hat{\\sigma}^2\\right)=\\sigma^2\\)\n\nAnd we learnt a lot of tricks and pattern recognition in the process\nAnd some stuff about how to work with data"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#heteroskedasticity-in-slr",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#heteroskedasticity-in-slr",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Heteroskedasticity in SLR",
    "text": "Heteroskedasticity in SLR\n\nHeteroskedasticity: \\[Var(u_i \\mid x_i)=\\sigma^2(x_i) \\quad \\text{(not constant)}\\]\nUnder SLR1–SLR4 (but not SLR5):\n\nOLS remains unbiased for \\((\\beta_0,\\beta_1)\\)\nUsual variance formulae and standard errors are invalid\nOLS is no longer efficient among linear unbiased estimators\n\nLarge-sample variance (idea): weights depend on \\(u_i^2\\) \nFix (inference): use heteroskedasticity-robust (White/Eicker–Huber) SEs\n\nSame point estimates, different (robust) standard errors\n\n\n\n\n\n\n\n\nWarning\n\n\nHeteroskedasticity is common in cross-sectional data (wages, firm sizes, housing prices)"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#section-5",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#section-5",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "",
    "text": "library(lmtest)  # for BP test\nlibrary(sandwich)  # for robust SEs\n\n# Generate data with heteroskedasticity\nset.seed(456)\nn &lt;- 1000\neduc &lt;- rnorm(n, 12, 3)\nexper &lt;- runif(n, 0, 20)\nu &lt;- rnorm(n, 0, sd = 0.5 + 3*educ^2)\nwage &lt;- 10 + 2*educ + u\n\nmodel &lt;- lm(wage ~ educ)\n\n# Breusch-Pagan test for heteroskedasticity\nbptest(model)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model\nBP = 184.4, df = 1, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#section-6",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#section-6",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "",
    "text": "plot(fitted(model), residuals(model), \n     main=\"Residuals vs Fitted\", xlab=\"Fitted values\", \n     ylab=\"Residuals\", pch=20, col=\"darkblue\")\nabline(h=0, col=\"red\", lty=2)"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#recommendation-to-get-robust-ses",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#recommendation-to-get-robust-ses",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Recommendation to get robust SEs",
    "text": "Recommendation to get robust SEs\nIn the lm function, use:\n\nlibrary(sandwich)\nlibrary(lmtest)\n\nmodel &lt;- lm(wage ~ educ)\ncoeftest(model, vcov = vcovHC(model, type = \"HC1\"))\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   9.4847    76.1247  0.1246   0.9009\neduc          2.3196     7.1087  0.3263   0.7443\n\n# instead of\nsummary(model)\n\n\nCall:\nlm(formula = wage ~ educ)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2058.77  -251.34    -4.47   264.48  2258.63 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)    9.485     69.421   0.137    0.891\neduc           2.320      5.545   0.418    0.676\n\nResidual standard error: 515.7 on 998 degrees of freedom\nMultiple R-squared:  0.0001753, Adjusted R-squared:  -0.0008265 \nF-statistic: 0.175 on 1 and 998 DF,  p-value: 0.6758"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#the-four-types",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#the-four-types",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "The Four Types",
    "text": "The Four Types\n\nLevel-level: \\(y = b_0 + b_1x\\)\nLevel-log: \\(y = b_0 + b_1\\log(x)\\)\nLog-level: \\(\\log(y) = b_0 + b_1x\\)\nLog-log: \\(\\log(y) = b_0 + b_1\\log(x)\\)"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#interpreting-log-specifications",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#interpreting-log-specifications",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Interpreting Log Specifications",
    "text": "Interpreting Log Specifications\n\n\n\nSpecification\nChange in x\nEffect on y\n\n\n\n\nLevel-level\n+1 unit\n+\\(b_1\\) units\n\n\nLevel-log\n+1%\n+\\(\\frac{b_1}{100}\\) units\n\n\nLog-level\n+1 unit\n+\\((100 \\times b_1)\\%\\)\n\n\nLog-log\n+1%\n+\\(b_1\\%\\)"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#why-use-logs",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#why-use-logs",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Why Use Logs?",
    "text": "Why Use Logs?\n\nNormalize skewed distributions\nReduce impact of outliers\nInterpret coefficients as elasticities\nTransform multiplicative relationships to additive"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#example-ceo-salaries",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#example-ceo-salaries",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Example: CEO Salaries",
    "text": "Example: CEO Salaries\n\ndata(\"ceosal1\", package = \"wooldridge\")\npar(mfrow=c(1,2))\nplot(salary ~ sales, data = ceosal1, \n     main = \"Raw Values\")\nplot(log(salary) ~ log(sales), data = ceosal1, \n     main = \"Log Transformed\")"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#non-linear-relationships",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#non-linear-relationships",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Non-Linear Relationships",
    "text": "Non-Linear Relationships\n\nNot all relationships are linear\nCan add polynomial terms: \\(y_i = \\beta_0 + \\beta_1x_i^2 + u_i\\)\nVisual inspection is crucial\nAlways plot your data first!\n\n\n\n\n\n\n\nNote\n\n\nLinear in linear regression means linear in parameters not variables"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/03_regression.html#remaining-things-for-you-to-sudy",
    "href": "teachingmaterials/UG_intro_metrics/03_regression.html#remaining-things-for-you-to-sudy",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Remaining things for you to sudy",
    "text": "Remaining things for you to sudy\n\nRead up on the properties of the conditional expectation in Math Refresher B\nRead up:\n\n2-6: Regression through the Origin and Regression on a Constant\n2-7: Regression on a Binary Explanatory Variable\n\n\nnext class we will start chapter 3"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/ps/ugmetricsps2.html",
    "href": "teachingmaterials/UG_intro_metrics/ps/ugmetricsps2.html",
    "title": "Econometrics PS 2",
    "section": "",
    "text": "Md Moshi Ul Alam\n      Assistant Professor of Economics\n      \n        Clark University\n      \n      \n        \n          \n          Email\n        \n        \n          \n          CV"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/ps/ugmetricsps2.html#question-2",
    "href": "teachingmaterials/UG_intro_metrics/ps/ugmetricsps2.html#question-2",
    "title": "Econometrics PS 2",
    "section": "Question 2",
    "text": "Question 2\nDifferent states have different minimum wages. For example:\n\nAlabama, Georgia, Mississippi: $7.25/hour (federal minimum)\nFlorida: $12.00/hour\n\nNew York: $15.00/hour\nCalifornia: $15.50/hour\n\nYou want to estimate the effect of minimum wage on employment rates using data from different counties in the US indexed by c. You consider two regression specifications:\n\\text{employment rate}_c = \\beta_0 + \\beta_1 \\text{min\\_wage}_c + u_c \\quad (1)\n\\text{employment rate}_c = \\beta_0 + \\beta_1 \\text{min\\_wage}_c + \\beta_2 \\text{avg\\_wage}_c + u_c \\quad (2)\nwhere avg_wage is the average wage in the county and min_wage is the minimum wage in the county (which is the same as the state minimum wage for all counties in that state).\nWhich model correctly estimates the effect of minimum wage on employment and why?"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/ps/ugmetricsps2.html#question-3",
    "href": "teachingmaterials/UG_intro_metrics/ps/ugmetricsps2.html#question-3",
    "title": "Econometrics PS 2",
    "section": "Question 3",
    "text": "Question 3\nLoad the wooldridge library and using the wage1 dataset, youa re interested in documenting the gender wage gap.\n\nStart with running the following regression: log(wage_i) = \\beta_0 + \\beta_1 \\, female_i + u_i where female is a binary variable that takes the value 1 if the worker is female and 0 otherwise. Interpret the coefficient on female.\nIn estimating the average gender wage gap, why did we not include a dummy variable for male in the regression above?\nNow include educ (years of education), exper (years of experience), and tenure (years with current employer) in your analysis. How and why does the inclusion of these variables affect the coefficient on female? (Report the estimates side by side using the stargazer or modelsummary package).\nNow suppose someone asks you does an additional year of education effects wages differently for male and female workers. How would you modify your regression to answer this question? Run the regression and interpret the relevant coefficients.\nDo you think adding any other variables non-linearly will be helpful? If so, which ones and do your best to show data patterns to support your argument? (You do not need to run any regressions for this part)."
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/ps/ugmetricsps2.html#question-4",
    "href": "teachingmaterials/UG_intro_metrics/ps/ugmetricsps2.html#question-4",
    "title": "Econometrics PS 2",
    "section": "Question 4",
    "text": "Question 4\n\nProblem C6 Wooldridge Chapter 3.\nHow is this different from the partialling out analysis we did in class? Explain briefly."
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/ps/ugmetricsps2.html#question-5",
    "href": "teachingmaterials/UG_intro_metrics/ps/ugmetricsps2.html#question-5",
    "title": "Econometrics PS 2",
    "section": "Question 5",
    "text": "Question 5\n\nProblem C13 Wooldridge Chapter 3."
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#introduction",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#introduction",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Introduction",
    "text": "Introduction\nPopulation model: \\(y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\ldots + \\beta_k x_{ki} + u_i\\).\n\n\n\nestimate the parameters using OLS\nobtain the standard errors of the OLS estimates\nAnd imlement them in R\n\n\nNow, based on sample estimates and standard errors. we will answer questions like:\n\nWhether the estimated coefficient is statistically significant at any confidence level?\n\nMeans: Whether the true population parameter is different from zero at any confidence level?\n\n\n\n\nlibrary(wooldridge)\nwage_data &lt;- wage1\nreg1 &lt;- lm(log(wage) ~ educ + exper + tenure, data = wage_data)\nsummary(reg1)\n\n\nCall:\nlm(formula = log(wage) ~ educ + exper + tenure, data = wage_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.05802 -0.29645 -0.03265  0.28788  1.42809 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.284360   0.104190   2.729  0.00656 ** \neduc        0.092029   0.007330  12.555  &lt; 2e-16 ***\nexper       0.004121   0.001723   2.391  0.01714 *  \ntenure      0.022067   0.003094   7.133 3.29e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4409 on 522 degrees of freedom\nMultiple R-squared:  0.316, Adjusted R-squared:  0.3121 \nF-statistic: 80.39 on 3 and 522 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#normal-distributions",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#normal-distributions",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Normal Distributions",
    "text": "Normal Distributions\nRandom variable: \\(X \\sim N(\\mu, \\sigma^2)\\)\nPDF: \\(f(X = x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}\\), where\n\n\\(E(X) = \\mu\\) is the mean and \\(Var(X) = \\sigma^2\\) is the variance\nThe distirbution is symmetric around \\(\\mu\\)\n\nProperties:\n\n\\(Z = \\frac{X - \\mu}{\\sigma}\\) is the standard normal random variable with mean 0 and variance 1\nTransformation of X such as \\(aX + b \\sim N(a\\mu + b, a^2\\sigma^2)\\)\nIf X & Y are independent normal random variables, then \\(X + Y \\sim N(\\mu_X + \\mu_Y, \\sigma_X^2 + \\sigma_Y^2)\\)\nAny linear combination of normal random variables is normal"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#chi2_n-and-t_n",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#chi2_n-and-t_n",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "\\(\\chi^2_n\\) and \\(t_n\\)",
    "text": "\\(\\chi^2_n\\) and \\(t_n\\)\n\\(Z_i \\sim N(0,1)\\) for \\(i = 1, 2, \\ldots, n\\) independent standard normal random variables\n\nThe sum of squared independent standard normal random variables follows a chi-square distribution with \\(n\\) d.o.f.. Let \\(Q = \\sum_{i=1}^n Z_i^2\\) \\[Q \\sim \\chi^2_n\\]\n\n\\(E(Q) = n\\) and \\(Var(Q) = 2n\\) \n\nThe ratio of a standard normal random variable and a chi-square random variable follows a t-distribution. Let \\(T = \\frac{Z}{\\sqrt{Q/n}}\\) \\[T \\sim t_n\\] \n\n\\(E(T) = 0\\) and \\(Var(T) = n/(n-2)\\) for \\(n &gt; 2\\)\nShape of the t-distribution is similar to the normal distribution but more spread out (heavier tails). Check out Figure B-9 in Math Refresher B\n\nAs sample size increases, the t-distribution approaches the standard normal distribution"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#sampling-distributions",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#sampling-distributions",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "sampling distributions",
    "text": "sampling distributions\n\nThe distribution of the sample statistic (e.g., sample mean, sample variance, regression coefficients) over repeated independent sampling\n\nSampling variability: The variability of the sample statistic across different samples\nStandard error: The estimate of the standard deviation of the sampling distribution of a sample statistic\nCentral Limit Theorem (CLT): Let \\(\\{X_1, X_2, \\ldots, X_n\\}\\) be a random sample of size \\(n\\) from a population with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Then, as \\(n\\) approaches infinity, \\(\\bar{X} \\sim N(\\mu, \\sigma^2/n)\\) so that \\(\\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1)\\) \nRecall from Econ 160:\n\nThe sampling distribution of the sample proportion \\(\\hat{p}\\) approaches a normal distribution with mean \\(p\\) and variance \\(p(1-p)/n\\).\nThe sampling distribution of sample means follows a \\(t\\)-distribution with \\(n-1\\) degrees of freedom"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#hypothesis-testing",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#hypothesis-testing",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\n\nNull hypothesis \\(H_0\\): A statement about the population parameter that is assumed to be true and test against an alternative hypothesis \\(H_1\\)\nType I error: Rejecting the \\(H_0\\) when it is true\nType II error: Failing to reject the \\(H_0\\) when it is false\nSignificance level: The probability of committing a Type I error, denoted by \\(\\alpha\\)\nConfidence interval: A range of values where we are \\(1-\\alpha\\) confident that the true population parameter lies within\n\nestimate \\(\\pm\\) critical value \\(\\times\\) standard error\nWhat happens if the CI contains the hypothesized value?\n\nTests can only allow us to reject the \\(H_0\\) or fail to reject the \\(H_0\\) but never accept the \\(H_0\\). Why?"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#section",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#section",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "",
    "text": "p-value: The probability of observing the sample (statistic) if \\(H_0\\) were true\n\nWhat does it mean when p-value is less than \\(\\alpha\\)?\n\nCritical value: The value that separates the rejection region from the non-rejection region in hypothesis testing\n\nWhat is critical value for a two-tailed test with \\(\\alpha = 0.05\\)?\n\nCritical region: The range of values that leads to the rejection of the \\(H_0\\)\nOne-tailed test: A hypothesis test that tests the \\(H_0\\) in one direction\nTwo-tailed test: A hypothesis test that tests the \\(H_0\\) in both directions"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#big-picture",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#big-picture",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Big picture",
    "text": "Big picture\n\nTo test hypotheses about regression coeffcients \\(\\hat{\\beta}_j\\), we need to know:\n\nthe sampling distribution of the \\(\\hat{\\beta}_j\\)’s  \n\nOnce we have the sampling distribution of the \\(\\hat{\\beta}_j\\)\n\nWe can get the standard errors of the \\(\\hat{\\beta}_j\\)’s\n\nEstimates of the standard deviation of the sampling distribution of the \\(\\hat{\\beta}_j\\)’s\n\n\nWe can make inferences about the population parameters \\(\\beta_j\\)’s’ based on\n\nthe estimates, standard errors and the sampling distribution of the \\(\\hat{\\beta}_j\\)’s\n\nTurns out that under some assumptions, the \\(\\hat{\\beta}_j\\)’s follow a \\(t\\) distribution (in practice) \n\nIt requires an assumption on the distribution of the errors \\(u_i\\)’s MLR.6"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#assumption-mlr.6",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#assumption-mlr.6",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Assumption MLR.6:",
    "text": "Assumption MLR.6:\n\n\n\nAssumption MLR.6: Normality of Errors\n\n\nThe population error \\(u_i\\) is normally distributed with mean 0 and constant variance \\(\\sigma^2\\) and independent of the regressors for all \\(i = 1, 2, \\ldots, n\\) \\[u_i \\sim N(0, \\sigma^2)\\]\n\n\n\n\nThis directly implies MLR.4 and MLR.5\nThe full set of assumptions MLR.1-MLR.6 are called the classical linear model (CLM) assumptions. Together they imply: \\[y \\mid x_1, \\ldots x_k \\sim N(\\beta_0 + \\beta_1 x_{1i} + \\ldots + \\beta_k x_{ki}, \\sigma^2)\\]\nHard to defend this very strong assumption\n\nThankfully not required for large samples (Ch-5)"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#section-1",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#section-1",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "",
    "text": "Theorem 4.1: Normality of OLS Estimators\n\n\nUnder the CLM assumptions MLR. 1 through MLR.6, conditional on the sample values of the independent variables,\n\\[\n\\hat{\\beta}_j \\sim \\operatorname{Normal}\\left(\\beta_j , \\operatorname{Var}\\left(\\hat{\\beta}_j\\right)\\right),\n\\]\nwhere \\(\\operatorname{Var}\\left(\\hat{\\beta}_{j}\\right)\\)was given in Chapter 3 [equation (3.51)]. Therefore,\n\\[\n\\frac{\\hat{\\beta}_j-\\beta_j}{\\operatorname{sd}\\left(\\hat{\\beta}_j\\right)} \\sim \\operatorname{Normal}(0,1) \\quad \\text { for } j=0,1, \\ldots, k\n\\]\n\n\n\n\n\n\nTheorem 4.2: t-distribution of OLS Estimates\n\n\nUnder the CLM assumptions MLR. 1 through MLR.6, conditional on the sample values of the independent variables,\n\\[\n\\frac{\\hat{\\beta}_j-\\beta_j}{\\operatorname{se}\\left(\\hat{\\beta}_j\\right)} \\sim t_{n-k-1} \\quad \\text { for } j=0,1, \\ldots, k\n\\]\nwhere \\(k+1\\) is the number of unknown parameters in the population model and \\(n-k-1\\) is the d.o.f.\n\n\n\nObserve the differences!"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#single-hypothesis-testing-1",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#single-hypothesis-testing-1",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Single hypothesis testing",
    "text": "Single hypothesis testing\nInference on the \\(j\\)th population parameter \\(\\beta_j\\):\n\nNull hypothesis: \\(H_0: \\beta_j = b\\), where \\(b\\) is a hypothesized value typically 0\nNote that this is a test on the \\(j\\)th population parameter while holding all other parameters constant\nOne-sided alternative: \\(H_1: \\beta_j &gt; b\\) or \\(H_1: \\beta_j &lt; b\\)\nTwo-sided alternative: \\(H_1: \\beta_j \\neq b\\)\nset b= 0. Test statistic: \\(t_{\\hat{\\beta}_j} = \\frac{\\hat{\\beta}_j - b}{\\operatorname{se}(\\hat{\\beta}_j)} = \\frac{\\hat{\\beta}_j}{\\operatorname{se}(\\hat{\\beta}_j)}\\)\nWe know that the sampling distribution of \\(t_{\\hat{\\beta}_j}\\) follows…"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#one-sided-test",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#one-sided-test",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "One-sided test",
    "text": "One-sided test\nFix a level of significance \\(\\alpha\\) (e.g., 0.05)\n\n\n\nalternative: \\(H_1: \\beta_j &gt; 0\\) (or \\(H_1: \\beta_j &lt; 0)\\)\n\nReject \\(H_0\\) if \\(t_{\\hat{\\beta}_j} &gt; t_{\\alpha, n-k-1}^*\\)\np-value = \\(P(T &gt; t_{\\hat{\\beta}_j})\\) where \\(T \\sim t_{n-k-1}\\)\nConfidence interval: \\(\\hat{\\beta}_j \\pm t_{\\alpha, n-k-1}^* \\times \\operatorname{se}(\\hat{\\beta}_j)\\)\n\nFor large samples, t-distribution \\(\\rightarrow\\) standard normal\n\nthe critical value \\(\\approx  \\pm 1.65\\) for \\(\\alpha = 0.05\\)\n\n\n\n\n\n\nOne-sided test (right)\n\n\n\n\n\nOne-sided test (left)"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#examples",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#examples",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Examples",
    "text": "Examples\n\n\n\nlibrary(wooldridge)\nnrow(meap93) - 4 - 1 # DOF\n\n[1] 403\n\nsummary(lm(math10 ~ totcomp + staff + enroll, data=meap93))\n\n\nCall:\nlm(formula = math10 ~ totcomp + staff + enroll, data = meap93)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.235  -7.008  -0.807   6.097  40.689 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.2740209  6.1137938   0.372    0.710    \ntotcomp      0.0004586  0.0001004   4.570 6.49e-06 ***\nstaff        0.0479199  0.0398140   1.204    0.229    \nenroll      -0.0001976  0.0002152  -0.918    0.359    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.24 on 404 degrees of freedom\nMultiple R-squared:  0.05406,   Adjusted R-squared:  0.04704 \nF-statistic: 7.697 on 3 and 404 DF,  p-value: 5.179e-05\n\n\n\n\nlibrary(wooldridge)\nnrow(meap93) - 4 - 1 # DOF\n\n[1] 403\n\nsummary(lm(math10 ~ log(totcomp) + log(staff) + log(enroll), data=meap93))\n\n\nCall:\nlm(formula = math10 ~ log(totcomp) + log(staff) + log(enroll), \n    data = meap93)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.735  -6.838  -0.835   6.139  39.718 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -207.6649    48.7031  -4.264 2.50e-05 ***\nlog(totcomp)   21.1550     4.0555   5.216 2.92e-07 ***\nlog(staff)      3.9800     4.1897   0.950   0.3427    \nlog(enroll)    -1.2680     0.6932  -1.829   0.0681 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.18 on 404 degrees of freedom\nMultiple R-squared:  0.06538,   Adjusted R-squared:  0.05844 \nF-statistic:  9.42 on 3 and 404 DF,  p-value: 4.974e-06"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#two-sided-test",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#two-sided-test",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Two-sided test",
    "text": "Two-sided test\nFix a level of significance \\(\\alpha\\) (e.g., 0.05)\n\n\n\nalternative: \\(H_1: \\beta_j \\neq 0\\)\n\nReject \\(H_0\\) if \\(|t_{\\hat{\\beta}_j}| &gt; t_{\\alpha/2, n-k-1}^*\\)\np-value = \\(P(|T| &gt; |t_{\\hat{\\beta}_j}|)\\)\nConfidence interval: \\(\\hat{\\beta}_j \\pm t_{\\alpha/2, n-k-1}^* \\times \\operatorname{se}(\\hat{\\beta}_j)\\)\n\nFor large samples, t-distribution \\(\\rightarrow\\) standard normal\n\nthe critical value \\(\\approx \\pm 1.96\\) for \\(\\alpha = 0.05\\)\n\n\n\n\n\n\nTwo-sided test"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#examples-1",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#examples-1",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Examples",
    "text": "Examples\n\nlibrary(wooldridge)\nnrow(meap93) - 4 - 1 # DOF\n\n[1] 403\n\nsummary(lm(math10 ~ log(totcomp) + log(staff) + log(enroll), data=meap93))\n\n\nCall:\nlm(formula = math10 ~ log(totcomp) + log(staff) + log(enroll), \n    data = meap93)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.735  -6.838  -0.835   6.139  39.718 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -207.6649    48.7031  -4.264 2.50e-05 ***\nlog(totcomp)   21.1550     4.0555   5.216 2.92e-07 ***\nlog(staff)      3.9800     4.1897   0.950   0.3427    \nlog(enroll)    -1.2680     0.6932  -1.829   0.0681 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.18 on 404 degrees of freedom\nMultiple R-squared:  0.06538,   Adjusted R-squared:  0.05844 \nF-statistic:  9.42 on 3 and 404 DF,  p-value: 4.974e-06"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#computing-p-values",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#computing-p-values",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Computing p-values",
    "text": "Computing p-values\n\np-value is the probability of observing the sample (statistic) if \\(H_0\\) is true\nIndeed you can do this using a table.\nBut R can do this for you\n\n\nt_value &lt;- 2.5    #  t-value\ndf &lt;- 100         #  degrees of freedom\n\n# Two-tailed p-value\np_value &lt;- 2 * (1 - pt(abs(t_value), df))\nprint(p_value)\n\n[1] 0.01404579\n\n# One-tailed p-value (right tail)\np_right &lt;- 1 - pt(t_value, df)\nprint(p_right)\n\n[1] 0.007022895\n\n# One-tailed p-value (left tail)\np_left &lt;- pt(t_value, df)\nprint(p_left)\n\n[1] 0.9929771"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#testing-linear-combinations",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#testing-linear-combinations",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Testing Linear combinations",
    "text": "Testing Linear combinations\n\\[log(wage_i) = \\beta_0 + \\beta_1 jc_i + \\beta_2 univ_i + \\beta_3 exper_i + u_i\\]\n\nlibrary(wooldridge)\nsummary(lm(lwage ~ jc + univ + exper, data = twoyear))\n\n\nCall:\nlm(formula = lwage ~ jc + univ + exper, data = twoyear)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.10362 -0.28132  0.00551  0.28518  1.78167 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.4723256  0.0210602  69.910   &lt;2e-16 ***\njc          0.0666967  0.0068288   9.767   &lt;2e-16 ***\nuniv        0.0768762  0.0023087  33.298   &lt;2e-16 ***\nexper       0.0049442  0.0001575  31.397   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4301 on 6759 degrees of freedom\nMultiple R-squared:  0.2224,    Adjusted R-squared:  0.2221 \nF-statistic: 644.5 on 3 and 6759 DF,  p-value: &lt; 2.2e-16\n\n\n\nDoes attending \\(jc_i\\) has lower effect on wages as going to university \\(univ_i\\)?\nWhat are we testing?"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#section-2",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#section-2",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "",
    "text": "\\[log(wage_i) = \\beta_0 + \\beta_1 jc_i + \\beta_2 univ_i + \\beta_3 exper_i + u_i\\] \\(H_0: \\beta_1 = \\beta_2\\) and \\(H_1: \\beta_1 &lt; \\beta_2\\)\n\n\n\nCall \\(\\beta_1 - \\beta_2 = \\theta\\) and so \\(H_0: \\theta = 0\\) and \\(H_1: \\theta &lt; 0\\)\nTest statistic: \\(t_{\\hat{\\theta}} = \\frac{\\hat{\\theta}}{\\operatorname{se}(\\hat{\\theta})} = \\frac{\\hat{\\beta}_1 - \\hat{\\beta}_2}{\\operatorname{se}(\\hat{\\beta}_1-\\hat{\\beta}_2)} = \\frac{\\hat{\\beta}_1 - \\hat{\\beta}_2}{\\sqrt{\\operatorname{se}(\\hat{\\beta}_1)^2 + \\operatorname{se}(\\hat{\\beta}_2)^2 + 2\\widehat{\\operatorname{cov}(\\hat{\\beta}_1, \\hat{\\beta}_2)}}}\\) \n\nReject \\(H_0\\)\n\nif \\(t_{\\hat{\\theta}} &lt; t_{\\alpha, n-k-1}^*\\)\nOr if p-value = \\(P(T &lt; t_{\\hat{\\theta}})\\) where \\(T \\sim t_{n-k-1}\\) is less than \\(\\alpha\\)\nOr if CI: \\([\\hat{\\theta} \\pm t_{\\alpha, n-k-1}^* \\times \\operatorname{se}(\\hat{\\theta})]\\) does not contain zero\n\n\nEasy to calculate \\(t_{\\hat{\\theta}}\\) if \\(\\widehat{\\operatorname{cov}(\\hat{\\beta}_1, \\hat{\\beta}_2)}\\) is known"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#implementation",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#implementation",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Implementation",
    "text": "Implementation\n\n\n\\[log(wage_i) = \\beta_0 + \\beta_1 jc_i + \\beta_2 univ_i + \\beta_3 exper_i + u_i\\] \n\\[log(wage_i) = \\beta_0 + (\\theta + \\beta_2) jc_i + \\beta_2 univ_i + \\beta_3 exper_i + u_i\\]\n\\[log(wage_i) = \\beta_0 + \\theta jc_i + \\beta_2 (jc_i + univ_i) + \\beta_3 exper_i + u_i\\]"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#r-code",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#r-code",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "R code",
    "text": "R code\n\\[log(wage_i) = \\beta_0 + \\theta jc_i + \\beta_2 (jc_i + univ_i) + \\beta_3 exper_i + u_i\\]\n\nlibrary(wooldridge)\nsummary(lm(lwage ~ jc + I(jc + univ) + exper, data = twoyear))\n\n\nCall:\nlm(formula = lwage ~ jc + I(jc + univ) + exper, data = twoyear)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.10362 -0.28132  0.00551  0.28518  1.78167 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.4723256  0.0210602  69.910   &lt;2e-16 ***\njc           -0.0101795  0.0069359  -1.468    0.142    \nI(jc + univ)  0.0768762  0.0023087  33.298   &lt;2e-16 ***\nexper         0.0049442  0.0001575  31.397   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4301 on 6759 degrees of freedom\nMultiple R-squared:  0.2224,    Adjusted R-squared:  0.2221 \nF-statistic: 644.5 on 3 and 6759 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#testing-multiple-restrictions",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#testing-multiple-restrictions",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Testing multiple restrictions",
    "text": "Testing multiple restrictions\n\nThe \\(t\\)-test is used to test a hypothesis about a single population parameter \\(\\beta_j\\)\nThe \\(F\\)-test is used to test multiple hypotheses about multiple population parameters jointly\nExample: \\(\\log (\\text { salary }_i )=  \\beta_0+\\beta_1 \\text { years }_i +\\beta_2 \\text { gamesyr }_i +\\beta_3 \\text { bavg }_i  +\\beta_4 \\text { hrunsyr }_i +\\beta_5 \\text { rbisyr }_i +u_i\\)\n\n\\(\\mathrm{H}_0: \\beta_3=0, \\beta_4=0, \\beta_5=0\\)\n\\(\\mathrm{H}_1:\\) \\(H_0\\) is not true: At least one of \\(\\beta_3, \\beta_4, \\beta_5\\) is not zero"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#section-3",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#section-3",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "",
    "text": "library(wooldridge)\nsummary(lm(log(salary) ~ years + gamesyr + bavg + hrunsyr + rbisyr, data = mlb1))\n\n\nCall:\nlm(formula = log(salary) ~ years + gamesyr + bavg + hrunsyr + \n    rbisyr, data = mlb1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.02508 -0.45034 -0.04013  0.47014  2.68924 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.119e+01  2.888e-01  38.752  &lt; 2e-16 ***\nyears       6.886e-02  1.211e-02   5.684 2.79e-08 ***\ngamesyr     1.255e-02  2.647e-03   4.742 3.09e-06 ***\nbavg        9.786e-04  1.104e-03   0.887    0.376    \nhrunsyr     1.443e-02  1.606e-02   0.899    0.369    \nrbisyr      1.077e-02  7.175e-03   1.500    0.134    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7266 on 347 degrees of freedom\nMultiple R-squared:  0.6278,    Adjusted R-squared:  0.6224 \nF-statistic: 117.1 on 5 and 347 DF,  p-value: &lt; 2.2e-16\n\n\n\nBased on what we have learned so far, what do we think about \\(\\mathrm{H}_0: \\beta_3=0, \\beta_4=0, \\beta_5=0\\)?"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#let-us-run-both-models",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#let-us-run-both-models",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Let us run both models",
    "text": "Let us run both models\n\n\nUnrestricted model\n\nlibrary(wooldridge)\nunrestricted &lt;-  lm(log(salary) ~ years + gamesyr + bavg + hrunsyr + rbisyr, data = mlb1)\nsummary(unrestricted)\n\n\nCall:\nlm(formula = log(salary) ~ years + gamesyr + bavg + hrunsyr + \n    rbisyr, data = mlb1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.02508 -0.45034 -0.04013  0.47014  2.68924 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.119e+01  2.888e-01  38.752  &lt; 2e-16 ***\nyears       6.886e-02  1.211e-02   5.684 2.79e-08 ***\ngamesyr     1.255e-02  2.647e-03   4.742 3.09e-06 ***\nbavg        9.786e-04  1.104e-03   0.887    0.376    \nhrunsyr     1.443e-02  1.606e-02   0.899    0.369    \nrbisyr      1.077e-02  7.175e-03   1.500    0.134    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7266 on 347 degrees of freedom\nMultiple R-squared:  0.6278,    Adjusted R-squared:  0.6224 \nF-statistic: 117.1 on 5 and 347 DF,  p-value: &lt; 2.2e-16\n\nsum(unrestricted$residuals^2)\n\n[1] 183.1863\n\n\n\nRestricted model\n\nlibrary(wooldridge)\nrestricted &lt;-  lm(log(salary) ~ years + gamesyr, data = mlb1)\nsummary(restricted)\n\n\nCall:\nlm(formula = log(salary) ~ years + gamesyr, data = mlb1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.66858 -0.46412 -0.01177  0.49219  2.68829 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11.223804   0.108312 103.625  &lt; 2e-16 ***\nyears        0.071318   0.012505   5.703  2.5e-08 ***\ngamesyr      0.020174   0.001343  15.023  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7527 on 350 degrees of freedom\nMultiple R-squared:  0.5971,    Adjusted R-squared:  0.5948 \nF-statistic: 259.3 on 2 and 350 DF,  p-value: &lt; 2.2e-16\n\nsum(restricted$residuals^2)\n\n[1] 198.3115\n\n\n\n\nIs the increase in SSR large enough to reject the null hypothesis?\nBut what is the test statistic that we can use?"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#the-f-test",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#the-f-test",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "The F-test",
    "text": "The F-test\n\n\n\n\n\\(y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\ldots + \\beta_k x_{ki} + u_i\\) \nHypotheses Test of \\(q\\) exclusion restrictions\n\n\\(H_0: \\beta_{k-q} = \\beta_{k-q+1} = \\ldots = \\beta_k = 0\\)\n\\(H_1:\\) At least one of \\(\\beta_{k-q}, \\beta_{k-q+1}, \\ldots, \\beta_k\\) is not zero\n\nTest statistic: \\(F = \\frac{(SSR_r - SSR_{ur})/q}{SSR_{ur}/(n-k-1)}\\)\n\nCan bee re-written as \\(\\frac{(R^2_{UR} - R^2_R)/q}{(1 - R^2_{UR})/(n-k-1)}\\) \nFollows an \\(F\\)-distribution with \\(q\\) and \\(n-k-1\\) d.o.f.\nNote:\n\n\\(q = dof_r - dof_{ur}\\)\nF &gt; 0\n\n\n\n\n\n\n\n\nExample F sampling distribution"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#going-back-to-the-example",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#going-back-to-the-example",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Going back to the example",
    "text": "Going back to the example\n\\(\\log(salary_i)=  \\beta_0+\\beta_1 years_i +\\beta_2 gamesyr_i +\\beta_3 bavg_i  +\\beta_4 hrunsyr_i +\\beta_5 rbisyr_i +u_i\\)\n\n\\(\\mathrm{H}_0: \\beta_3=0, \\beta_4=0, \\beta_5=0\\)\n\\(\\mathrm{H}_1:\\) \\(H_0\\) is not true.\n\n\nlibrary(wooldridge)\nunrestricted &lt;-  lm(log(salary) ~ years + gamesyr + bavg + hrunsyr + rbisyr, data = mlb1)\nrestricted &lt;-  lm(log(salary) ~ years + gamesyr, data = mlb1)\nF_stat &lt;- ((sum(restricted$residuals^2) - sum(unrestricted$residuals^2))/3)/(sum(unrestricted$residuals^2)/(nrow(mlb1) - 5-1))\nprint(F_stat)\n\n[1] 9.550254\n\ncritical_F &lt;- qf(0.05, df1 = 3, df2 = nrow(mlb1) - 5-1, lower.tail = F)\nprint(critical_F)\n\n[1] 2.630641\n\nif (F_stat &gt; critical_F) {\n  print(\"Reject H0\")\n} else {\n  print(\"Fail to reject H0\")\n}\n\n[1] \"Reject H0\""
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#section-4",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#section-4",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "",
    "text": "\\(\\log(salary_i)=  \\beta_0+\\beta_1 years_i +\\beta_2 gamesyr_i +\\beta_3 bavg_i  +\\beta_4 hrunsyr_i +\\beta_5 rbisyr_i +u_i\\)\n\n\nBut recall when we ran the unrestricted model, we found that \\(\\beta_3, \\beta_4, \\beta_5\\) were not statistically significant.\nSo, whats going on here with the joint F-test?\n\\(hrunsyr\\) and \\(rbisyr\\) are highly correlated.\n\nmulticollinearity \\(\\rightarrow\\) large standard errors \\(\\rightarrow\\) low t-stats \\(\\rightarrow\\) individual statistical insignificance\n\nThe F-test is a joint test (including bavg) and is not affected by multicollinearity\nHence F-tests of joint hypotheses can be useful in the presence of multicollinearity"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#relationship-between-t-and-f-tests",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#relationship-between-t-and-f-tests",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Relationship between t and F tests",
    "text": "Relationship between t and F tests\n\nFor a single restriction \\(q=1\\), the F-test is equivalent to the t-test\n\n\\(F_{1,n-k-1} = t^2_{n-k-1}\\)\n\nFor single hypothesis testing, the t-test is more powerful\n\nF-tests remain under-powered than t-tests (See Math refresher C)"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#reporting-regression-results",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#reporting-regression-results",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Reporting Regression Results",
    "text": "Reporting Regression Results\n\nReport Estimated Coefficients\n\nInterpret key variables’ estimates in economic or practical terms.\n\nInclude Standard Errors\n\nPreferred over just \\(t\\)-statistics as they help interpret hypothesis tests and confidence intervals.\n\nReport \\(R^2\\) and Other Fit Statistics\n\n\\(R^2\\) is essential for goodness-of-fit.\n\nReporting F-statistics helps test exclusion restrictions.\n\nSummarize in Tables for Multiple Models\n\nIf multiple equations are estimated, use tables instead of inline equations.\n\nDependent variable should be clearly indicated.\n\nIndependent variables should be listed in the first column.\n\nStandard errors in parentheses below estimates."
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#salary-and-benefits-tradeoff-example",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#salary-and-benefits-tradeoff-example",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Salary and Benefits Tradeoff example",
    "text": "Salary and Benefits Tradeoff example\n\n\nTotal compensation (\\(totcomp\\)) consists of salary and benefits:\\[totcomp = salary + benefits = salary \\left( 1 + \\frac{benefits}{salary} \\right)\\]\nTaking the log transformation: \\(\\log(totcomp) = \\log(salary) + \\log(1 + b/s).\\)\nFor small \\(b/s\\), can approximate \\(\\log(1 + b/s) \\approx b/s.\\)\nThis leads to the econometric model:\\[\\log(salary) = \\beta_0 + \\beta_1 (b/s) + \\text{other *controls*}. \\]\nHypothesis Test: Testing the salary-benefits tradeoff:\n\n\\(H_0: \\beta_1 = -1\\) (full tradeoff)\n\\(H_1: \\beta_1 \\neq -1\\) (partial or no tradeoff)\n\nData from MEAP93 controls for enrollment, staff size, dropout, and graduation rates."
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#section-5",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#section-5",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "",
    "text": "library(stargazer, wooldridge)\nmodel1 &lt;- lm(log(salary) ~ I(benefits/salary), data = meap93)\nmodel2 &lt;- lm(log(salary) ~ I(benefits/salary) + I(log(enroll)) + I(log(staff)), data = meap93)\nmodel3 &lt;- lm(log(salary) ~ I(benefits/salary) + I(log(enroll)) + I(log(staff)) + droprate + gradrate, data = meap93)\n\nstargazer(model1, model2, model3, \n          type = \"text\", \n          title = \"Regression Results\",\n          omit.stat = c(\"ser\"), # Omit some stats if needed\n          dep.var.labels = \"Log(salary)\",\n          column.labels = c(\"Model 1\", \"Model 2\", \"Model 3\"),\n          covariate.labels = c(\"benefits to salary ratio\", \"log of enrollment\", \"log of staff\", \"Drop out rate\", \"Graduation rate\", \"Intercept\"))\n\n\nRegression Results\n================================================================================================\n                                                   Dependent variable:                          \n                         -----------------------------------------------------------------------\n                                                       Log(salary)                              \n                                 Model 1                 Model 2                 Model 3        \n                                   (1)                     (2)                     (3)          \n------------------------------------------------------------------------------------------------\nbenefits to salary ratio        -0.825***               -0.605***               -0.589***       \n                                 (0.200)                 (0.165)                 (0.165)        \n                                                                                                \nlog of enrollment                                       0.087***                0.088***        \n                                                         (0.007)                 (0.007)        \n                                                                                                \nlog of staff                                            -0.222***               -0.218***       \n                                                         (0.050)                 (0.050)        \n                                                                                                \nDrop out rate                                                                    -0.0003        \n                                                                                 (0.002)        \n                                                                                                \nGraduation rate                                                                   0.001         \n                                                                                 (0.001)        \n                                                                                                \nIntercept                       10.523***               10.844***               10.738***       \n                                 (0.042)                 (0.252)                 (0.258)        \n                                                                                                \n------------------------------------------------------------------------------------------------\nObservations                       408                     408                     408          \nR2                                0.040                   0.353                   0.361         \nAdjusted R2                       0.038                   0.348                   0.353         \nF Statistic              17.050*** (df = 1; 406) 73.386*** (df = 3; 404) 45.428*** (df = 5; 402)\n================================================================================================\nNote:                                                                *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/05_inference.html#modelsummary-package",
    "href": "teachingmaterials/UG_intro_metrics/05_inference.html#modelsummary-package",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "modelsummary package",
    "text": "modelsummary package\n\nlibrary(modelsummary)\nmodels &lt;- list(\n  \"Model 1\" = model1,\n  \"Model 2\" = model2,\n  \"Model 3\" = model3\n)\nmodelsummary(models, stars = TRUE, output = \"markdown\")\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Model 1\n                Model 2\n                Model 3\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  10.523***\n                  10.844***\n                  10.738***\n                \n                \n                  \n                  (0.042)\n                  (0.252)\n                  (0.258)\n                \n                \n                  I(benefits/salary)\n                  -0.825***\n                  -0.605***\n                  -0.589***\n                \n                \n                  \n                  (0.200)\n                  (0.165)\n                  (0.165)\n                \n                \n                  I(log(enroll))\n                  \n                  0.087***\n                  0.088***\n                \n                \n                  \n                  \n                  (0.007)\n                  (0.007)\n                \n                \n                  I(log(staff))\n                  \n                  -0.222***\n                  -0.218***\n                \n                \n                  \n                  \n                  (0.050)\n                  (0.050)\n                \n                \n                  droprate\n                  \n                  \n                  -0.000\n                \n                \n                  \n                  \n                  \n                  (0.002)\n                \n                \n                  gradrate\n                  \n                  \n                  0.001\n                \n                \n                  \n                  \n                  \n                  (0.001)\n                \n                \n                  Num.Obs.\n                  408\n                  408\n                  408\n                \n                \n                  R2\n                  0.040\n                  0.353\n                  0.361\n                \n                \n                  R2 Adj.\n                  0.038\n                  0.348\n                  0.353\n                \n                \n                  AIC\n                  8070.3\n                  7913.7\n                  7912.4\n                \n                \n                  BIC\n                  8082.4\n                  7933.7\n                  7940.5\n                \n                \n                  Log.Lik.\n                  192.417\n                  272.763\n                  275.397\n                \n                \n                  F\n                  17.050\n                  73.386\n                  45.428\n                \n                \n                  RMSE\n                  0.15\n                  0.12\n                  0.12"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "Md Moshi Ul Alam\n      Assistant Professor of Economics\n      \n        Clark University\n      \n      \n        \n          \n          Email\n        \n        \n          \n          CV\n        \n        \n          \n        \n        \n          \n        \n      \n    \n  \n\n\n\n\n\n\n\n  Download my CV \nLast updated: September 2025"
  },
  {
    "objectID": "teach.html",
    "href": "teach.html",
    "title": "Teaching",
    "section": "",
    "text": "Md Moshi Ul Alam\n      Assistant Professor of Economics\n      \n        Clark University\n      \n      \n        \n          \n          Email\n        \n        \n          \n          CV\n        \n        \n          \n        \n        \n          \n        \n      \n    \n  \n\n\n\n\n\n\n\n\nClark University\n\n\nUndergraduate: Statistics [Course website]\nUndergraduate: Econometrics\nPhD: Applied Econometrics-I [Course website]\n\nStarting Spring 2025:\n\nPhD: Applied Econometrics-II\nPhD: Econometric Theory\n\n\n\n\n\nQueen’s University\n\n\nPhD: Structural and reduced form methods in labor economics\n[Course Outline]\nUndergraduate: Applied Econometrics: Causal Inference\n[Course Outline] [Course Website]\n\n\n\n\n\nUniversity of Wisconsin-Madison\n\n🏆 Department of Economics Nominee for UW-Madison Letters & Sciences Teaching Fellows Award 2019\n\n\n\nEcon 705 – Econometric Theory (Graduate) Instructors: Jack Porter & Jeffrey Smith\n\n🏆 Juli Plant Grainger Teaching Excellence Award\n🏆 Distinguished Teaching Assistant\n\nEcon 706 – Applied Econometrics (Graduate) Instructors: Christopher Taber & Kenneth West\n\n🏆 Distinguished Teaching Assistant\n\nEcon 101 [Head TA] Instructors: Korinna Hansen, Elizabeth Kelly, Gwen Eudey\n\n🏆 Distinguished Teaching Assistant (×2)\n\n\n\n\n\n\nResources for Students\n\nCoding for Undergrads\nPodcasts Discussing Empirical Research in Economics"
  },
  {
    "objectID": "papers_complete.html",
    "href": "papers_complete.html",
    "title": "Moshi Alam",
    "section": "",
    "text": "Md Moshi Ul Alam\n      Assistant Professor of Economics\n      \n        Clark University\n      \n      \n        \n          \n          Email\n        \n        \n          \n          CV\n        \n        \n          \n        \n        \n          \n        \n      \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n Working Papers\n\n\n\n1. Employee-Side Discrimination: Beliefs and Preferences\n\n\nwith Mehreen Mookerjee and Sanket Roy\n\n\nR&R at Quantitative Economics PhD JMP \n\n\nAbstract Online Appendix A-C\n\n\n\n\n2. The Unintended Benefits of Women’s Empowerment on Household Sanitation\n\n\nwith Monica Agarwal\n\n\nR&R at Journal of Human Resources \n\n\nAbstract SSRN\n\n\n\n\n3. Labor Market Consequences of Pay-Equity Laws\n\n\nwith Steven Lehrer and Nuno Souso Pereira\n\n\nUnder Review  Post-doc JMP\n\n\nAbstract\n\n\n\n\n4. Computing Optimal Place‑Based Transfers with Direct Measurement of Geographic Variation in Marginal Utility of Consumption\n\n\nwith Morris Davis and Jesse Gregory\n\n\nDraft Coming Soon\n\n\n\n\n\n Publications\n\n\n\nIncreases in shared custody after divorce in the United States\n\n\nwith Daniel Meyer and Marcia Carlson\n\n\n Demographic Research 46(38) 2022: 1137-1162 ⭐ Editor’s Choice Article\n\n\nAbstract Replication\n\n\n\n\n\n Research in Progress\n\n\n\nSpatial Inequality and School Choice Mechanisms\n\n\nwith Monica Agarwal, Chao Fu, and YingHua He\n\n\n\n\nLabor Market Inequality and Collective Bargaining\n\n\nwith Steven Lehrer and Nuno Souso Pereira\n\n\n\n\nRacial Gaps in Wage Growth: Discrimination, Selection and Search Frictions\n\n\n\n\n\n\n\n\nDormant Papers\n\n\n Click to view dormant papers\n\n\n\n\nIdentification in Models of Discrimination\n\n\n[Subsumed in PhD job market paper “Employee Side Discrimination: Beliefs and Preferences”]\n\n\n\n\n\n\n\n\n\nCode\nmicro = require(\"micromodal@0.4.10\");\n\nmicro.init({\n  awaitOpenAnimation: true,\n  awaitCloseAnimation: true\n});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIncreases in shared custody after divorce in the United States\nwith Daniel Meyer and Marcia Carlson\n\n\nThis paper provides new evidence on the time trend in shared physical custody after divorce in the U.S., using eight waves of data from the Current Population Survey - Child Support Supplement. We find that the likelihood of shared custody more than doubled between divorces that occurred before 1985 and those in 2010-2014, from 12% to 28%. We show that non-Hispanic Whites and those who are more socioeconomically advantaged are more likely to have shared custody. Using more formal methods we show that the increase cannot be explained by changes in the characteristics of those divorcing; instead, we infer that this is the result of changing norms and policies that favor shared custody. Finally, this paper complements previous analyses using court record data from Wisconsin and shows that while the rate of shared custody in Wisconsin is higher than the national rate, a large increase over time has occurred in the nation as well as in Wisconsin. These changing patterns have important implications for children’s living arrangements and for the parental investments that children receive after their parents’ divorce.\n\n\n\n\n\n\n\n\n\n\n\nLabor Market Consequences of Pay-Equity Laws\nwith Steven Lehrer and Nuno Souso Pereira\n\n\nLimited policy variation, data constraints in defining ‘equal work’, and compliance issues have prevented rigorous causal analysis of pay equity laws. We address these challenges using Portugal’s 2018 legislation, which penalized firms with over 250 employees for gender wage gaps exceeding 5%. Using administrative data that links employees to industry-defined job titles within firms, we analyze its impact both within and between genders using an event study design. We find the law achieved its intended effects in some areas while generating unintended consequences in others. Jobs with initial gaps exceeding 5% saw a 9% reduction, mainly through slower male wage growth. A small fraction of jobs with negative gaps saw reduced female wage growth, closing the gap by half. However, gaps in jobs initially between 0-5% unexpectedly widened by 21% due to slower female wage growth. These unintended consequences are more pronounced in male-dominated industries and cannot be explained by productivity differences. Further, we find that firms did not change their size to evade the law, nor did it impact job gender composition, or hours worked. Our findings reveal both intended and unintended consequences of pay equity laws implemented with uniform regulatory targets: while the 5% target effectively reduced large disparities, it inadvertently widened gaps below the threshold as firms, with enforcement rules now clarified, strategically adjusted wages but not employment, offering crucial insights for policy design in achieving “equal pay for equal work”.\n\n\n\n\n\n\n\n\n\n\n\nEmployee-Side Discrimination: Beliefs and Preferences\nwith Mehreen Mookerjee and Sanket Roy\n\n\nWorkers’ preferences and beliefs shape labor market outcomes, yet remain understudied. We develop a theory-driven novel identification strategy for information experiments that separates preferences from beliefs. We apply this to provide the first evidence on the distribution of workers’ preferences on manager gender and their beliefs on managers’ mentoring ability. In the absence of information on manager mentoring ability, workers are indifferent to manager gender. However, upon receiving information on manager mentorship ability, workers prefer to work for female managers—willing to forgo 1.3–2.2% of average annual wages. Hence, absent additional information, workers believe female managers are worse mentors (1.6% wage equivalent). Non-parametric estimates of the distributions reveal that 75% prefer female managers with mentoring information, and 67% believe male managers are better mentors absent information. Machine learning algorithms show that individuals with higher education are less likely to hold such beliefs. These beliefs are primarily driven by perceptions that women are less competent. Evidence suggests these beliefs are likely biased, highlighting scope for information-based policy interventions that could reduce suboptimal matching and gender gaps in management levels. Our identification-driven design provides a general framework for information experiments to study beliefs.\n\n\n\n\n\n\n\n\n\n\n\nThe Unintended Benefits of Women’s Empowerment on Household Sanitation\nwith Monica Agarwal\n\n\nExisting research shows that women benefit more from private toilets, but misperceptions about the net benefits from toilets and lack of women’s decision-making power can hinder toilet adoption by households. In this paper, we explore a novel link between household sanitation and policies that empower women. We show that a policy aimed at improving women’s property inheritance rights in India led to an increase in toilet adoption in the households of treated cohorts by at least 10%. Prior literature shows mixed evidence on whether the policy increased women’s inheritance, but shows that the policy had significant indirect effects, such as improving women’s education. To generate empirical tests for the mechanisms driving our main results, we build a discrete choice model with idiosyncratic household preference shocks that produces policy-relevant complementarity between women’s education and decision-making power in adoption of a household public good valued more by women. Using a heterogeneity-robust event-study design, we find that, consistent with our model, the increase in toilet adoption is concentrated in states where the policy boosted women’s education—plausibly reducing misperceptions about the benefits of toilets—and increased women’s decision-making power. Our findings highlight that policies empowering women can yield unintended benefits beyond their original scope—while we document improvements in toilet coverage, the implications extend to other household investments where women’s preferences are stronger, but various frictions limit adoption."
  },
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "Research",
    "section": "",
    "text": "Md Moshi Ul Alam\n      Assistant Professor of Economics\n      \n        Clark University\n      \n      \n        \n          \n          Email\n        \n        \n          \n          CV\n        \n        \n          \n        \n        \n          \n        \n      \n    \n  \n\n\n\n\n\n\n\n\n\n Working papers\n\nEmployee-Side Discrimination: Beliefs and Preferences (PhD JMP)\nwith Mehreen Mookerjee and Sanket Roy\n[+Abstract] Online Appendix A-C\n\n\nR&R at Quantitative Economics Updated July ’25\n\nThe Unintended Benefits of Women’s Empowerment on Household Sanitation\nwith Monica Agarwal\n[+Abstract] [ SSRN]\n\n\nR&R at Journal of Human Resources Updated April ’25\n\nLabor Market Consequences of Pay-Equity Laws (Post-doc JMP)\nwith Steven Lehrer and Nuno Souso Pereira\n[+Abstract]\n\n\nUpdated Aug ’25 [Under review]\n\nComputing Optimal Place‑Based Transfers with Direct Measurement of Geographic Variation in Marginal Utility of Consumption\nwith Morris Davis and Jesse Gregory\n\n\nDraft coming soon!\n\n\n\n\n\n\n Publications/Accepted\n\n“Increases in shared custody after divorce in the United States.”\nwith Daniel Meyer and Marcia Carlson.\n[+Abstract] [ Replication]\n\n\nDemographic Research 46 (2022): 1137-1162. “Editor’s Choice” article\n\n\n\n\n\n\n Research in progress\n\nSpatial Inequality and School Choice Mechanisms\nwith Monica Agarwal, Chao Fu, YingHua He\nLabor Market Inequality and Collective Bargaining\nwith Steven Lehrer and Nuno Souso Pereira\nRacial Gaps in Wage Growth: Discrimination, Selection and Search Frictions\n\n\n\n\n\nDormant papers\n\nIdentification in Models of Discrimination\n[subsumed in my PhD job market paper “Employee Side Discrimination: Beliefs and Preferences”]\n\n\n\n\n\n\nCode\nmicro = require(\"micromodal@0.4.10\");\n\nmicro.init({\n  awaitOpenAnimation: true,\n  awaitCloseAnimation: true\n});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIncreases in shared custody after divorce in the United States\n\n\nThis paper provides new evidence on the time trend in shared physical custody after divorce in the U.S., using eight waves of data from the Current Population Survey - Child Support Supplement. We find that the likelihood of shared custody more than doubled between divorces that occurred before 1985 and those in 2010-2014, from 12% to 28%. We show that non-Hispanic Whites and those who are more socioeconomically advantaged are more likely to have shared custody. Using more formal methods we show that the increase cannot be explained by changes in the characteristics of those divorcing; instead, we infer that this is the result of changing norms and policies that favor shared custody. Finally, this paper complements previous analyses using court record data from Wisconsin and shows that while the rate of shared custody in Wisconsin is higher than the national rate, a large increase over time has occurred in the nation as well as in Wisconsin. These changing patterns have important implications for children’s living arrangements and for the parental investments that children receive after their parents’ divorce."
  },
  {
    "objectID": "blogs/dropbox_git_issues.html",
    "href": "blogs/dropbox_git_issues.html",
    "title": "Issues with cloud sync and version control",
    "section": "",
    "text": "Md Moshi Ul Alam\n      Assistant Professor of Economics\n      \n        Clark University\n      \n      \n        \n          \n          Email\n        \n        \n          \n          CV\n        \n        \n          \n        \n        \n          \n        \n      \n    \n  \n\n\n\n\n\n\n\nMany of us use cloud based services to sync important files. These services are not version controlling systems. If you have been use cloud based services to sync your files like Dropbox, it is recommended not to have a git repo inside a Dropbox folder for reasons described here, [here] and here or ensure Dropbox ignores the files that could cause conflicts with git. In short, this is how I do this:\n\nI only use git to version control my code. All other documentation that do not need version control for example, related papers, data etc. live in a project folder inside Dropbox.\nIf I am the only person writing the code, I either tell Dropbox to ignore the code directory if it is living in Dropbox, or I keep it outside of Dropbox. The latter works best if you are the sole person working on the code. The former is useful only for project organization—given that data and other documentation is living in the Dropbox folder, it is only convenient to also have the code directory inside the same project folder.\nIf I am collaborating with someone on code and they do not want to use git then I do not version control our code in the Dropbox folder. Rather, I keep a separate folder with all my codes in a non-Dropbox folder and version control that. At the end of each day, I then sync them with the Dropbox folder. This is only possible because my scripts are typically different from my collaborators script."
  },
  {
    "objectID": "blogs/julia_environments.html",
    "href": "blogs/julia_environments.html",
    "title": "Reproducible julia projects",
    "section": "",
    "text": "Md Moshi Ul Alam\n      Assistant Professor of Economics\n      \n        Clark University\n      \n      \n        \n          \n          Email\n        \n        \n          \n          CV\nA workflow that starts your project to be reproducible from day one, will reduce your work going forward. Since I have started using this workflow, I have realized that by construction it forces certain good habits. In this post, I will share a simple example of creating a reproducible project in julia, by using julia environments.1 The setup below works for me to ensure that the correct versions of my dependencies are used automatically on any machine."
  },
  {
    "objectID": "blogs/julia_environments.html#setting-up-a-julia-project-environment",
    "href": "blogs/julia_environments.html#setting-up-a-julia-project-environment",
    "title": "Reproducible julia projects",
    "section": "Setting up a julia project environment",
    "text": "Setting up a julia project environment\nFirst, setting up a new julia project typically involves creating the working directory, initializing the environment within that directory, and then modifying the environment, e.g. adding any necessary packages.\nmkdir -p path/to/MyCoolProject\ncd path/to/MyCoolProject\nThen I typically create a setup.jl file in the path/to/MyCoolProject/code folder to activate the environment and adding necessary packages.\nusing Pkg\ncd(@__DIR__)\nPkg.activate(@__DIR__)\nThese commands will create a Project.toml and Manifest.toml file in your project directory. These files will track your project dependencies and their exact versions, ensuring reproducibility.\n\nOnce you’ve initialized your project, you can add packages to it, as you normally would.\n# Add packages to your project\n(MyCoolProject) pkg&gt; add DataFramesMeta\nAdding the very first package does two things:\n\nUpdates Project.toml: It adds DataFrames to the list of dependencies.\nUpdates Manifest.toml: It records the exact version of DataFrames and its dependencies to ensure reproducibility."
  },
  {
    "objectID": "blogs/julia_environments.html#reusing-and-modifying-your-environment",
    "href": "blogs/julia_environments.html#reusing-and-modifying-your-environment",
    "title": "Reproducible julia projects",
    "section": "Reusing and modifying your environment",
    "text": "Reusing and modifying your environment\nIf you’ve closed your julia session and want to continue working on your project later, navigate back to your project directory and reactivate the environment:\ncd(\"path/to/MyCoolProject\")\njulia&gt; ]\n(v1.9) pkg&gt; activate .\nThis reactivates the environment associated with the Project.toml and Manifest.toml files in the same directory.\nTo add more packages to your project at any time:\n(MyCoolProject) pkg&gt; add Plots\nThis command updates your environment to include the Plots package. The Project.toml and Manifest.toml files will be updated accordingly.\nTo see the current status of your environment run Pkg.status() and to update run Pkg.update()"
  },
  {
    "objectID": "blogs/julia_environments.html#sharing-your-environment",
    "href": "blogs/julia_environments.html#sharing-your-environment",
    "title": "Reproducible julia projects",
    "section": "Sharing your environment",
    "text": "Sharing your environment\nTo share your project with others, provide the directory containing Project.toml and Manifest.toml. Others can recreate your environment by running:\njulia&gt; ]\n(v1.9) pkg&gt; activate .\n(MyCoolProject) pkg&gt; instantiate\nThis installs all the dependencies listed in Manifest.toml, recreating your exact environment."
  },
  {
    "objectID": "blogs/julia_environments.html#setting-up-the-project-on-a-different-machine",
    "href": "blogs/julia_environments.html#setting-up-the-project-on-a-different-machine",
    "title": "Reproducible julia projects",
    "section": "Setting up the project on a different machine",
    "text": "Setting up the project on a different machine\n\nWhen you clone a project repository or copy a project folder to a different machine, the Manifest.toml file contains the exact versions of all dependencies needed.\nRunning Pkg.instantiate() in this context installs these exact versions, recreating the original environment.\n\njulia&gt; ]\n(v1.9) pkg&gt; instantiate\nThe instantiate command has two more useful features:\n\nRestoring an Environment:\n\nIf you’ve made changes to your environment or accidentally removed packages and want to restore the environment to the exact state described in the Manifest.toml, you can use Pkg.instantiate() to reinstall all necessary dependencies.\nThis is also useful if you’ve deleted the ~/.julia/environments directory or are switching between environments and want to ensure the current project has all the required packages.\n\nEnsuring consistency across users: If you’re working in a shared project and the Manifest.toml has been updated by someone else (e.g., new packages have been added or versions changed), running Pkg.instantiate will synchronize your local environment with the latest dependencies as defined in the Manifest.toml.\n\nHappy researching!"
  },
  {
    "objectID": "blogs/julia_environments.html#footnotes",
    "href": "blogs/julia_environments.html#footnotes",
    "title": "Reproducible julia projects",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is different from creating a julia package. Excellent resources on creating julia packages can be found here, here and using the PkgTemplates.jl package to create a new package can be found here↩︎"
  },
  {
    "objectID": "morepages/podcasts.html",
    "href": "morepages/podcasts.html",
    "title": "Podcasts",
    "section": "",
    "text": "Md Moshi Ul Alam\n      Assistant Professor of Economics\n      \n        Clark University\n      \n      \n        \n          \n          Email\n        \n        \n          \n          CV\n        \n        \n          \n        \n        \n          \n        \n      \n    \n  \n\n\n\n\n\n\n\nPodcasts can at times offer an engaging and accessible way to grasp the underlying intuition of various econometric methods. They aid in building a deeper understanding of these methods and in exploring a wide range of problems through the lens of economics.\nI found the following episodes to be particularly interesting. This is a continously evolving list. Suggestions are welcome!\n\nRCT:\n\nAmy Finkelstein on the impact of health insurance (Oregon Medicaid expansion) experiment. “How do we know what really works in healthcare?” – Freakonomics (April 2, 2015)\nNicholas Bloom on the impact of working from home on productivity. “When you start to miss Tony from Accounting” – Hidden Brain (Nov 2020)\nJim Heckman on the impact of early childhood investments on long run outcomes. “Whats not on the test” – Hidden Brain\nJohn List (U-Chicago) on the rise of RCT and worries of scaling up experiments (SUTVA violations). “The Price of Doing Business with John List” – People I (Mostly) Admire\n“A Nobel-Prize for Development RCTs!” by David McKenzie World Bank Blog\n\n\n\nIV (Continuous and discrete treatments):\n\nSairt Weisburd on Police presence and Crime – Probable Causation July 21, 2020\nCarolina Artega on Parental Incarceration and Educational outcomes – Probable Causation July 20, 2021\n\n\n\nD-i-D:\n\nMolly Schnell on how exposure to school shootings affects students’ outcomes Probable Causation March 15, 2022\n(advanced) Adrew Goodman-Bacon on recent advances on D-i-D with staggered timing APPAM - Methodological Advances for Difference-in-Differences with Staggered Timing\n\n\n\nRD:\n\n“If Mayors ruled the worlds” Freakonomics April 10, 2014\n“Why is Uber an Economist’s Dream?” Freakonomics September 7, 2016\nMichael Lovenheim on grade retention and crime. Probable Causation September 17, 2019\nGaurav Khanna on Employment and Crime in Colombia. Probable Causation Aug 17, 2021\n\n\n\nDiff-in-RD (advanced)\n\nKirabo Jackson on single-sex education and academic and criminal outcomes. Probable Causation March 31, 2020"
  },
  {
    "objectID": "morepages/econ852.html",
    "href": "morepages/econ852.html",
    "title": "Structural and reduced form methods in labor economics: Course Outline",
    "section": "",
    "text": "Md Moshi Ul Alam\n      Assistant Professor of Economics\n      \n        Clark University\n      \n      \n        \n          \n          Email\n        \n        \n          \n          CV\n        \n        \n          \n        \n        \n          \n        \n      \n    \n  \n\n\n\n\n\n\n\n\nOutline\n\nPrinciples of empirical research\n\nIdentification\n\n“Non-parametric identification”, Matzkin (2007)\n“Identification of models of the labor market” Taber and French (2010)\n“Partial Identification in Econometrics” Tamer (2010)\n\nAdvantages of reduced form methods and advantages of structural models\nFeasibility of each approach individually and jointly\n“Structural Equations, Treatment Effects and Econometric Policy Evaluation” Heckman and Vytlacil (2003)\n\nReduced form methods\n\nShift-share exclusion restrictions\n\n“Bartik Instruments: What, When, Why and How” Goldsmith-Pinkham, Sorkin and Swift (2020)\n“Quasi-experimental shift-share research designs” Borusyak, Hull and Jaravel (2020)\n\nVariation in treatment timings and heterogenous effects\n\n“What’s trending in difference-in-differences?” Roth and Santa’Anna (2022)\n“Two-way fixed effects estimators with heterogeneous treatment effects” de Chaisemartin and d`Haultfoeuille (2020)\n“The Unintended Benefits of Women’s Empowerment on Household Sanitation” Alam and Agarwal (2023)\n\nEmpirical Bayes\n\n“Empirical Bayes deconvolution estimates.” Efron (2016)\n“Systemic discrimination among large US employers”, Kline, Rose and Walters (2022)\n“Robust empirical bayes confidence intervals” Armstrong, Kolesár and Plagborg-Møller (2022)\n\nDiscontinuity based designs\n\n“Inference on causal effects in a generalized regression kink design” Card et al. (2015)\n“Inference in Regression Discontinuity Designs with a Discrete Running Variable”, Kolesar and Rothe (2018)\n\n\nStructural models\n\nConditional choice probabilities, discrete choice and dynamic models\n\n“Discrete Choice Methods with Simulation” Train (2009)\n“Conditional Choice Probabilities and the Estimation of Dynamic Models”, Hotz and Miller (1993)\n“College Attrition and the Dynamics of Information Revelation”, Arcidiacono, Aucejo, Maurel, et al. (2018)\n“The Structural Estimation of Behavioral Models: Discrete Choice Dynamic Programming Methods and Applications,” Keane and Wolpin (2011)\n\nStated preference\n\n“Preference for the workplace, investment in human capital and gender”, Wiswall Zafar (2018)\n“Understanding Migration Aversion Using Elicited Choice Probabilities”, Kosar, Ransom, van der Klaauw (2022)\n“Optimal Place-based redistribution” Alam, Davis and Gregory (2023)\n\n\nModels on wage inequality\n\n“High wage workers and high wage firms” Abowd, Kramarz and Margolis (1999)\n“A Distributional Framework for Matched Employer-employee data”, Bonhomme, Lamadon, Manresa (2019)\n“Racial Gaps in Wage Growth: Discrimination and Search Frictions” Alam (2020)\n“Discretizing Unobserved heterogeneity” Bonhomme, Lamadon, Manresa (2019)\n“Monopsony in Movers: The Elasticity of Labor Supply to Firm Wage Policies”, Bassier, Dube, Naidu (2021)\n\n\nUsing both reduced form and structural models\n\n“Worker-side discrimination–Beliefs and Preferences - Evidence from an Information Experiment on Jobseekers”, Alam, Mookerjee, Roy (2022)\n“Estimating Equilibrium Effects of Job Search Assistance,” Gautier, Muller, van der Klaauw, Rosholm and Svarer (2018)\n“The Demand for Food of Poor Urban Mexican Households: Understanding Policy Impacts Using Structural Models” Angelucci and Attanasio (2013)\n“Evaluating Search and Matching Models Using Experimental Data” Lise, Seitz and Smith (2015)\n“Evaluating a Structural Model of Labor Supply and Welfare Participation: Evidence from State Welfare Reform Experiments,” Choi (2018)\n“Estimating labour supply responses and welfare participation: Using a natural experiment to validate a structural labour supply model” Hansen and Liu (2015)\n“The Role of Labor and Marriage Markets, Preference Heterogeneity and the Welfare System on the Life Cycle Decisions of Black, Hispanic and White Women,” Keane and Wolpin (2010)\n“Approximating the Equilibrium Effects of Informed School Choice,” Allende, Gallego and Nielson (2019)\n\nCoding in Julia (if time permits)\n\nWhy Julia?\nEfficient code, version control and large project workflow\nAutomatic testing and reproducibility\nIntroduction to numerical optimization #### Other readings\n\nControl function approach\n\nControl Function Methods in Applied Econometrics, Wooldridge (2015)\n“Identification of Treatment Effects Using Control Functions in Models with Continuous, Endogenous Treatment and Heterogeneous Effects”, Heckman et. al (2008)\n“Using Matching, Instrumental Variables, and Control Functions to Estimate Economic Choice Models” Heckman and Navarro (2004)\n“Control functions” Navarro (2010)"
  },
  {
    "objectID": "morepages/UG_intro_metrics.html",
    "href": "morepages/UG_intro_metrics.html",
    "title": "Introduction to Econometrics",
    "section": "",
    "text": "Md Moshi Ul Alam\n      Assistant Professor of Economics\n      \n        Clark University\n      \n      \n        \n          \n          Email\n        \n        \n          \n          CV"
  },
  {
    "objectID": "morepages/UG_intro_metrics.html#course-information",
    "href": "morepages/UG_intro_metrics.html#course-information",
    "title": "Introduction to Econometrics",
    "section": " Course Information",
    "text": "Course Information\n\n  Download Syllabus \nAlso see  Resources for Students"
  },
  {
    "objectID": "morepages/UG_intro_metrics.html#lecture-slides",
    "href": "morepages/UG_intro_metrics.html#lecture-slides",
    "title": "Introduction to Econometrics",
    "section": " Lecture Slides",
    "text": "Lecture Slides\n\n\nIntroduction to R\nStatistical Review & Population Concepts\nSimple Linear Regressions\nMultiple Linear Regressions\nMore on MLR\nInference in Regression Models\nHeteroskedasticity & Serial Correlation [Discussion section]\nPotential Outcomes\nRandomized Controlled Trials\nInstrumental Variables, 2SLS & HTE\nCourse Summary"
  },
  {
    "objectID": "morepages/UG_intro_metrics.html#problem-sets",
    "href": "morepages/UG_intro_metrics.html#problem-sets",
    "title": "Introduction to Econometrics",
    "section": " Problem Sets",
    "text": "Problem Sets\n\nProblem Set 1 (Due Sep 18)\n Assignment |  Solutions\nProblem Set 2 (Due Oct 5)\n Assignment"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Md Moshi Ul Alam\n      Assistant Professor of Economics\n      \n        Clark University\n      \n      \n        \n          \n          Email\n        \n        \n          \n          CV\n        \n        \n          \n        \n        \n          \n        \n      \n    \n  \n\n\n\n\n\n\n\n\n\nClark University\n\nUndergraduate: Statistics\nUndergraduate: Econometrics \nPhD: Applied Econometrics-I\n\nStarting Spring 2025:\n\nPhD: Applied Econometrics-II\nPhD: Econometric Theory\n\n\n\n\nQueen’s University\n\nPhD: Structural and reduced form methods in labor economics\n\n[Course Outline]\n\nUndergraduate: Applied Econometrics: Causal Inference.\n\n[Course Outline] [Course Website]\n\n\n\n\n\nUniversity of Wisconsin-Madison\n\nDepartment of Economics Nominee for UW-Madison Letters & Sciences Teaching Fellows Award 2019\n\n\nEcon 705 – Econometric Theory (Graduate). Instructors: Jack Porter & Jeffrey Smith\n\nJuli Plant Grainger Teaching Excellence Award\nDistinguished Teaching Assistant\n\nEcon 706 – Applied Econometrics (Graduate). Instructors: Christopher Taber & Kenneth West\n\nDistinguished Teaching Assistant\n\nEcon 101 [Head TA]. Instructor: Korinna Hansen; Elizabeth Kelly; Gwen Eudey \n\nDistinguished Teaching Assistant (x 2)\n\n\n\n\n\n\n\nResources for students\nCoding for undergrads\nPodcasts discussing empircal research in economics"
  },
  {
    "objectID": "morepages/code.html",
    "href": "morepages/code.html",
    "title": "Code",
    "section": "",
    "text": "Md Moshi Ul Alam\n      Assistant Professor of Economics\n      \n        Clark University\n      \n      \n        \n          \n          Email\n        \n        \n          \n          CV\n        \n        \n          \n        \n        \n          \n        \n      \n    \n  \n\n\n\n\n\n\n\n\nCode for research\nHere are the codes of some julia/R functions that I have implemented in my research. There may be more efficient routines. I have often found julia to be quite fast if the data or the parameter space is large. And I like coding in it.\nI have tried to make them context agnostic. However, they may not be fully general, some almost by construction. Hence these are not packages. Please feel free to use them, or report a bug if you find one!\n\nQuantile regressions with FE (Canay 2011) (julia)\nNelder-Mead simplex algorithm (julia)\nParametric shrinkage with heteroskedastic errors (julia)\nNon-parametric shrinkage with heteroskedastic errors and shape restrictions (R)\nEvent studies with interacted FE (julia)\n\n\n\nCode for teaching\nMost of these are in R. Their objective is to simulate data with failures in assumptions required for identification, or for inference and show what happens. Some of them are simply hard-coding the estimator instead of using a package which I have found to be helpful being able to compare various estimators.\n\nSimple linear regression\nCentral Limit Theorem\nFrisch-Waugh-Lovell theorem\nIV and 2SLS\nWeak IV vs Strong IV"
  },
  {
    "objectID": "morepages/resourcesforugstudents.html",
    "href": "morepages/resourcesforugstudents.html",
    "title": "Resources for undergrad students",
    "section": "",
    "text": "Md Moshi Ul Alam\n      Assistant Professor of Economics\n      \n        Clark University\n      \n      \n        \n          \n          Email\n        \n        \n          \n          CV"
  },
  {
    "objectID": "morepages/resourcesforugstudents.html#coding-resources",
    "href": "morepages/resourcesforugstudents.html#coding-resources",
    "title": "Resources for undergrad students",
    "section": "Coding resources",
    "text": "Coding resources\nStudents are free to use any language of their preference–Stata, R, julia, python, Matlab etc. Historically, most students have used Stata for the analysis they conduct in their term paper. However, starting 2025, I have shifted my teaching of Econometrics, specifically implementation of methods—both simulation and estimation—to R. Below you will find old open-source resources as well as new resources that I recommend to students.\nI strongly encourage students to use open-source languages like R, julia and python."
  },
  {
    "objectID": "morepages/resourcesforugstudents.html#r-resources",
    "href": "morepages/resourcesforugstudents.html#r-resources",
    "title": "Resources for undergrad students",
    "section": "R resources",
    "text": "R resources\n\nStata2R: Students used to working in Stata will find this website incredibly useful if they want to transition to R.\nR manuals from CRAN and the CRAN homepage for R\nMaster repo with various cheatsheets\nRMarkdown: I ask students to complete their problem sets in RMarkdown to have both code compilation as well as theoretical proofs written and executed in the same file.\n\nR Markdown: The Definitive Guide: far more extensive, covers interactivity and additional features.\nQuarto markdown is another alternative that I really like for its versatility in languages. RMarkdown can also handle multiple languages (see here) but I prefer the ease of customization of Quarto when it comes to multiple languages.\n\nPackage specific:\n\ndata.table\ndplyr\nggplot2\nfixest\nAER (Applied Econometrics with R)\nbinsreg\nvtable\nstargazer\nmore on stargazer\nmodelsummary"
  },
  {
    "objectID": "morepages/resourcesforugstudents.html#stata-resources",
    "href": "morepages/resourcesforugstudents.html#stata-resources",
    "title": "Resources for undergrad students",
    "section": "Stata Resources",
    "text": "Stata Resources\nBesides google search being a very good friend while picking up any language, here are some resources on Stata from the internet that may be useful for students.\n\nSocial Science Computing Center at UW-Madison\nData and Statistical services at Princeton\nAdvanced Research computing at UCLA\nProject management in Stata\nTransitioning between Stata and R\n\nMost of them, have corresponding pages for R as well."
  },
  {
    "objectID": "morepages/resourcesforugstudents.html#best-practices",
    "href": "morepages/resourcesforugstudents.html#best-practices",
    "title": "Resources for undergrad students",
    "section": "Best practices",
    "text": "Best practices\n\nUseful checklist while working with regression outputs\nCode and Data for the Social Sciences: A Practitioner’s Guide, by Gentzkow and Shapiro"
  },
  {
    "objectID": "morepages/econ452.html",
    "href": "morepages/econ452.html",
    "title": "Econ 452: Course Outline",
    "section": "",
    "text": "Md Moshi Ul Alam\n      Assistant Professor of Economics\n      \n        Clark University\n      \n      \n        \n          \n          Email\n        \n        \n          \n          CV\n        \n        \n          \n        \n        \n          \n        \n      \n    \n  \n\n\n\n\n\n\n\n\nOutline\n\nPotential Outcomes Framework (1 lecture)\nRandomized Control Trials (1.5 lectures)\nRegressions (1.5 lectures)\nNon-linear regressions (2 lectures)\nInstrumental Variables (5 lectures)\nHeterogeneous Treatment Effects (1 lecture)\nPanel data and Fixed Effects (3 lectures)\nDifference-in-Differences (5 lectures)\nRegression Discontinuity (2 lectures)\nInference (1 lecture)\nTake away from the course (1 lecture)\n\n\n\nAssessment\n\n10% Discussion and class participation\n30% 5 Problem sets\n60% Group project"
  },
  {
    "objectID": "blogs/namingconventions.html",
    "href": "blogs/namingconventions.html",
    "title": "Naming conventions",
    "section": "",
    "text": "Md Moshi Ul Alam\n      Assistant Professor of Economics\n      \n        Clark University\n      \n      \n        \n          \n          Email\n        \n        \n          \n          CV\nChoosing clear, concise, and meaningful names for variables, functions, files, etc., is challenging. Good names improve code readability and maintainability, but finding the right name can be surprisingly difficult, especially when your project folder is overflowing with files and subfolders. Good names can help users of your package or code replication kit.\nI will describe here a file naming convention that works for me. Sometimes this might go against styling guides of the language used although I try as much as possible to avoid this. This may not necessarily fit everyone’s requirement as coding and project management styles vary widely. I will not cover how I name folders as that is more about project management."
  },
  {
    "objectID": "blogs/namingconventions.html#footnotes",
    "href": "blogs/namingconventions.html#footnotes",
    "title": "Naming conventions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis quote is widely attributed to Phil Karlton, a software engineer known for his work at Netscape.↩︎"
  },
  {
    "objectID": "blogs/versioncontrol.html",
    "href": "blogs/versioncontrol.html",
    "title": "Version control",
    "section": "",
    "text": "Md Moshi Ul Alam\n      Assistant Professor of Economics\n      \n        Clark University\n      \n      \n        \n          \n          Email\n        \n        \n          \n          CV\nDisclaimer: There are many amazing resources all over the internet on how to version control. This by no means is exhaustive, in fact what I have shared below is the minimal setup that works for me."
  },
  {
    "objectID": "blogs/versioncontrol.html#step-1-initialize-git-in-your-project",
    "href": "blogs/versioncontrol.html#step-1-initialize-git-in-your-project",
    "title": "Version control",
    "section": "Step 1: Initialize Git in Your Project",
    "text": "Step 1: Initialize Git in Your Project\nNavigate to your project directory and initialize a Git repository:\ncd path/to/MyCoolProject/code\ngit init\nThis creates a new Git repository in the MyCoolProject/code directory."
  },
  {
    "objectID": "blogs/versioncontrol.html#step-2-create-a-.gitignore-file-in-code",
    "href": "blogs/versioncontrol.html#step-2-create-a-.gitignore-file-in-code",
    "title": "Version control",
    "section": "Step 2: Create a .gitignore File in /code",
    "text": "Step 2: Create a .gitignore File in /code\nTo exclude certain files from being tracked by Git, create a .gitignore file in the /code directory. Suppose you want to ignore all files with extension .bak and the folder called temp.\n# Ignore temporary files\n*.bak\n/temp/\nAlthough you can create multiple .gitignore within subfolders. I typically prefer having a single gitignore in the main directory and then use relative paths to ensure which files and folders I want git to ignore."
  },
  {
    "objectID": "blogs/versioncontrol.html#step-3-connect-to-a-remote-repository-github",
    "href": "blogs/versioncontrol.html#step-3-connect-to-a-remote-repository-github",
    "title": "Version control",
    "section": "Step 3: Connect to a Remote Repository (GitHub)",
    "text": "Step 3: Connect to a Remote Repository (GitHub)\nI host my version controlled files on GitHub.\n\nCreate a new repository on GitHub\nAdd the remote URL to your local Git repository:\n\ngit remote add origin https://github.com/YourUsername/MyCoolProject-code.git"
  },
  {
    "objectID": "blogs/versioncontrol.html#step-4-stage-and-commit-your-files",
    "href": "blogs/versioncontrol.html#step-4-stage-and-commit-your-files",
    "title": "Version control",
    "section": "Step 4: Stage and Commit Your Files",
    "text": "Step 4: Stage and Commit Your Files\n\n\n\n\n\n\nConfirm working branch\n\n\n\n\n\nEnsure you are on the same branch as your primary branch in your repo. In my first attempt my primary branch in the repo was main while that on VSCode was master. The solution to this is a careful force merge of the two branches:\ngit checkout main\ngit merge master --allow-unrelated-histories\n\n\n\nAfter setting up the remote repository, you can stage and commit your files. Itypically do this on the fly using VSCode default GUI.\n\nClick on the Source Control icon in the left sidebar in VSCode.\nYou should see all your files listed as untracked. Click on the + icon next to each file to stage them, or click + next to Changes to stage all files.\nEnter a commit message in the text box at the top.\nClick on the checkmark icon to commit your staged changes.\nPush your changes to the remote repository."
  },
  {
    "objectID": "blogs/versioncontrol.html#step-5-track-changes-and-collaborate",
    "href": "blogs/versioncontrol.html#step-5-track-changes-and-collaborate",
    "title": "Version control",
    "section": "Step 5: Track Changes and Collaborate",
    "text": "Step 5: Track Changes and Collaborate\nNow that your /code folder is under version control and hosted on GitHub, you can track changes, create branches, merge changes, and collaborate with others. VSCode provides a powerful interface for managing Git repositories, making it easier to review changes and resolve conflicts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Moshi Alam",
    "section": "",
    "text": "Md Moshi Ul Alam\n      Assistant Professor of Economics\n      \n        Clark University\n      \n      \n        \n          \n          Email\n        \n        \n          \n          CV\n        \n        \n          \n        \n        \n          \n        \n      \n    \n  \n\n\n\n\n\n\nHi!😊 I am a labor economist. My research fields are Labor Economics, Public Economics and Applied Econometrics. In my work, I use both design-based (reduced form) and model-based (structural) methods, and at times I design surveys to collect data with identifying variation. I received my PhD in Economics, from the University of Wisconsin-Madison.\n\n\n\n\n\n\n Working Papers\n\n\n\n1. Employee-Side Discrimination: Beliefs and Preferences\n\n\nwith Mehreen Mookerjee and Sanket Roy\n\n\nR&R at Quantitative Economics PhD JMP \n\n\nAbstract Online Appendix A-C\n\n\n\n\n2. The Unintended Benefits of Women’s Empowerment on Household Sanitation\n\n\nwith Monica Agarwal\n\n\nR&R at Journal of Human Resources \n\n\nAbstract SSRN\n\n\n\n\n3. Labor Market Consequences of Pay-Equity Laws\n\n\nwith Steven Lehrer and Nuno Souso Pereira\n\n\nUnder Review  Post-doc JMP\n\n\nAbstract\n\n\n\n\n4. Computing Optimal Place‑Based Transfers with Direct Measurement of Geographic Variation in Marginal Utility of Consumption\n\n\nwith Morris Davis and Jesse Gregory\n\n\nDraft Coming Soon\n\n\n\n\n\n Publications\n\n\n\nIncreases in shared custody after divorce in the United States\n\n\nwith Daniel Meyer and Marcia Carlson\n\n\n Demographic Research 46(38) 2022: 1137-1162 ⭐ Editor’s Choice Article\n\n\nAbstract Replication\n\n\n\n\n\n Research in Progress\n\n\n\nSpatial Inequality and School Choice Mechanisms\n\n\nwith Monica Agarwal, Chao Fu, and YingHua He\n\n\n\n\nLabor Market Inequality and Collective Bargaining\n\n\nwith Steven Lehrer and Nuno Souso Pereira\n\n\n\n\nRacial Gaps in Wage Growth: Discrimination, Selection and Search Frictions\n\n\n\n\n\n\n\nDormant Papers\n\n\n Click to view dormant papers\n\n\n\n\nIdentification in Models of Discrimination\n\n\n[Subsumed in PhD job market paper “Employee Side Discrimination: Beliefs and Preferences”]\n\n\n\n\n\n\n\n\n\nCode\nmicro = require(\"micromodal@0.4.10\");\n\nmicro.init({\n  awaitOpenAnimation: true,\n  awaitCloseAnimation: true\n});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIncreases in shared custody after divorce in the United States\nwith Daniel Meyer and Marcia Carlson\n\n\nThis paper provides new evidence on the time trend in shared physical custody after divorce in the U.S., using eight waves of data from the Current Population Survey - Child Support Supplement. We find that the likelihood of shared custody more than doubled between divorces that occurred before 1985 and those in 2010-2014, from 12% to 28%. We show that non-Hispanic Whites and those who are more socioeconomically advantaged are more likely to have shared custody. Using more formal methods we show that the increase cannot be explained by changes in the characteristics of those divorcing; instead, we infer that this is the result of changing norms and policies that favor shared custody. Finally, this paper complements previous analyses using court record data from Wisconsin and shows that while the rate of shared custody in Wisconsin is higher than the national rate, a large increase over time has occurred in the nation as well as in Wisconsin. These changing patterns have important implications for children’s living arrangements and for the parental investments that children receive after their parents’ divorce.\n\n\n\n\n\n\n\n\n\n\n\nLabor Market Consequences of Pay-Equity Laws\nwith Steven Lehrer and Nuno Souso Pereira\n\n\nLimited policy variation, data constraints in defining ‘equal work’, and compliance issues have prevented rigorous causal analysis of pay equity laws. We address these challenges using Portugal’s 2018 legislation, which penalized firms with over 250 employees for gender wage gaps exceeding 5%. Using administrative data that links employees to industry-defined job titles within firms, we analyze its impact both within and between genders using an event study design. We find the law achieved its intended effects in some areas while generating unintended consequences in others. Jobs with initial gaps exceeding 5% saw a 9% reduction, mainly through slower male wage growth. A small fraction of jobs with negative gaps saw reduced female wage growth, closing the gap by half. However, gaps in jobs initially between 0-5% unexpectedly widened by 21% due to slower female wage growth. These unintended consequences are more pronounced in male-dominated industries and cannot be explained by productivity differences. Further, we find that firms did not change their size to evade the law, nor did it impact job gender composition, or hours worked. Our findings reveal both intended and unintended consequences of pay equity laws implemented with uniform regulatory targets: while the 5% target effectively reduced large disparities, it inadvertently widened gaps below the threshold as firms, with enforcement rules now clarified, strategically adjusted wages but not employment, offering crucial insights for policy design in achieving “equal pay for equal work”.\n\n\n\n\n\n\n\n\n\n\n\nEmployee-Side Discrimination: Beliefs and Preferences\nwith Mehreen Mookerjee and Sanket Roy\n\n\nWorkers’ preferences and beliefs shape labor market outcomes, yet remain understudied. We develop a theory-driven novel identification strategy for information experiments that separates preferences from beliefs. We apply this to provide the first evidence on the distribution of workers’ preferences on manager gender and their beliefs on managers’ mentoring ability. In the absence of information on manager mentoring ability, workers are indifferent to manager gender. However, upon receiving information on manager mentorship ability, workers prefer to work for female managers—willing to forgo 1.3–2.2% of average annual wages. Hence, absent additional information, workers believe female managers are worse mentors (1.6% wage equivalent). Non-parametric estimates of the distributions reveal that 75% prefer female managers with mentoring information, and 67% believe male managers are better mentors absent information. Machine learning algorithms show that individuals with higher education are less likely to hold such beliefs. These beliefs are primarily driven by perceptions that women are less competent. Evidence suggests these beliefs are likely biased, highlighting scope for information-based policy interventions that could reduce suboptimal matching and gender gaps in management levels. Our identification-driven design provides a general framework for information experiments to study beliefs.\n\n\n\n\n\n\n\n\n\n\n\nThe Unintended Benefits of Women’s Empowerment on Household Sanitation\nwith Monica Agarwal\n\n\nExisting research shows that women benefit more from private toilets, but misperceptions about the net benefits from toilets and lack of women’s decision-making power can hinder toilet adoption by households. In this paper, we explore a novel link between household sanitation and policies that empower women. We show that a policy aimed at improving women’s property inheritance rights in India led to an increase in toilet adoption in the households of treated cohorts by at least 10%. Prior literature shows mixed evidence on whether the policy increased women’s inheritance, but shows that the policy had significant indirect effects, such as improving women’s education. To generate empirical tests for the mechanisms driving our main results, we build a discrete choice model with idiosyncratic household preference shocks that produces policy-relevant complementarity between women’s education and decision-making power in adoption of a household public good valued more by women. Using a heterogeneity-robust event-study design, we find that, consistent with our model, the increase in toilet adoption is concentrated in states where the policy boosted women’s education—plausibly reducing misperceptions about the benefits of toilets—and increased women’s decision-making power. Our findings highlight that policies empowering women can yield unintended benefits beyond their original scope—while we document improvements in toilet coverage, the implications extend to other household investments where women’s preferences are stronger, but various frictions limit adoption."
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Moshi Alam",
    "section": "",
    "text": "Md Moshi Ul Alam\n      Assistant Professor of Economics\n      \n        Clark University\n      \n      \n        \n          \n          Email\n        \n        \n          \n          CV"
  },
  {
    "objectID": "links.html#software-engineering-for-economists",
    "href": "links.html#software-engineering-for-economists",
    "title": "Moshi Alam",
    "section": "Software engineering for economists",
    "text": "Software engineering for economists\n\nVersion control: GitHub and Git\n\nfrom quantecon\nfrom Tilburg Science\n\n\n\nUnit testing\n\nfrom quantecon\n\n\n\nContinuous integration"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#prerequisites",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#prerequisites",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nR and RStudio is installed\nUpdate R packages regularly\nRequired packages for today:\n\nBase R (primary focus)\n\n\nToday’s Agenda\n\nLive coding\nFocus on typing commands yourself\nAvoid copy-paste to build muscle memory\n\n\n\n\n\n\n\nNote\n\n\nThis is not a course in R!"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#rstudio-interface",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#rstudio-interface",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "RStudio Interface",
    "text": "RStudio Interface\n\nRStudio Interface\nExplain each pane’s function: - Source editor (top left) - Console (bottom left) - Environment (top right) - Files/Plots/Help (bottom right)"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#basic-arithmetic",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#basic-arithmetic",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Basic Arithmetic",
    "text": "Basic Arithmetic\n\n# Addition\n1 + 2\n\n# Subtraction\n6 - 7\n\n# Division\n5 / 2\n\n# Exponentiation\n2^3\n\n# Integer division\n100 %/% 60  # How many whole hours in 100 minutes?\n\n# Modulo (remainder)\n100 %% 60   # How many minutes are left over?"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#logic-operations",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#logic-operations",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Logic Operations",
    "text": "Logic Operations\n\n\n\n# Basic comparisons\n1 &gt; 2\n1 &gt; 2 & 1 &gt; 0.5  # AND\n1 &gt; 2 | 1 &gt; 0.5  # OR\n\n# Truth testing\nisTRUE(1 &lt; 2)\n\n\nImportant Operators:\n\n&gt;, &lt;: Greater/less than\n&gt;=, &lt;=: Greater/less equal\n==: Equality test\n!=: Not equal\n&: AND\n|: OR"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#logic-important-details",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#logic-important-details",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Logic: Important Details",
    "text": "Logic: Important Details\n\n\nOrder of Precedence:\n\nLogical operators (&gt;, &lt;, etc.)\nBoolean operators (&, |)\n\n\n# Be explicit!\n1 &gt; 0.5 & 1 &gt; 2\n\n# Not\n1 &gt; 0.5 & 2  # Can be confusing\n\n\nFloating Point Numbers:\n\n# This returns FALSE!\n0.1 + 0.2 == 0.3\n\n# Use instead:\nall.equal(0.1 + 0.2, 0.3)"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#value-matching",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#value-matching",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Value Matching",
    "text": "Value Matching\n\n# Check if value is in a vector\n4 %in% 1:10\n\n# Create a \"not in\" operator\n`%ni%` = Negate(`%in%`)\n4 %ni% 5:10\n\n\nThe backticks (`) help specify functions"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#everything-is-an-object",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#everything-is-an-object",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Everything is an Object",
    "text": "Everything is an Object\nCommon object types in R:\n\nVectors\nMatrices\nData frames\nLists\nFunctions\n\n\n# Create different objects\nvec &lt;- 1:5\nmat &lt;- matrix(1:9, nrow=3)\ndf &lt;- data.frame(x=1:3, y=4:6)"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#objects-have-classes",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#objects-have-classes",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Objects Have Classes",
    "text": "Objects Have Classes\n\n\n\n# Create data frame\nd &lt;- data.frame(\n  x = 1:2,\n  y = 3:4\n)\n\n# Check properties\nclass(d)\ntypeof(d)\nstr(d)\n\n\nUnderstanding Objects:\n\nclass(): Object’s class\ntypeof(): Object’s type\nstr(): Object’s structure\nView(): Interactive viewing"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#global-environment",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#global-environment",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Global Environment",
    "text": "Global Environment\n\n\n\n# Create data frame\nd &lt;- data.frame(\n  x = 1:2,\n  y = 3:4\n)\n\n# This fails:\nlm(y ~ x)\n\n# This works:\nlm(y ~ x, data = d)\n\n\nKey Points:\n\nObjects live in the global environment\nMust specify data source\nDifferent from Stata’s single-dataset approach\nMultiple objects can exist simultaneously"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#assignment-operators",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#assignment-operators",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Assignment Operators",
    "text": "Assignment Operators\n\n\nUsing Arrow (&lt;-):\n\n# Standard assignment\na &lt;- 10 + 5\n\n# Right assignment\n10 + 5 -&gt; a\n\nEmbodies the idea of assigned to\n\nUsing Equals (=):\n\n# Alternative assignment\nb = 10 + 10\n\n# Must be on left with =\n# This won't work:\n# 10 + 10 = b\n\n\nAssignment Choice\n\nMost R users prefer &lt;- for assignment\n= has specific role in function evaluation\nPersonal choice, but be consistent\n= is quicker to type and familiar from other languages"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#variables-and-assignment",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#variables-and-assignment",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Variables and Assignment",
    "text": "Variables and Assignment\n\n# Variable assignment\nx &lt;- 1\ny &lt;- \"roses\"\nz &lt;- function(x) { sqrt(x) }\n\n# Using variables\nz(9)\n\n[1] 3"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#control-flow-ifelse",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#control-flow-ifelse",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Control Flow: if/else",
    "text": "Control Flow: if/else\n\nx &lt;- 5\ny &lt;- 3\n\nif (x &gt; y) {\n  print(\"x is larger than y\")\n} else {\n  print(\"x is less than or equal to y\")\n}\n\n[1] \"x is larger than y\""
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#loops",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#loops",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Loops",
    "text": "Loops\n\n# For loop\nfor(i in 1:5) {\n  print(paste(\"Iteration\", i))\n}\n\n[1] \"Iteration 1\"\n[1] \"Iteration 2\"\n[1] \"Iteration 3\"\n[1] \"Iteration 4\"\n[1] \"Iteration 5\"\n\n# Loop with conditions\nfor(fruit in c(\"mangos\",\"bananas\",\"apples\")) {\n  print(paste(\"I love\", fruit))\n}\n\n[1] \"I love mangos\"\n[1] \"I love bananas\"\n[1] \"I love apples\""
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#functions",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#functions",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Functions",
    "text": "Functions\n\n# Define function\ngreet &lt;- function(name = \"Lord Vader\") {\n  paste(\"Hello,\", name)\n}\n\n# Use function\ngreet()\n\n[1] \"Hello, Lord Vader\"\n\ngreet(\"Luke\")\n\n[1] \"Hello, Luke\""
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#creating-variables",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#creating-variables",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Creating Variables",
    "text": "Creating Variables\n\n# Price of a good\nprice &lt;- 50\n\n# Quantity sold\nquantity &lt;- 10\n\n# Calculate revenue\nrevenue &lt;- price * quantity\nrevenue\n\n[1] 500"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#multiple-assignments",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#multiple-assignments",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Multiple Assignments",
    "text": "Multiple Assignments\n\n# Assign same value to multiple variables\nx &lt;- y &lt;- 5\n\n# Check values\nx\n\n[1] 5\n\ny\n\n[1] 5"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#numeric-data",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#numeric-data",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Numeric Data",
    "text": "Numeric Data\n\n# GDP growth rate\ngdp_growth &lt;- 2.5\nclass(gdp_growth)\n\n[1] \"numeric\"\n\n# Population (whole number)\npopulation &lt;- 1000L  # L makes it integer\nclass(population)\n\n[1] \"integer\""
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#text-and-logical-data",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#text-and-logical-data",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Text and Logical Data",
    "text": "Text and Logical Data\n\n# Country name (character/string)\ncountry &lt;- \"United States\"\nclass(country)\n\n[1] \"character\"\n\n# Logical (TRUE/FALSE)\nis_developed &lt;- TRUE\nclass(is_developed)\n\n[1] \"logical\""
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#reserved-words",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#reserved-words",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Reserved Words",
    "text": "Reserved Words\n\n\nStrictly Reserved:\n\nif\nelse\nwhile\nfunction\nfor\nTRUE/FALSE\nNULL\nInf\nNA\n\n\nSemi-Reserved:\n\nc() (concatenate)\npi\nMany function names\n\n\n# Don't do this!\npi &lt;- 2\nc &lt;- 4"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#namespace-conflicts",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#namespace-conflicts",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Namespace Conflicts",
    "text": "Namespace Conflicts\n\nlibrary(dplyr)\n\n# Shows conflicts:\n# filter masked from 'package:stats'\n# lag masked from 'package:stats'\n\nTwo solutions:\n\nUse package::function()\n\n\nstats::filter(1:10, rep(1, 2))\n\n\nAssign permanently\n\n\nfilter &lt;- stats::filter"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#using-square-brackets",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#using-square-brackets",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Using Square Brackets",
    "text": "Using Square Brackets\n\n\nBasic Indexing:\n\n# Vector indexing\na &lt;- 1:10\na[4]        # 4th element\na[c(4, 6)]  # 4th and 6th\n\n# Matrix/dataframe\nd[1, 1]     # First row & column\n\n\nList Indexing:\n\nmy_list &lt;- list(\n  a = \"hello\",\n  b = 1:3\n)\n\nmy_list[[1]]     # First element\nmy_list[[2]][3]  # Third item of second element"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#using-dollar-sign",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#using-dollar-sign",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Using Dollar Sign",
    "text": "Using Dollar Sign\n\n# List example\nmy_list$a           # Access 'a' element\nmy_list$b[3]        # Third item of 'b'\n\n# Data frame\nstarwars$name[1]    # First name\n\n\n$ provides direct access to named elements"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#removing-objects",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#removing-objects",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Removing Objects",
    "text": "Removing Objects\n\n# Remove specific objects\nrm(a, b)\n\n# Remove all objects (not recommended)\nrm(list = ls())\n\n# Detach package\ndetach(package:dplyr)\n\n# Clear plots\ndev.off()\n\n\n\n\n\n\n\nTip\n\n\nBetter to restart R session than use rm(list = ls())"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#overview-of-data-structures",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#overview-of-data-structures",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Overview of Data Structures",
    "text": "Overview of Data Structures\nR has several basic data structures:\n\n\n\nDimension\nHomogeneous\nHeterogeneous\n\n\n\n\n1\nVector\nList\n\n\n2\nMatrix\nData Frame\n\n\n3+\nArray\nnested Lists"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#vectors",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#vectors",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Vectors",
    "text": "Vectors\nVectors are containers for objects of identical type:\n\n# Create vectors with c()\nx &lt;- c(1, 3, 5, 7, 8, 9)\nprint(x)\n\n[1] 1 3 5 7 8 9\n\n# Create sequence\ny &lt;- 1:10\nprint(y)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n# Repeat values\nrep(\"A\", times = 5)\n\n[1] \"A\" \"A\" \"A\" \"A\" \"A\""
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#vector-operations",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#vector-operations",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Vector Operations",
    "text": "Vector Operations\n\n# Subsetting vectors\nx &lt;- c(1, 3, 5, 7, 8, 9)\nx[1]      # First element\n\n[1] 1\n\nx[1:3]    # First three elements\n\n[1] 1 3 5\n\nx[c(1,3,4)] # Selected elements\n\n[1] 1 5 7"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#vector-logic",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#vector-logic",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Vector Logic",
    "text": "Vector Logic\n\nx &lt;- c(1, 3, 5, 7, 8, 9)\n# Logical operations\nx &gt; 3\n\n[1] FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n\nx[x &gt; 3]  # Subsetting with logic\n\n[1] 5 7 8 9\n\nsum(x &gt; 3) # Count values &gt; 3\n\n[1] 4"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#matrices",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#matrices",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Matrices",
    "text": "Matrices\nTwo-dimensional arrays with same data type:\n\n# Create matrix\nX &lt;- matrix(1:9, nrow = 3, ncol = 3)\nprint(X)\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n# Matrix by rows\nY &lt;- matrix(1:9, nrow = 3, ncol = 3, byrow = TRUE)\nprint(Y)\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#matrix-operations",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#matrix-operations",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Matrix Operations",
    "text": "Matrix Operations\n\n# Matrix arithmetic\nX + Y  # Element-wise addition\n\n     [,1] [,2] [,3]\n[1,]    2    6   10\n[2,]    6   10   14\n[3,]   10   14   18\n\nX * Y  # Element-wise multiplication\n\n     [,1] [,2] [,3]\n[1,]    1    8   21\n[2,]    8   25   48\n[3,]   21   48   81\n\nX %*% Y  # Matrix multiplication\n\n     [,1] [,2] [,3]\n[1,]   66   78   90\n[2,]   78   93  108\n[3,]   90  108  126\n\n# Subsetting\nX[1, 2]  # Element at row 1, column 2\n\n[1] 4\n\nX[1, ]   # First row\n\n[1] 1 4 7\n\nX[, 1]   # First column\n\n[1] 1 2 3"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#creating-lists",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#creating-lists",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Creating Lists",
    "text": "Creating Lists\nLists can contain elements of different types:\n\n# Create a list\nex_list &lt;- list(\n  a = c(1, 2, 3, 4),\n  b = TRUE,\n  c = \"Hello!\",\n  d = matrix(1:4, 2, 2)\n)\nprint(ex_list)\n\n$a\n[1] 1 2 3 4\n\n$b\n[1] TRUE\n\n$c\n[1] \"Hello!\"\n\n$d\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#list-operations",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#list-operations",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "List Operations",
    "text": "List Operations\n\n# Access list elements\nex_list$a  # Using $\n\n[1] 1 2 3 4\n\nex_list[[1]]  # Using [[]]\n\n[1] 1 2 3 4\n\nex_list[\"a\"]  # Using []\n\n$a\n[1] 1 2 3 4\n\n# Add new elements\nex_list$e &lt;- \"New element\""
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#installing-packages",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#installing-packages",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Installing Packages",
    "text": "Installing Packages\n\n# Install a package\ninstall.packages(\"tidyverse\")\n\n# Load the package\nlibrary(tidyverse)\n\n\n\n\n\n\n\nNote\n\n\nYou only need to install a package once, but you need to load it each session"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#getting-help",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#getting-help",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Getting Help",
    "text": "Getting Help\n\n\nBasic Help:\n\n# Full help\nhelp(plot)\n\n# Shorthand\n?plot\n\n# Examples\nexample(plot)\n\n\nPackage Help:\n\n# Package vignettes\nvignette(\"dplyr\")\n\n# List all vignettes\nvignette(all = FALSE)\n\n# Package demos\ndemo(package = \"graphics\")"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#introduction-to-data-frames",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#introduction-to-data-frames",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Introduction to Data Frames",
    "text": "Introduction to Data Frames\nData frames are table-like structures:\n\n# Create a data frame\ndf &lt;- data.frame(\n  id = 1:3,\n  name = c(\"John\", \"Jane\", \"Bob\"),\n  score = c(85, 92, 78)\n)\nprint(df)\n\n  id name score\n1  1 John    85\n2  2 Jane    92\n3  3  Bob    78"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#data-frame-operations",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#data-frame-operations",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Data Frame Operations",
    "text": "Data Frame Operations\n\n# Basic operations\nnames(df)  # Column names\n\n[1] \"id\"    \"name\"  \"score\"\n\nnrow(df)   # Number of rows\n\n[1] 3\n\nncol(df)   # Number of columns\n\n[1] 3\n\n# Access columns\ndf$name\n\n[1] \"John\" \"Jane\" \"Bob\" \n\ndf[[\"score\"]]\n\n[1] 85 92 78"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#subsetting-data-frames",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#subsetting-data-frames",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Subsetting Data Frames",
    "text": "Subsetting Data Frames\n\n# Subset rows\ndf[df$score &gt; 80, ]\n\n  id name score\n1  1 John    85\n2  2 Jane    92\n\n# Subset columns\ndf[, c(\"name\", \"score\")]\n\n  name score\n1 John    85\n2 Jane    92\n3  Bob    78\n\n# Using subset()\nsubset(df, score &gt; 80, select = c(\"name\", \"score\"))\n\n  name score\n1 John    85\n2 Jane    92"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#additional-resources",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#additional-resources",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nR Documentation: r-project.org\nRStudio Cheatsheets: rstudio.com/resources/cheatsheets\nAdvanced R by Hadley Wickham: adv-r.hadley.nz\nStack Overflow R Tag: stackoverflow.com/questions/tagged/r"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/01_introRv2.html#practice-on-your-own",
    "href": "teachingmaterials/UG_intro_metrics/01_introRv2.html#practice-on-your-own",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Practice on your own",
    "text": "Practice on your own\n\nCreate vectors using different methods (c(), :, seq(), rep())\nPractice logical operations and understand operator precedence\nCreate a list with different types of elements and practice indexing\nLoad a package and resolve a namespace conflict\nCreate and remove objects from your environment"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/04_mlr.html#introduction",
    "href": "teachingmaterials/UG_intro_metrics/04_mlr.html#introduction",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Introduction",
    "text": "Introduction\n\nThe idea here would be extend the concepts discussed simple linear regression\nAgain, it is crucial that you make yourself comfortable with the concepts of simple linear regression before moving on here\nConsequently, we will be much faster than the previous topic\nOnce again the big picture will be to extend the assumptions revolving around \\(u_i\\) and \\(x_i\\) to multiple variables\n\nto understand how does the OLS estimator plays out"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/04_mlr.html#multiple-linear-regression",
    "href": "teachingmaterials/UG_intro_metrics/04_mlr.html#multiple-linear-regression",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\n\nPopulation model: \\(y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\ldots + \\beta_k x_{ki} + u_i\\)\n\n\\(i = 1, 2, \\ldots, n\\) indexes the observations\n\\(y_i\\) is the dependent variable,\n\\(x_{1i}, x_{2i}, \\ldots, x_{ki}\\) are the independent variables, \\(u_i\\) is the error term\n\\(\\beta_0, \\beta_1, \\ldots, \\beta_k\\) are the parameters\n\nThe key assumption will be a generalized versions in SLR \\[E(u_i|x_{1i}, x_{2i}, \\ldots, x_{ki}) = 0\\] \\[E(u_i) = 0\\]\nThese together imply: \\(E(x_{1i}u_i) = E(x_{2i}u_i) = \\ldots = E(x_{ki}u_i) = 0\\)"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/04_mlr.html#obtaining-the-ols-estimates",
    "href": "teachingmaterials/UG_intro_metrics/04_mlr.html#obtaining-the-ols-estimates",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Obtaining the OLS estimates",
    "text": "Obtaining the OLS estimates\n\nOnce again, we will choose \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_k\\) to minimize the sum of squared residuals : \\[\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_{1i} - \\hat{\\beta}_2 x_{2i} - \\ldots - \\hat{\\beta}_k x_{ki})^2\\]\n\nThis minimization problem leads to \\(k+1\\) linear equations (F.O.C.s) in \\(k+1\\) unknowns \\(\\hat{\\boldsymbol{\\beta}}_0, \\hat{\\boldsymbol{\\beta}}_1, \\ldots, \\hat{\\boldsymbol{\\beta}}_k\\) :\n\\[\n\\begin{aligned}\n& \\sum_{i=1}^n\\left(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_{i 1}-\\cdots-\\hat{\\beta}_k x_{i k}\\right)=0 \\\\\n& \\sum_{i=1}^n x_{i 1}\\left(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_{i 1}-\\cdots-\\hat{\\beta}_k x_{i k}\\right)=0 \\\\\n% & \\sum_{i=1}^n x_{i 2}\\left(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_{i 1}-\\cdots-\\hat{\\beta}_k x_{i k}\\right)=0 \\\\\n& \\vdots \\\\\n& \\sum_{i=1}^n x_{i k}\\left(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_{i 1}-\\cdots-\\hat{\\beta}_k x_{i k}\\right)=0\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/04_mlr.html#the-focs",
    "href": "teachingmaterials/UG_intro_metrics/04_mlr.html#the-focs",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "The FOCs",
    "text": "The FOCs\n\n\nThe FOCs give us \\(k+1\\) equations in \\(k+1\\) unknowns\nNote that we arrive at this because we assume that any change in \\(x_{1i}\\), \\(x_{2i}\\), \\(\\ldots\\), \\(x_{ki}\\) will not affect the error term \\(u_i\\)\nGives us the sample counterparts of \\(k+1\\) moment conditions\n\nRecall we did something similar in SLR\n\nThe moment conditions are given by: \\[\\begin{aligned}\nE(u_i) &= 0 \\\\\nE(x_{1i}u_i) &= 0 \\\\\nE(x_{2i}u_i) &= 0 \\\\\n\\vdots \\\\\nE(x_{ki}u_i) &= 0\n\\end{aligned}\\]\nNote that we arrive at this because we assume that any change in \\(x_{1i}\\), \\(x_{2i}\\), \\(\\ldots\\), \\(x_{ki}\\) will not affect the error term \\(u_i\\)"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/04_mlr.html#section",
    "href": "teachingmaterials/UG_intro_metrics/04_mlr.html#section",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "",
    "text": "library(wooldridge)\nwage_data &lt;- wage1\n# run a regression of log(wage) on educ, exper, and tenure\nreg1 &lt;- lm(log(wage) ~ educ + exper + tenure, data = wage_data)\nsummary(reg1)\n\n\nCall:\nlm(formula = log(wage) ~ educ + exper + tenure, data = wage_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.05802 -0.29645 -0.03265  0.28788  1.42809 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.284360   0.104190   2.729  0.00656 ** \neduc        0.092029   0.007330  12.555  &lt; 2e-16 ***\nexper       0.004121   0.001723   2.391  0.01714 *  \ntenure      0.022067   0.003094   7.133 3.29e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4409 on 522 degrees of freedom\nMultiple R-squared:  0.316, Adjusted R-squared:  0.3121 \nF-statistic: 80.39 on 3 and 522 DF,  p-value: &lt; 2.2e-16\n\n\n\nkeeping educ fixed, what is the effect of changing exper and tenure by one year simultaneously?"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/04_mlr.html#section-1",
    "href": "teachingmaterials/UG_intro_metrics/04_mlr.html#section-1",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "",
    "text": "# keeping educ fixed, what is the effect of changing exper and tenure by one year simultaneously?\n\nreg1$coefficients[3] + reg1$coefficients[4]\n\n     exper \n0.02618833 \n\n\n\nIt returns an object of type Named num\nAlso not easy to read code\nLet us be complete and verbose\n\n\nbeta_exper &lt;- reg1$coefficients[3] \nbeta_tenure &lt;-  reg1$coefficients[4] \ntotal_effect &lt;- as.numeric(beta_exper + beta_tenure)\ntotal_effect\n\n[1] 0.02618833"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/04_mlr.html#residual-and-fitted-values",
    "href": "teachingmaterials/UG_intro_metrics/04_mlr.html#residual-and-fitted-values",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Residual and fitted values",
    "text": "Residual and fitted values\n\n\n\n\nThe fitted/predicted values are given by: \\[\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{1i} + \\hat{\\beta}_2 x_{2i} + \\ldots + \\hat{\\beta}_k x_{ki}\\]\nThe residuals are given by: \\[\\hat{u}_i = y_i - \\hat{y}_i\\] \\[= y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_{1i} - \\hat{\\beta}_2 x_{2i} - \\ldots - \\hat{\\beta}_k x_{ki}\\]\n\n\n\n# Fitted values\nfitted_values &lt;- reg1$fitted.values\nhead(fitted_values)\n\n       1        2        3        4        5        6 \n1.304921 1.523506 1.304921 1.819802 1.461690 1.970451 \n\n# Residuals\nresiduals &lt;- reg1$residuals\nhead(residuals)\n\n          1           2           3           4           5           6 \n-0.17351855 -0.34793290 -0.20630834 -0.02804287  0.20601726  0.19860264 \n\n\n\n# manual residuals\nobserved_wage &lt;- log(wage_data$wage)\nmanual_residuals &lt;- observed_wage - fitted_values\n# test if manual residuals are approximately the same as residuals\nall.equal(residuals, manual_residuals)\n\n[1] TRUE"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/04_mlr.html#properties-of-fitted-values-and-residuals",
    "href": "teachingmaterials/UG_intro_metrics/04_mlr.html#properties-of-fitted-values-and-residuals",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Properties of fitted values and residuals",
    "text": "Properties of fitted values and residuals\n\nImmediate extension of SLR and implications from the moment conditions:\n\nSample average of residuals is zero: \\(\\sum_{i=1}^n \\hat{u}_i = 0\\)\nSample covariance between residuals and each of the independent variables is zero: \\(\\sum_{i=1}^n x_{1i} \\hat{u}_i = 0\\) \\(\\sum_{i=1}^n x_{2i} \\hat{u}_i = 0\\) \\(\\vdots\\) \\(\\sum_{i=1}^n x_{ki} \\hat{u}_i = 0\\)\nThe point \\((\\bar{x}_1, \\bar{x}_2, \\ldots, \\bar{x}_k, \\bar{y})\\) lies on the regression plane \\[\\bar{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x}_1 + \\hat{\\beta}_2 \\bar{x}_2 + \\ldots + \\hat{\\beta}_k \\bar{x}_k\\]\nTry them out in R"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/04_mlr.html#partialing-out-interpretation",
    "href": "teachingmaterials/UG_intro_metrics/04_mlr.html#partialing-out-interpretation",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Partialing out interpretation",
    "text": "Partialing out interpretation\nConsider a population model with k=2, i.e., two independent variables.\n\\[y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + u_i\\]\nLet us focus on \\(\\beta_1\\)\n\n# run a regression of log(wage) on educ, exper\nreg_wage_on_educ_exp &lt;- lm(log(wage) ~ educ + exper, data = wage_data)\nsummary(reg_wage_on_educ_exp)\n\n\nCall:\nlm(formula = log(wage) ~ educ + exper, data = wage_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.05800 -0.30136 -0.04539  0.30601  1.44425 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.216854   0.108595   1.997   0.0464 *  \neduc        0.097936   0.007622  12.848  &lt; 2e-16 ***\nexper       0.010347   0.001555   6.653 7.24e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4614 on 523 degrees of freedom\nMultiple R-squared:  0.2493,    Adjusted R-squared:  0.2465 \nF-statistic: 86.86 on 2 and 523 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/04_mlr.html#section-2",
    "href": "teachingmaterials/UG_intro_metrics/04_mlr.html#section-2",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "",
    "text": "# step 1: run a regression of log(wage) on exper\nreg_logwage_on_exp &lt;- lm(log(wage) ~ exper, data = wage_data)\nresiduals_logwage_on_exp &lt;- reg_logwage_on_exp$residuals\n# step 2: run a regression of educ on exper\nreg_educ_on_exp &lt;- lm(educ ~ exper, data = wage_data)\nresiduals_educ_on_exp &lt;- reg_educ_on_exp$residuals\n# step 3: run a regression of residuals from step 1 on residuals from step 2\npartial_out_reg &lt;- lm(residuals_logwage_on_exp ~ residuals_educ_on_exp)\nsummary(partial_out_reg)\n\n\nCall:\nlm(formula = residuals_logwage_on_exp ~ residuals_educ_on_exp)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.05800 -0.30136 -0.04539  0.30601  1.44425 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           -2.192e-18  2.010e-02    0.00        1    \nresiduals_educ_on_exp  9.794e-02  7.615e-03   12.86   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.461 on 524 degrees of freedom\nMultiple R-squared:  0.2399,    Adjusted R-squared:  0.2385 \nF-statistic: 165.4 on 1 and 524 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/04_mlr.html#whats-going-on",
    "href": "teachingmaterials/UG_intro_metrics/04_mlr.html#whats-going-on",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "What’s going on?",
    "text": "What’s going on?\n\n\nThe population model is given by: \\[y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + u_i\\]\nTo get the effect of \\(x_{1i}\\) on \\(y_i\\), the MLR is equivalent to the following two-step procedure:\n\nStep 1: Regress \\(y_i\\) on \\(x_{2i}\\) and obtain the residuals \\(\\hat{e}_i\\)\n\n\\(\\hat{e}_i\\) has the variation left in \\(y_i\\) after partialling out variation from \\(x_{2i}\\)\n\nStep 2: Regress \\(x_{1i}\\) on \\(x_{2i}\\) and obtain the residuals \\(\\hat{v}_i\\)\n\n\\(\\hat{v}_i\\) has the variation left in \\(x_{1i}\\) after partialling out variation from \\(x_{2i}\\)\n\nStep 3: Regress \\(\\hat{e}_i\\) on \\(\\hat{v}_i\\)\n\nThe idea is to partial out the effect of \\(x_{2i}\\) from \\(x_{1i}\\) and \\(y_i\\)\n\nThis is the same as regressing \\(y_i\\) on \\(x_{1i}\\) and \\(x_{2i}\\)\n\nHence comes the interpretation of \\(\\beta_1\\) as the effect of \\(x_{1i}\\) on \\(y_i\\) keeping \\(x_{2i}\\) fixed\nThere is another way to approach this. See textbook 3-2f and implement it in R"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/04_mlr.html#comparison-of-simple-and-multiple-regression-estimates",
    "href": "teachingmaterials/UG_intro_metrics/04_mlr.html#comparison-of-simple-and-multiple-regression-estimates",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Comparison of Simple and Multiple Regression Estimates",
    "text": "Comparison of Simple and Multiple Regression Estimates\n\n\nThe population model is given by: \\[y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + u_i\\]\nSuppose instead we only estimate the SLR of \\(y\\) on \\(x_{1i}\\): \\[\\tilde{y_i} = \\tilde{\\beta}_0 + \\tilde{\\beta}_1 x_{1i}\\]\nSLR estimate \\(\\tilde{\\beta}_1\\) relates to the MLR estimate \\(\\hat{\\beta}_1\\) via: \\[\\tilde{\\beta}_1 = \\hat{\\beta}_1 + \\hat{\\beta}_2 \\delta_1\\] where \\(\\delta_1\\) is the slope from the regression of \\(x_{2i}\\) on \\(x_{1i}\\) from estimating: \\[x_{2i} = \\delta_0 + \\delta_1 x_{1i} + \\epsilon\\]"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/04_mlr.html#section-3",
    "href": "teachingmaterials/UG_intro_metrics/04_mlr.html#section-3",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "",
    "text": "Confounding term:\n\n\\(\\hat{\\beta}_2 \\delta_1\\): The partial effect of \\(x_{2i}\\) on \\(\\hat{y}\\) scaled by the relationship between \\(x_{2i}\\) and \\(x_{1i}\\).\n\nTwo distinct cases where \\(\\tilde{\\beta}_1 = \\hat{\\beta}_1\\):\n\nThe partial effect of \\(x_{2i}\\) on \\(\\hat{y}\\) is zero: \\(\\hat{\\beta}_2 = 0\\).\n\\(x_{1i}\\) and \\(x_{2i}\\) are uncorrelated: \\(\\delta_1 = 0\\).\n\nThus generically in a MLR of \\(y_i\\) on \\(x_{1i}, \\cdots, x_{ki}\\), the OLS estimate on the coefficient on \\(x_{1i}\\)\n\nwill be identical to one obtained from a simple regression of \\(y_i\\) on \\(x_{1i}\\)\n\niff \\(x_{1i}\\) and \\(x_{2i}, \\cdots, x_{ki}\\) are uncorrelated.\n\n\nExample of ability bias in wage regressions"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/04_mlr.html#goodness-of-fit",
    "href": "teachingmaterials/UG_intro_metrics/04_mlr.html#goodness-of-fit",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Goodness of Fit",
    "text": "Goodness of Fit\n\nThe idea of the \\(R^2\\) statistic in MLR is the same as in SLR\n\nThe proportion of the total variation in \\(y_i\\) that is explained by the independent variables in the model \\[R^2 = \\frac{SSE}{SST} = 1 - \\frac{SSR}{SST}\\], where:\n\n\\(SSE = \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2\\) is the explained sum of squares\n\\(SSR = \\sum_{i=1}^n \\hat{u}_i^2\\) is the sum of squared residuals\n\\(SST = \\sum_{i=1}^n (y_i - \\bar{y})^2\\) is the total sum of squares\n\n\nAlways between 0 and 1"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/04_mlr.html#assumptions-of-mlr",
    "href": "teachingmaterials/UG_intro_metrics/04_mlr.html#assumptions-of-mlr",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Assumptions of MLR",
    "text": "Assumptions of MLR\n\nMLR.1: Linearity: The population model \\(y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\ldots + \\beta_k x_{ki} + u_i\\) is linear in the parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_k\\)\nMLR.2: Random Sampling: The data \\((y_i, x_{1i}, x_{2i}, \\ldots, x_{ki})\\) for \\(i = 1, 2, \\ldots, n\\) are a random sample from the population\nMLR.3: No Perfect Collinearity: The X’s are not perfectly collinear\n\n\nThey are not a linear combination of one another\n\n\nMLR.4: Zero Conditional Mean: \\(E(u_i|x_{1i}, x_{2i}, \\ldots, x_{ki}) = 0\\) \n\nThis implies that \\(E(u_i) = 0\\) and \\(E(x_{ji}u_i) = 0\\) for all \\(j = 1, 2, \\ldots, k\\)\nIf this holds then the \\(x_{ji}\\) are called strictly exogeneous\n\n\n\n\n\nTheorem: Unbiasedness of the OLS estimator in MLR\n\n\nUnder Assumptions MLR.1-MLR.4, the OLS estimator is unbiased. That is, \\(E(\\hat{\\beta}_j) = \\beta_j\\) for \\(j = 0, 1, \\ldots, k\\)."
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/04_mlr.html#section-4",
    "href": "teachingmaterials/UG_intro_metrics/04_mlr.html#section-4",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "",
    "text": "We will start with the homoskedasticity assumption since it helps set some fundamental intuition.\nThis extends to heteroskedasticity as well.\nBut the math of homoskedasticity is much more straightforward and easier to follow.\nSo here we will focus on homoskedasticity.\nImplementation in practice should be heteroskedastic"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/04_mlr.html#homoskedasticity-assumption",
    "href": "teachingmaterials/UG_intro_metrics/04_mlr.html#homoskedasticity-assumption",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Homoskedasticity assumption",
    "text": "Homoskedasticity assumption\n\n\nHaving an estimate is of no use without a measureof its precision.\nSimilar to SLR, we will make an assumption on the Variance of the error term \\(u_i\\) conditional on the independent variables \\(x_{1i}, x_{2i}, \\ldots, x_{ki}\\):\nMLR.5: Homoskedasticity: \\(Var(u_i|x_{1i}, x_{2i}, \\ldots, x_{ki}) = \\sigma^2\\)\n\nThis implies that \\(Var(u_i) = \\sigma^2\\)\n\nUsing the popoulation model \\(y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\ldots + \\beta_k x_{ki} + u_i\\), we can take conditional variance of both sides to get: \\[Var(y_i|x_{1i}, x_{2i}, \\ldots, x_{ki}) = Var(u_i|x_{1i}, x_{2i}, \\ldots, x_{ki}) = \\sigma^2\\]\nWhy do we get this? Recall the formula of a variance of sums of objects.\nAgain as before we do not know \\(\\sigma^2\\) but we can estimate it using the residuals similar to SLR.\nBut first lets look at the sampling variance of the OLS estimates"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/04_mlr.html#sampling-variance-of-ols-estimator",
    "href": "teachingmaterials/UG_intro_metrics/04_mlr.html#sampling-variance-of-ols-estimator",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Sampling variance of OLS estimator",
    "text": "Sampling variance of OLS estimator\n\n\n\nTheorem: Sampling variance of OLS estimator in MLR\n\n\nUnder assumptions MLR.1-MLR.5, the sampling variance of the OLS estimate on the \\(j^{th}\\) coefficient \\(\\hat{\\beta}_j\\) is given by: \\[Var(\\hat{\\beta}_j) = \\frac{\\sigma^2}{(1 - R_j^2)\\sum_{i=1}^n (x_{ji} - \\bar{x}_j)^2}\\] for \\(j = 0, 1, \\ldots, k\\). where:\n\n\n\\(\\sigma^2\\) is the variance of the error term \\(u_i\\) conditional on the independent variables\n\\(\\bar{x}_j\\) is the sample mean of \\(x_{ji}\\)\n\\(\\sum_{i=1}^n (x_{ji} - \\bar{x}_j)^2\\) is the total sample variance of \\(x_{ji}\\)\n\\(R_j^2\\) is the \\(R^2\\) from regressing \\(x_{ji}\\) on all other independent variables and an intercept.\n\n\n\n\n\n\nObserve how each component matters in the formula and why each of them important.\nLook for connections with MLR.3"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/04_mlr.html#the-role-of-r_j2",
    "href": "teachingmaterials/UG_intro_metrics/04_mlr.html#the-role-of-r_j2",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "The role of \\(R_j^2\\)",
    "text": "The role of \\(R_j^2\\)\n\n\n\\(R_j^2\\) is the \\(R^2\\) from regressing \\(x_{ji}\\) on all other independent variables and an intercept.\nWhat happens to \\(Var(\\hat{\\beta}_j)\\) as \\(R_j^2\\) approaches 1?\nWhat does it mean when \\(R_j^2\\) is very close to 1 versus equal to 1?\n\nMulticollinearity VS Perfect Collinearity\n\nDefinition: Multicollinearity arises when independent variables in a regression model are highly correlated, making it difficult to estimate the effect of each variable accurately.\n\n\nExample: Estimating the effect of different school expenditure categories (e.g., teacher salaries, materials, athletics) on student performance.\nChallenge:\n\nWealthier schools spend more across all categories, leading to high correlations among variables.\nDifficult to estimate the partial effect of one category due to lack of variation.\n\nSolution: Consider combining correlated variables into a single index."
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/04_mlr.html#more-on-multicollinearity",
    "href": "teachingmaterials/UG_intro_metrics/04_mlr.html#more-on-multicollinearity",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "More on multicollinearity",
    "text": "More on multicollinearity\n\nConsider a 3 variable population model: \\[y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{3i} + u_i\\]\n\n\\(x_{2i}\\) and \\(x_{3i}\\) are highly correlated\n\\(x_{1i}\\) is uncorrelated with \\(x_{2i}\\) and \\(x_{3i}\\)\n\n\n\n\nThe OLS estimator will still be unbiased because MLR1-MLR4 are satisfied\nHowever, the variance of the OLS estimates of \\(\\beta_2\\) and \\(\\beta_3\\) will be very high\nThis is because the \\(R^2\\) from\n\nregressing \\(x_{2i}\\) on \\(x_{3i}\\) and \\(x_{1i}\\) will be very high\nregressing \\(x_{3i}\\) on \\(x_{2i}\\) and \\(x_{1i}\\) will be very high\n\nBut the var of \\(\\hat{\\beta}_1\\) will be unaffected. Why?\nObserve that \\(Var(\\hat{\\beta}_1) = \\frac{\\sigma^2}{(1 - R_1^2)\\sum_{i=1}^n (x_{1i} - \\bar{x}_1)^2}\\)\n\nwhat is \\(R_1^2\\) based on the information you have?"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/04_mlr.html#example-final-exam-scores",
    "href": "teachingmaterials/UG_intro_metrics/04_mlr.html#example-final-exam-scores",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Example: Final Exam Scores",
    "text": "Example: Final Exam Scores\n\nScenario: Predicting final exam scores using:\n\nKey Variable: Number of classes attended.\nOther control Variables: Cumulative GPA, SAT score, and high school performance.\n\nConcern: Other control variables are highly correlated (multicollinear).\nA useful exercise is to look at the standard errors of the OLS estimates."
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/04_mlr.html#completing-the-model",
    "href": "teachingmaterials/UG_intro_metrics/04_mlr.html#completing-the-model",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Completing the model",
    "text": "Completing the model\n\nNow thatw e have laid out the assumptions for unbiasedness, and homoskedasticity for precision\n\nWe are left to estimate the variance of the error term \\(\\sigma^2\\) to complete the model\n\nSimialr to SLR, we can estimate \\(\\sigma^2\\) using the residuals: \\[\\hat{\\sigma}^2 = \\frac{1}{n - k - 1} \\sum_{i=1}^n \\hat{u}_i^2\\]\nThe degrees of freedom is \\(n - k - 1\\) because we have \\(n\\) observations and \\(k+1\\) parameters to estimate\n\n\n\n\nTheorem:\n\n\nUnder assumptions MLR.1-MLR.5, \\(E(\\hat{\\sigma}^2) = \\sigma^2\\)"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/04_mlr.html#gauss-markov-theorem",
    "href": "teachingmaterials/UG_intro_metrics/04_mlr.html#gauss-markov-theorem",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Gauss-Markov Theorem",
    "text": "Gauss-Markov Theorem\n\n\nThe Gauss-Markov Theorem is a key result in econometrics that states that under the assumptions MLR.1-MLR.5, the OLS estimator is the best linear unbiased estimator (BLUE) of the population parameters.\nBest: The OLS estimator has the smallest variance among all linear unbiased estimators.\nLinear: The OLS estimator is a linear function of the dependent variable.\nUnbiased: The OLS estimator is unbiased.\nThe OLS estimator is the most efficient estimator among all unbiased estimators."
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/04_mlr.html#standard-errors-of-hatbeta_j",
    "href": "teachingmaterials/UG_intro_metrics/04_mlr.html#standard-errors-of-hatbeta_j",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Standard errors of \\(\\hat{\\beta}_j\\)",
    "text": "Standard errors of \\(\\hat{\\beta}_j\\)\n\n\nUnder homoskedasticity, the standard error of the OLS estimate \\(\\hat{\\beta}_j\\) is given by: \\[SE(\\hat{\\beta}_j) = \\sqrt{Var(\\hat{\\beta}_j)}\\]\nan estimate of the standard deviation of the sampling distribution of \\(\\hat{\\beta}_j\\).\na measure of the precision of the OLS estimate \\(\\hat{\\beta}_j\\).\nused to construct confidence intervals and conduct hypothesis tests.\nestimated using the formula: \\[SE(\\hat{\\beta}_j) = \\sqrt{\\frac{\\hat{\\sigma}^2}{(1 - R_j^2)\\sum_{i=1}^n (x_{ji} - \\bar{x}_j)^2}} = \\sqrt{\\frac{\\hat{\\sigma}^2}{(1 - R_j^2)\\sqrt{n} \\quad sd(x_j)}}\\]\nNotice the role of \\(R_j^2\\) and \\(n\\) in the formula."
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/04_mlr.html#recall",
    "href": "teachingmaterials/UG_intro_metrics/04_mlr.html#recall",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Recall",
    "text": "Recall\n\nRecall the example we did in class with the wage data on educ and exper\nFor the true population model satisfying MLR1-MLR4: \\[y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + u_i\\] \nSuppose we omitted \\(x_{2i}\\) and only ran the simple regression of \\(y_i\\) on \\(x_{1i}\\): \\[\\tilde{y_i} = \\tilde{\\beta}_0 + \\tilde{\\beta}_1 x_{1i}\\]\n\\(\\tilde{\\beta}_1\\) relates to \\(\\hat{\\beta}_1\\) via: \\[\\tilde{\\beta}_1 = \\hat{\\beta}_1 + \\hat{\\beta}_2 \\delta_1\\] where \\(\\delta_1\\) is the slope from the regression of \\(x_{2i}\\) on \\(x_{1i}\\) from estimating: \\[x_{2i} = \\delta_0 + \\delta_1 x_{1i} + \\epsilon\\]\nSo \\(E(\\tilde{\\beta}_1) = \\beta_1 + \\beta_2 \\delta_1\\). Bias = \\(E(\\tilde{\\beta}_1) - \\beta_1 = \\beta_2 \\delta_1\\)\nLet us generalize this to a three variable case"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/04_mlr.html#omitted-variable-bias",
    "href": "teachingmaterials/UG_intro_metrics/04_mlr.html#omitted-variable-bias",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Omitted variable bias",
    "text": "Omitted variable bias\nFor the true population modelsatisfying MLR1-MLR4: \\[y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{3i} + u_i\\] - Suppose \\(x_{3i}\\) and \\(x_{2i}\\) are uncorrelated but \\(x_{3i}\\) and \\(x_{1i}\\) are correlated - Suppose we omitted \\(x_{3i}\\) and only ran the regression of \\(y_i\\) on \\(x_{1i}\\) and \\(x_{2i}\\): \\[\\tilde{y_i} = \\tilde{\\beta}_0 + \\tilde{\\beta}_1 x_{1i} + \\tilde{\\beta}_2 x_{2i}\\]\n\n\nMight be tempting to think that \\(\\tilde{\\beta}_2\\) is unbiased and only \\(\\tilde{\\beta}_1\\) is biased\nBut both will be biased because of the induced correlation between \\(x_{1i}\\) and \\(x_{2i}\\) when \\(x_{3i}\\) is omitted\n\\(\\tilde{\\beta}_2\\) is unbiased only when\\(x_{1i}\\) and \\(x_{2i}\\) are uncorrelated. Then we can show that: \\[\\mathrm{E}\\left(\\widetilde{\\beta}_1\\right)=\\beta_1+\\beta_3 \\frac{\\sum_{i=1}^n\\left(x_{i 1}-\\bar{x}_1\\right) (x_{i 3} - \\bar{x_3})}{\\sum_{i=1}^n\\left(x_{i 1}-\\bar{x}_1\\right)^2}\\]"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/04_mlr.html#ability-bias-example",
    "href": "teachingmaterials/UG_intro_metrics/04_mlr.html#ability-bias-example",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Ability bias example",
    "text": "Ability bias example\n\nThe true population model is given by: \\[log(wage_i) = \\beta_0 + \\beta_1 educ_i + \\beta_2 exper_i + \\beta_3 ability_i + u_i\\]\nSuppose \\(exper_i\\) and \\(ability_i\\) are uncorrelated (probably not true)\n\n\n\nSuppose we omitted the variable \\(ability_i\\) and only ran the regression of \\(log(wage_i)\\) on \\(educ_i\\) and \\(exper_i\\): \\[\\tilde{log(wage_i)} = \\tilde{\\beta}_0 + \\tilde{\\beta}_1 educ_i + \\tilde{\\beta}_2 exper_i\\]\nBoth \\(\\tilde{\\beta}_1\\) and \\(\\tilde{\\beta}_2\\) will be biased\n\nEven if we assume suppose \\(exper_i\\) and \\(ability_i\\) are uncorrelated\n\nThe bias in \\(\\tilde{\\beta}_2\\) is due to the induced correlation between \\(educ_i\\) and \\(exper_i\\) when \\(ability_i\\) is omitted because \\(educ_i\\) and \\(ability_i\\) are correlated\nConfounding the effect of ability. Direction of the bias?\nSo imagine the further complexity if we assumed all variables were pairwise correlated."
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/04_mlr.html#variances-in-misspecified-models",
    "href": "teachingmaterials/UG_intro_metrics/04_mlr.html#variances-in-misspecified-models",
    "title": "Econ 265: Introduction to Econometrics",
    "section": "Variances in misspecified models",
    "text": "Variances in misspecified models\n\nWe saw what happens to bias. What happens to precision?\nTrue model: \\(y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + u_i\\) and \\(x_{1i}\\) and \\(x_{2i}\\) are correlated\nEstimate two models:\n\nModel 1: \\(\\hat{y_i} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{1i} + \\hat{\\beta}_2 x_{2i}\\)\nModel 2: \\(\\tilde{y_i} = \\tilde{\\beta}_0 + \\tilde{\\beta}_1 x_{1i}\\)\n\n\n\n\nWhen \\(\\beta_2 \\neq 0, \\widetilde{\\beta}_1\\) is biased, \\(\\hat{\\beta}_1\\) is unbiased, and \\(\\operatorname{Var}\\left(\\widetilde{\\beta}_1\\right)&lt;\\operatorname{Var}\\left(\\hat{\\beta}_1\\right)\\)\nWhen \\(\\beta_2=0, \\widetilde{\\beta}_1\\) and \\(\\hat{\\beta}_1\\) are both unbiased, and \\(\\operatorname{Var}\\left(\\widetilde{\\beta}_1\\right)&lt;\\operatorname{Var}\\left(\\hat{\\beta}_1\\right)\\)\n\n\nThis is because (recall the formula for the variance of the OLS estimator):\n\n\\(\\operatorname{Var}\\left(\\hat{\\beta}_1\\right)=\\sigma^2 /\\left[\\operatorname{SST}_1\\left(1-R_1^2\\right)\\right]\\)\n\\(\\operatorname{Var}\\left(\\widetilde{\\beta}_1\\right)=\\sigma^2 /\\left[\\operatorname{SST}_1\\right]\\)\n\nTake away: Including irrelevant variables in the model “may” not affect the unbiasedness of the OLS estimator, but it will affect the precision of the estimates."
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/ps/ugmetricsps1.html",
    "href": "teachingmaterials/UG_intro_metrics/ps/ugmetricsps1.html",
    "title": "Econometrics PS 1",
    "section": "",
    "text": "Md Moshi Ul Alam\n      Assistant Professor of Economics\n      \n        Clark University\n      \n      \n        \n          \n          Email\n        \n        \n          \n          CV"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/ps/ugmetricsps1.html#q-3.1",
    "href": "teachingmaterials/UG_intro_metrics/ps/ugmetricsps1.html#q-3.1",
    "title": "Econometrics PS 1",
    "section": "Q 3.1",
    "text": "Q 3.1\n\nLoad the data and create a scatter plot of monthly wages (wage) against years of education (educ). What do you observe about the relationship? Are there any concerning patterns?\nCalculate and report the sample correlation between wages and education. Then manually calculate \\hat{\\beta}_1 using the formula: \\hat{\\beta}_1 = \\hat{\\rho}_{xy} \\cdot \\left(\\frac{\\hat{\\sigma}_y}{\\hat{\\sigma}_x}\\right) by computing each component separately. Show that this matches the OLS estimate from lm().\nEstimate the simple linear regression:\n\n\nwage_i = \\beta_0 + \\beta_1 \\, educ_i + u_i\n\nReport and interpret both coefficients. Is the intercept meaningful in this context?\n\nCalculate the fitted values and residuals manually (without using fitted() or residuals()). Verify that:\n\n\nThe mean of residuals equals zero\n\nThe point (\\bar{educ}, \\bar{wage}) lies on the regression line\n\nThe sample covariance between education and residuals is zero"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/ps/ugmetricsps1.html#q-3.2",
    "href": "teachingmaterials/UG_intro_metrics/ps/ugmetricsps1.html#q-3.2",
    "title": "Econometrics PS 1",
    "section": "Q 3.2",
    "text": "Q 3.2\n\nEstimate:\n\n\n\\log(wage_i) = \\beta_0 + \\beta_1 \\, educ_i + u_i\n\n\nUsing the log-level specification:\n\n\nCalculate the exact percentage return to one additional year of education\n\nPredict the percentage wage difference between workers with 12 and 16 years of education\n\n\n\nCreate histograms of wage and log(wage). Which estimation would you prefer to run: level-level or log-level? Justify your choice."
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/ps/ugmetricsps1.html#q-3.3",
    "href": "teachingmaterials/UG_intro_metrics/ps/ugmetricsps1.html#q-3.3",
    "title": "Econometrics PS 1",
    "section": "Q 3.3",
    "text": "Q 3.3\n\nCreate a residual plot (residuals vs. fitted values) for your log-level specification. Do you see evidence of heteroskedasticity? Explain what pattern you would look for.\nGroup the data into three education categories: low (educ &lt; 12), medium (12 \\leq educ &lt; 16), and high (educ \\geq 16).\n\n\nwage2$educ_group &lt;- cut(wage2$educ, \n                        breaks = c(-Inf, 12, 16, Inf),\n                        labels = c(\"Low (&lt;12)\", \"Medium (12-16)\", \"High (≥16)\"),\n                        right = FALSE)\n\nCalculate the variance of residuals within each group. What do these variances suggest about the homoskedasticity assumption?\n\nIf heteroskedasticity is present, what are the consequences for the unbiasedness of \\hat{\\beta}_1?"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/pssol/ps1sol.html",
    "href": "teachingmaterials/UG_intro_metrics/pssol/ps1sol.html",
    "title": "Econometrics PS 1 - Solutions",
    "section": "",
    "text": "Md Moshi Ul Alam\n      Assistant Professor of Economics\n      \n        Clark University\n      \n      \n        \n          \n          Email\n        \n        \n          \n          CV"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/pssol/ps1sol.html#part-a",
    "href": "teachingmaterials/UG_intro_metrics/pssol/ps1sol.html#part-a",
    "title": "Econometrics PS 1 - Solutions",
    "section": "Part (a)",
    "text": "Part (a)\n\npar(mfrow=c(1,2))\nplot(x, u1, main=\"u1 vs x\", pch=20, col=\"blue\")\nabline(lm(u1~x), col=\"red\", lwd=2)\nplot(x, u2, main=\"u2 vs x\", pch=20, col=\"blue\")\nabline(lm(u2~x), col=\"red\", lwd=2)\n\n\n\n\n\n\n\nIn the first population model, u_1 shows no systematic pattern with x (randomly scattered around zero). However, in the second one, u_2 exhibits a clear positive linear relationship with x, violating the zero conditional mean assumption i.e., E(u_2|x) \\neq 0."
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/pssol/ps1sol.html#part-b",
    "href": "teachingmaterials/UG_intro_metrics/pssol/ps1sol.html#part-b",
    "title": "Econometrics PS 1 - Solutions",
    "section": "Part (b)",
    "text": "Part (b)\nOnly the regression of y_1 on x will produce unbiased estimates.\nProof: For unbiased OLS, we need E(u|x) = 0.\n\nFor model 1: E(u_1|x) = 0 because u_1 is generated as N(0, 0.1) independent of x\n\nFor model 2: E(u_2|x) = E(x + \\text{rnorm}(0, 0.1)|x) = x \\neq 0. This is clear from the plot in part (a) where the average of u_2 increases with x.\n\nSince the zero conditional mean assumption is violated for model 2, OLS estimates will be biased."
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/pssol/ps1sol.html#part-c",
    "href": "teachingmaterials/UG_intro_metrics/pssol/ps1sol.html#part-c",
    "title": "Econometrics PS 1 - Solutions",
    "section": "Part (c)",
    "text": "Part (c)\n\nmodel1 &lt;- lm(y1 ~ x)\nmodel2 &lt;- lm(y2 ~ x)\n\nresults &lt;- data.frame(\n  Model = c(\"y1 ~ x\", \"y2 ~ x\"),\n  True_Beta0 = c(2, 2),\n  Beta0_hat = c(coef(model1)[1], coef(model2)[1]),\n  True_Beta1 = c(3, 3),\n  Beta1_hat = c(coef(model1)[2], coef(model2)[2])\n)\nkable(results, digits=4)\n\n\n\nModel\nTrue_Beta0\nBeta0_hat\nTrue_Beta1\nBeta1_hat\n\n\n\ny1 ~ x\n2\n1.9998\n3\n3.0019\n\n\ny2 ~ x\n2\n1.9991\n3\n4.0002\n\n\n\n\n\nModel 1 estimates are very close to true parameters (2 and 3). Model 2’s slope estimate is biased upward (~4 instead of 3) because u_2 is positively correlated with x, causing omitted variable bias.\nNot required for credit: –&gt;"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/pssol/ps1sol.html#part-d",
    "href": "teachingmaterials/UG_intro_metrics/pssol/ps1sol.html#part-d",
    "title": "Econometrics PS 1 - Solutions",
    "section": "Part (d)",
    "text": "Part (d)\nE(u|\\text{education}) \\neq 0 because ability is in the error term since it affects wages, and is correlated with education.\nSince ability is positively correlated with education and affects wages positively, OLS will overestimate the true return to education leading to upward bias in the estimate of β_1 because it will give you an estimate of the return to education that confounded by the effect of ability.\n\n\\text{Cov}(\\text{education}, u) &gt; 0, leading to positive bias: E(\\hat{\\beta}_1) = \\beta_1 + \\frac{\\text{Cov}(education,u)}{\\text{Var}(education)} &gt; \\beta_1"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/pssol/ps1sol.html#q-3.1",
    "href": "teachingmaterials/UG_intro_metrics/pssol/ps1sol.html#q-3.1",
    "title": "Econometrics PS 1 - Solutions",
    "section": "Q 3.1",
    "text": "Q 3.1\nPart (a)\n\ndata(wage2)\nggplot(wage2, aes(x = educ, y = wage)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Wages vs Education\", x = \"Years of Education\", y = \"Monthly Wage\")\n\n\n\n\n\n\n\nPositive relationship between education and wages. Observable heteroskedasticity (variance increases with education) and potential outliers at high wage levels.\nPart (b)\n\n# Sample correlation\nrho_xy &lt;- cor(wage2$wage, wage2$educ)\n\n# Standard deviations\nsigma_y &lt;- sd(wage2$wage)\nsigma_x &lt;- sd(wage2$educ)\n\n# Manual calculation of beta1\nbeta1_manual &lt;- rho_xy * (sigma_y / sigma_x)\n\n# Using lm()\nmodel &lt;- lm(wage ~ educ, data = wage2)\nbeta1_lm &lt;- coef(model)[2]\n\n\ncat(\"Beta1 (manual): \", round(beta1_manual, 4), \"\\n\", \"Beta1 (lm): \", round(beta1_lm, 4), \"\\n\")\n\nBeta1 (manual):  60.2143 \n Beta1 (lm):  60.2143 \n\n\nPart (c)\n\nsummary(model)\n\n\nCall:\nlm(formula = wage ~ educ, data = wage2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-877.38 -268.63  -38.38  207.05 2148.26 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  146.952     77.715   1.891   0.0589 .  \neduc          60.214      5.695  10.573   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 382.3 on 933 degrees of freedom\nMultiple R-squared:  0.107, Adjusted R-squared:  0.106 \nF-statistic: 111.8 on 1 and 933 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\\hat{\\beta}_0 = 146.95 gives the average monthly wage with zero education (not meaningful in this context as no one has zero education in the sample, and otherwise we are extrapolating). But that does not mean that we should drop it from the regression. Recall that the intercept is needed to ensure that E(u) = 0 through re-normalization.\n\n\\hat{\\beta}_1 = 60.21: Each additional year of education on average is associated with $60.21 increase in average monthly wages, all else equal.\nPart (d)\n\n# Manual calculations of Fitted values\nbeta0_hat &lt;- coef(model)[1]\nbeta1_hat &lt;- coef(model)[2]\ny_hat &lt;- beta0_hat + beta1_hat * wage2$educ\n\n# Residuals\nu_hat &lt;- wage2$wage - y_hat\n\n# residual at mean_educ, mean_wage\nmean_wage &lt;- mean(wage2$wage)\nmean_educ &lt;- mean(wage2$educ)\nresid_at_mean &lt;- mean_wage - (beta0_hat + beta1_hat * mean_educ)\n\n# Verifications\ncat(\"Mean of residuals: \", round(mean(u_hat), 5), \"\\n\", \n  \"Point (mean_educ, mean_wage): (\", mean(wage2$educ), \", \", mean(wage2$wage), \")\\n\",\n  \"Residual at (mean_educ, mean_wage): \", round(resid_at_mean, 5), \"\\n\",\n  \"Covariance(educ, residuals): \", round(cov(wage2$educ, u_hat), 10), \"\\n\")\n\nMean of residuals:  0 \n Point (mean_educ, mean_wage): ( 13.46845 ,  957.9455 )\n Residual at (mean_educ, mean_wage):  0 \n Covariance(educ, residuals):  0 \n\n\nWhile other parts are obvious, Point (mean_educ, mean_wage) lies on the regression line because the residual at that point is effectively zero."
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/pssol/ps1sol.html#q-3.2",
    "href": "teachingmaterials/UG_intro_metrics/pssol/ps1sol.html#q-3.2",
    "title": "Econometrics PS 1 - Solutions",
    "section": "Q 3.2",
    "text": "Q 3.2\nPart (a)\n\nlog_model &lt;- lm(log(wage) ~ educ, data = wage2)\nsummary(log_model)\n\n\nCall:\nlm(formula = log(wage) ~ educ, data = wage2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.94620 -0.24832  0.03507  0.27440  1.28106 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.973062   0.081374   73.40   &lt;2e-16 ***\neduc        0.059839   0.005963   10.04   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4003 on 933 degrees of freedom\nMultiple R-squared:  0.09742,   Adjusted R-squared:  0.09645 \nF-statistic: 100.7 on 1 and 933 DF,  p-value: &lt; 2.2e-16\n\n\nPart (b)\nE(\\log(wage)|educ =16) - E(\\log(wage)|educ =12) = 4\\beta_1\n\nbeta1_log &lt;- coef(log_model)[2]\n\n# Exact percentage return\npct_return &lt;- (exp(beta1_log) - 1) * 100\ncat(\"Exact % return to one year of education: \", round(pct_return, 2), \"%\\n\")\n\nExact % return to one year of education:  6.17 %\n\n#  avg Wage difference between 12 and 16 years\nwage_diff &lt;-  (exp(beta1_log * 4) - 1) * 100\ncat(\"% wage difference (16 vs 12 years): \", round(wage_diff, 2), \"%\\n\")\n\n% wage difference (16 vs 12 years):  27.04 %\n\n\nPart (c)\n\npar(mfrow = c(1, 2))\nhist(wage2$wage, main = \"Distribution of Wage\", xlab = \"Wage\", breaks = 30)\nhist(log(wage2$wage), main = \"Distribution of Log(Wage)\", xlab = \"Log(Wage)\", breaks = 30)\n\n\n\n\n\n\n\nLog-level specification is preferred because level wage is right-skewed and log transformation helps in reducing the influence of outliers and for approximate interpretations percentage interpretation which is more intuitive for wage returns"
  },
  {
    "objectID": "teachingmaterials/UG_intro_metrics/pssol/ps1sol.html#q-3.3",
    "href": "teachingmaterials/UG_intro_metrics/pssol/ps1sol.html#q-3.3",
    "title": "Econometrics PS 1 - Solutions",
    "section": "Q 3.3",
    "text": "Q 3.3\nPart (a)\n\nplot(fitted(log_model), residuals(log_model), \n     main = \"Residual Plot for Log-Level Model\",\n     xlab = \"Fitted Values\", ylab = \"Residuals\")\n\n\n\n\n\n\n\nSome heteroskedasticity visible - residual spread appears to increase slightly with fitted values. Thus variance of residuals changes systematically with fitted values.\nPart (b)\n\nwage2$educ_group &lt;- cut(wage2$educ, \n                        breaks = c(-Inf, 12, 16, Inf),\n                        labels = c(\"Low (&lt;12)\", \"Medium (12-16)\", \"High (≥16)\"),\n                        right = FALSE)\n\nresid &lt;- residuals(log_model)\n\nvar_resid_group1 &lt;- var(resid[wage2$educ_group == \"Low (&lt;12)\"])\nvar_resid_group2 &lt;- var(resid[wage2$educ_group == \"Medium (12-16)\"])\nvar_resid_group3 &lt;- var(resid[wage2$educ_group == \"High (≥16)\"])\n\nprint(data.frame(\n  Education_Group = c(\"Low (&lt;12)\", \"Medium (12-16)\", \"High (≥16)\"),\n  Variance_of_Residuals = c(var_resid_group1, var_resid_group2, var_resid_group3)\n))\n\n  Education_Group Variance_of_Residuals\n1       Low (&lt;12)             0.1360187\n2  Medium (12-16)             0.1631016\n3      High (≥16)             0.1624964\n\n# Shorter code using `tapply`:\n# variance_by_group &lt;- tapply(resid, wage2$educ_group, var)\n# print(variance_by_group)\n\nVariances of the residuals differs across education groups, suggesting heteroskedasticity is present (though differences are moderate in the log specification).\nPart (c)\nHeteroskedasticity does not affect the unbiasedness of \\hat{\\beta}_1. The OLS estimator remains unbiased as long as E(u|x) = 0 holds.\nNot for credit\nConsequences of heteroskedasticity:\n\nOLS estimates remain unbiased and consistent as long as E(u|x) = 0 holds\nOLS is no longer BLUE (efficient)\nHomoskedastic Standard errors are typically underestimated\nCosnequently, hypothesis tests and confidence intervals are invalid"
  }
]