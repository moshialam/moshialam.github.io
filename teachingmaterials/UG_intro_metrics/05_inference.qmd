---
title: "Econ 265: \n Introduction to Econometrics"
subtitle: "Topic 5: Inference"
author: "Moshi Alam"
format:
  revealjs:
    theme: serif
    slide-number: true
    incremental: false 
    chalkboard: true
execute:
  echo: true
---

## Introduction {.smaller}

Population model: $y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \ldots + \beta_k x_{ki} + u_i$.



:::{.columns}
:::{.column width="50%"}
- estimate the parameters \& SE using OLS 

<!-- Now we need to learn how to make inferences about the population parameters based on the *sample estimates and standard errors.* -->

Now, based on *sample estimates and standard errors.* we will answer questions like:

- Whether the estimated coefficient is statistically significant at any confidence level?
  - Whether the true population parameter estimate is statistically  different from zero at any confidence level?
  - Choice of models
:::
:::{.column width="50%"}

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(wooldridge)
wage_data <- wage1
reg1 <- lm(log(wage) ~ educ + exper + tenure, data = wage_data)
summary(reg1)
```
:::
:::

## Recall from Econ160 {.smaller}

::::::{.incremental}
- To test equality of proportions between two groups, we used the $z$-test
  - $H_0: p_1 - p_2 = 0$ vs $H_1: p_1 - p_2 \neq 0$
  - Test statistic: $z = \frac{(\hat{p}_1 - \hat{p}_2) - 0}{\sqrt{\hat{p}(1-\hat{p})(\frac{1}{n_1} + \frac{1}{n_2})}}$ Follows a standard normal distribution
  - CI: $(\hat{p}_1 - \hat{p}_2) \pm z_{\alpha/2}^* \sqrt{\hat{p}(1-\hat{p})(\frac{1}{n_1} + \frac{1}{n_2})}$
- To test equality of means between two groups, we used the $t$-test
  - $H_0: \mu_1 - \mu_2 = 0$ vs $H_1: \mu_1 - \mu_2 \neq 0$
  - Test statistic: $t = \frac{(\bar{y}_1 - \bar{y}_2) - 0}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}$ Follows a $t$ distribution with $n_1 + n_2 - 2$ (d.o.f.)
  - CI: $(\bar{y}_1 - \bar{y}_2) \pm t_{\alpha/2, n_1 + n_2 - 2}^* \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}$

::: {.callout-note icon="false"}
### Assumptions
random sampling, independence, and large sample size
:::

::::::

# Hypothesis testing in MLR

## Big picture {.smaller}
- To test hypotheses about regression coeffcients $\hat{\beta}_j$, we need to know:
  - the *sampling distribution* of the $\hat{\beta}_j$'s
  <!-- - the *standard errors* of the $\hat{\beta}_j$'s -->
    <!-- - Recall that the standard errors are the estimates of the standard deviation of the sampling distribution of the $\hat{\beta}_j$'s -->
- Once we have the sampling distribution of the $\hat{\beta}_j$
  - We can get the standard errors of the $\hat{\beta}_j$'s 
    - Estimates of the standard deviation of the sampling distribution of the $\hat{\beta}_j$'s
- We can make inferences about the population parameters $\beta_j$'s' based on 
  - the *estimates*, *standard errors* and the *sampling distribution* of the $\hat{\beta}_j$'s
- Turns out that under some assumptions, the $\hat{\beta}_j$'s follow a $t$ distribution (**in practice**)
  <!-- - when the s.d. of the population error is unknown  (which is the case in practice) -->
  - It requires an assumption on the distribution of the errors $u_i$'s **MLR.6**







## Assumption MLR.6: {.smaller}

::: {.callout-note icon="false"}
### Assumption MLR.6: Normality of Errors

The population error $u_i$ is normally distributed with mean 0 and constant variance $\sigma^2$  and independent of the regressors for all $i = 1, 2, \ldots, n$
$$u_i \sim N(0, \sigma^2)$$
:::

- This directly implies MLR.4 and MLR.5
- The full set of assumptions MLR.1-MLR.6 are called the *classical linear model (CLM) assumptions*. Together they imply: 
$$y \mid x_1, \ldots x_k \sim N(\beta_0 + \beta_1 x_{1i} + \ldots + \beta_k x_{ki}, \sigma^2)$$

- *Hard to defend this very strong assumption*
  - Thankfully not required for large samples (beyond the scope of this course)









## {.smaller}

::: {.callout-important icon="false"}
### Theorem 4.1: Normality of OLS Estimators

Under the CLM assumptions MLR. 1 through MLR.6, conditional on the sample values of the independent variables,

$$
\hat{\beta}_j \sim \operatorname{Normal}\left(\beta_j , \operatorname{Var}\left(\hat{\beta}_j\right)\right),
$$

where $\operatorname{Var}\left(\hat{\beta}_{j}\right)$was given in Chapter 3 [equation (3.51)]. Therefore,

$$
\frac{\hat{\beta}_j-\beta_j}{\operatorname{sd}\left(\hat{\beta}_j\right)} \sim \operatorname{Normal}(0,1) \quad \text { for } j=0,1, \ldots, k
$$
:::

::: {.callout-important icon="false"}
### Theorem 4.2: t-distribution of OLS Estimates

Under the CLM assumptions MLR. 1 through MLR.6, conditional on the sample values of the independent variables,

$$
\frac{\hat{\beta}_j-\beta_j}{\operatorname{se}\left(\hat{\beta}_j\right)} \sim t_{n-k-1} \quad \text { for } j=0,1, \ldots, k
$$

where $k+1$ is the number of unknown parameters in the population model and $n-k-1$ is the d.o.f.
:::

Observe the differences!



# Single hypothesis testing 

## Single hypothesis testing {.smaller}

Inference on the $j$th population parameter $\beta_j$:

- **Null hypothesis:** $H_0: \beta_j = b$, where $b$ is a hypothesized value typically 0
- Note that this is a test on the $j$th population parameter while holding all other parameters constant
- One-sided alternative: $H_1: \beta_j > b$ or $H_1: \beta_j < b$
- Two-sided alternative: $H_1: \beta_j \neq b$
- set b= 0. Test statistic: $t_{\hat{\beta}_j} = \frac{\hat{\beta}_j - b}{\operatorname{se}(\hat{\beta}_j)} = \frac{\hat{\beta}_j}{\operatorname{se}(\hat{\beta}_j)}$
- We know that the sampling distribution of $t_{\hat{\beta}_j}$  follows... 









## One-sided test {.smaller}

Fix a level of significance $\alpha$ (e.g., 0.05)


:::{.columns}
:::{.column width="50%"}
- **alternative:** $H_1: \beta_j > 0$ (or $H_1: \beta_j < 0)$
  - Reject $H_0$ if $t_{\hat{\beta}_j} > t_{\alpha, n-k-1}^*$
  - p-value = $P(T > t_{\hat{\beta}_j})$ where $T \sim t_{n-k-1}$
  - Confidence interval: $\hat{\beta}_j \pm t_{\alpha, n-k-1}^* \times \operatorname{se}(\hat{\beta}_j)$
- For large samples, t-distribution $\rightarrow$ standard normal
  - the critical value $\approx  \pm 1.65$ for $\alpha = 0.05$
:::
:::{.column width="50%"}
![One-sided test (right)](onesided.png){height=200}

![One-sided test (left)](onesided_left.png){height=200}
:::
:::





## Examples {.smaller}
:::{.columns}
:::{.column width="50%"}
```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(wooldridge)
nrow(meap93) - 4 - 1 # DOF
summary(lm(math10 ~ totcomp + staff + enroll, data=meap93))
```
:::
:::{.column width="50%"}
```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(wooldridge)
nrow(meap93) - 4 - 1 # DOF
summary(lm(math10 ~ log(totcomp) + log(staff) + log(enroll), data=meap93))
```
:::
:::





## Two-sided test {.smaller}

Fix a level of significance $\alpha$ (e.g., 0.05)

:::{.columns}
:::{.column width="50%"}
- **alternative:** $H_1: \beta_j \neq 0$
  - Reject $H_0$ if $|t_{\hat{\beta}_j}| > t_{\alpha/2, n-k-1}^*$
  - p-value = $P(|T| > |t_{\hat{\beta}_j}|)$
  - Confidence interval: $\hat{\beta}_j \pm t_{\alpha/2, n-k-1}^* \times \operatorname{se}(\hat{\beta}_j)$
- For large samples, t-distribution $\rightarrow$ standard normal
  - the critical value $\approx \pm 1.96$ for $\alpha = 0.05$
:::
:::{.column width="50%"}

![Two-sided test](twosided.png)

:::
:::

## Examples {.smaller}

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(wooldridge)
nrow(meap93) - 4 - 1 # DOF
summary(lm(math10 ~ log(totcomp) + log(staff) + log(enroll), data=meap93))
```





## Computing p-values directly in R {.smaller}

- p-value is the probability of observing the sample (statistic) if $H_0$ is true
- Indeed you can do this using a table.
- But R can do this for you

```{r, echo=TRUE, message=FALSE, warning=FALSE}
t_value <- 2.5    #  t-value
df <- 100         #  degrees of freedom

# Two-tailed p-value
p_value <- 2 * (1 - pt(abs(t_value), df))
print(p_value)

# One-tailed p-value (right tail)
p_right <- 1 - pt(t_value, df)
print(p_right)

# One-tailed p-value (left tail)
p_left <- pt(t_value, df)
print(p_left)
```

<!-- ## Computing Confidence Intervals {.smaller}

- Confidence interval is a range of values where we are $1-\alpha$ confident that the true population parameter lies within
- Example: -->

## Testing Linear combinations {.smaller}

- Does attending $jc_i$ has lower effect on wages as going to university $univ_i$?
$$log(wage_i) = \beta_0 + \beta_1 jc_i + \beta_2 univ_i + \beta_3 exper_i + u_i$$ 

- What are we testing?

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(wooldridge)
summary(lm(lwage ~ jc + univ + exper, data = twoyear))
```

## {.smaller}
$$log(wage_i) = \beta_0 + \beta_1 jc_i + \beta_2 univ_i + \beta_3 exper_i + u_i$$ $H_0: \beta_1 = \beta_2$ and $H_1: \beta_1 < \beta_2$



<!-- - $H_0: \beta_1 - \beta_2 = 0$ and $H_1: \beta_1 - \beta_2 < 0$ -->
:::{.incremental}
- Call $\beta_1 - \beta_2 = \theta$ and so $H_0: \theta = 0$ and $H_1: \theta < 0$
- Test statistic: $t_{\hat{\theta}} = \frac{\hat{\theta}}{\operatorname{se}(\hat{\theta})} = \frac{\hat{\beta}_1 - \hat{\beta}_2}{\operatorname{se}(\hat{\beta}_1-\hat{\beta}_2)} = \frac{\hat{\beta}_1 - \hat{\beta}_2}{\sqrt{\operatorname{se}(\hat{\beta}_1)^2 + \operatorname{se}(\hat{\beta}_2)^2 + 2\widehat{\operatorname{cov}(\hat{\beta}_1, \hat{\beta}_2)}}}$
  <!-- - Reject $H_0$ 
    - if $t_{\hat{\theta}} < t_{\alpha, n-k-1}^*$
    - Or if p-value = $P(T < t_{\hat{\theta}})$ where $T \sim t_{n-k-1}$ is less than $\alpha$
    - Or if CI: $[\hat{\theta} \pm t_{\alpha, n-k-1}^* \times \operatorname{se}(\hat{\theta})]$ does not contain zero -->
- Easy to calculate  $t_{\hat{\theta}}$ if $\widehat{\operatorname{cov}(\hat{\beta}_1, \hat{\beta}_2)}$ is known 
:::

## Implementation {.smaller}


:::{.incremental}
- $$log(wage_i) = \beta_0 + \beta_1 jc_i + \beta_2 univ_i + \beta_3 exper_i + u_i$$ <!--$H_0: \beta_1 = \beta_2$ and $H_1: \beta_1 < \beta_2$ -->
- $$log(wage_i) = \beta_0 + (\theta + \beta_2) jc_i + \beta_2 univ_i + \beta_3 exper_i + u_i$$
- $$log(wage_i) = \beta_0 + \theta jc_i + \beta_2 (jc_i + univ_i) + \beta_3 exper_i + u_i$$
:::

## R code {.smaller}
$$log(wage_i) = \beta_0 + \theta jc_i + \beta_2 (jc_i + univ_i) + \beta_3 exper_i + u_i$$
```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(wooldridge)
summary(lm(lwage ~ jc + I(jc + univ) + exper, data = twoyear))
```








# Testing Multiple Linear Restrictions 


## Testing multiple restrictions {.smaller}

- The $t$-test is used to test a hypothesis about a single population parameter $\beta_j$
- The $F$-test is used to test multiple hypotheses about multiple population parameters *jointly*
- Example: $\log (\text { salary }_i )=  \beta_0+\beta_1 \text { years }_i +\beta_2 \text { gamesyr }_i +\beta_3 \text { bavg }_i  +\beta_4 \text { hrunsyr }_i +\beta_5 \text { rbisyr }_i +u_i$
  - $\mathrm{H}_0: \beta_3=0, \beta_4=0, \beta_5=0$
  - $\mathrm{H}_1:$ $H_0$ is not true: At least one of $\beta_3, \beta_4, \beta_5$ is not zero


## {.smaller}          
```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(wooldridge)
summary(lm(log(salary) ~ years + gamesyr + bavg + hrunsyr + rbisyr, data = mlb1))
```
- Based on what we have learned so far, what do we think about $\mathrm{H}_0: \beta_3=0, \beta_4=0, \beta_5=0$?

<!-- ## Economic vs Statistical Significance {.smaller} -->

## Let us run both models {.smaller}

:::{.columns}
:::{.column width="50%"}
#### Unrestricted model
```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(wooldridge)
unrestricted <-  lm(log(salary) ~ years + gamesyr + bavg + hrunsyr + rbisyr, data = mlb1)
summary(unrestricted)
sum(unrestricted$residuals^2)
```
:::
:::{.column width="50%"}
#### Restricted model
```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(wooldridge)
restricted <-  lm(log(salary) ~ years + gamesyr, data = mlb1)
summary(restricted)
sum(restricted$residuals^2)
```
:::
:::

- Is the increase in SSR large enough to reject the null hypothesis?
- But what is the test statistic that we can use?

## The F-test {.smaller}

:::{.columns}
:::{.column width="50%"}
:::{.incremental}
- $y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \ldots + \beta_k x_{ki} + u_i$
<!-- - $y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \ldots + \beta_{k-q-1} x_{k-q-1} + \beta_{k-q} x_{k-q} + \beta_{k-q+1} x_{k-q+1} + \ldots + \beta_k x_{ki} + u_i$ -->
- Hypotheses Test of $q$ *exclusion restrictions*
  - $H_0: \beta_{k-q} = \beta_{k-q+1} = \ldots = \beta_k = 0$ 
  - $H_1:$ At least one of $\beta_{k-q}, \beta_{k-q+1}, \ldots, \beta_k$ is not zero
- Test statistic: $F = \frac{(SSR_r - SSR_{ur})/q}{SSR_{ur}/(n-k-1)}$
  - Can bee re-written as $\frac{(R^2_{UR} - R^2_R)/q}{(1 - R^2_{UR})/(n-k-1)}$
  <!-- - $SSR_r$ and $SSR_{ur}$ are the sum of squared residuals from the restricted and unrestricted models
  - $q$ is the number of restrictions
  - $n$ is the number of observations and $k$ is the number of parameters in the unrestricted model -->
  - Follows an $F$-distribution with $q$ and $n-k-1$ d.o.f.
  - Note:
    -  $q = dof_r - dof_{ur}$
    -  F > 0
:::
:::
:::{.column width="50%"}
![Example F sampling distribution](f_crit.png)
:::
:::

## Going back to the example {.smaller}

 $\log(salary_i)=  \beta_0+\beta_1 years_i +\beta_2 gamesyr_i +\beta_3 bavg_i  +\beta_4 hrunsyr_i +\beta_5 rbisyr_i +u_i$

- $\mathrm{H}_0: \beta_3=0, \beta_4=0, \beta_5=0$
- $\mathrm{H}_1:$ $H_0$ is not true.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(wooldridge)
unrestricted <-  lm(log(salary) ~ years + gamesyr + bavg + hrunsyr + rbisyr, data = mlb1)
restricted <-  lm(log(salary) ~ years + gamesyr, data = mlb1)
F_stat <- ((sum(restricted$residuals^2) - sum(unrestricted$residuals^2))/3)/(sum(unrestricted$residuals^2)/(nrow(mlb1) - 5-1))
print(F_stat)

critical_F <- qf(0.05, df1 = 3, df2 = nrow(mlb1) - 5-1, lower.tail = F)
print(critical_F)

if (F_stat > critical_F) {
  print("Reject H0")
} else {
  print("Fail to reject H0")
}
```

## {.smaller}
 $\log(salary_i)=  \beta_0+\beta_1 years_i +\beta_2 gamesyr_i +\beta_3 bavg_i  +\beta_4 hrunsyr_i +\beta_5 rbisyr_i +u_i$


:::{.incremental}
- But recall when we ran the unrestricted model, we found that $\beta_3, \beta_4, \beta_5$ were not statistically significant.
- So, whats going on here with the joint F-test?
- $hrunsyr$ and $rbisyr$ are highly correlated. 
  -  multicollinearity $\rightarrow$ large standard errors $\rightarrow$  low t-stats  $\rightarrow$  individual statistical insignificance
- The F-test is a joint test (including bavg) and is not affected by multicollinearity 
- Hence F-tests of joint hypotheses can be useful in the presence of multicollinearity
:::

## Relationship between t and F tests {.smaller}

- For a single restriction $q=1$, the F-test is equivalent to the t-test
  - $F_{1,n-k-1} = t^2_{n-k-1}$
- For single hypothesis testing, the t-test is more powerful
  -  F-tests remain under-powered than t-tests (See Math refresher C)



## Reporting Regression Results {.smaller}

- **Report Estimated Coefficients**  
  - Interpret key variables' estimates in economic or practical terms.

- **Include Standard Errors**  
  - Preferred over just $t$-statistics as they help interpret hypothesis tests and confidence intervals.

- **Report $R^2$ and Other Fit Statistics**  
  - $R^2$ is essential for goodness-of-fit.  
  - Reporting F-statistics helps test exclusion restrictions.

- **Summarize in Tables for Multiple Models**  
  - If multiple equations are estimated, use tables instead of inline equations.  
  - Dependent variable should be clearly indicated.  
  - Independent variables should be listed in the first column.  
  - Standard errors in parentheses below estimates.


## Salary and Benefits Tradeoff example {.smaller}

:::{.incremental}
- **Total compensation** ($totcomp$) consists of salary and benefits:$$totcomp = salary + benefits = salary \left( 1 + \frac{benefits}{salary} \right)$$
- Taking the **log transformation**: $\log(totcomp) = \log(salary) + \log(1 + b/s).$
- For **small** $b/s$, can approximate $\log(1 + b/s) \approx b/s.$
- This leads to the econometric model:$$\log(salary) = \beta_0 + \beta_1 (b/s) + \text{other *controls*}. $$
- **Hypothesis Test**: Testing the **salary-benefits tradeoff**:
  - $H_0: \beta_1 = -1$ (full tradeoff)
  - $H_1: \beta_1 \neq -1$ (partial or no tradeoff)
- Data from **MEAP93** controls for enrollment, staff size, dropout, and graduation rates.
:::

## {.smaller}


```{r, echo=TRUE, message=FALSE, warning=FALSE}

library(stargazer, wooldridge)
model1 <- lm(log(salary) ~ I(benefits/salary), data = meap93)
model2 <- lm(log(salary) ~ I(benefits/salary) + I(log(enroll)) + I(log(staff)), data = meap93)
model3 <- lm(log(salary) ~ I(benefits/salary) + I(log(enroll)) + I(log(staff)) + droprate + gradrate, data = meap93)

stargazer(model1, model2, model3, 
          type = "text", 
          title = "Regression Results",
          omit.stat = c("ser"), # Omit some stats if needed
          dep.var.labels = "Log(salary)",
          column.labels = c("Model 1", "Model 2", "Model 3"),
          covariate.labels = c("benefits to salary ratio", "log of enrollment", "log of staff", "Drop out rate", "Graduation rate", "Intercept"))


```


## `modelsummary` package   {.smaller}

```{r, echo=T, message=FALSE, warning=FALSE}

library(modelsummary)
models <- list(
  "Model 1" = model1,
  "Model 2" = model2,
  "Model 3" = model3
)
modelsummary(models, stars = TRUE, output = "markdown")
```

<!-- ## kableExtra package   {.smaller}

```{r, echo=T, message=FALSE, warning=FALSE}
library(kableExtra)
summary_table <- modelsummary(models, output = "data.frame")
kable(summary_table, format = "html") %>%
  kable_styling(full_width = FALSE)

``` -->