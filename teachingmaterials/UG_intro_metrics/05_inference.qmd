---
title: "Econ 265: \n Introduction to Econometrics"
subtitle: "Topic 5: Inference"
author: "Moshi Alam"
format:
  revealjs:
    theme: serif
    slide-number: true
    incremental: false 
    chalkboard: true
execute:
  echo: true
editor: visual
---

## Introduction {.smaller}

Population model: $y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \ldots + \beta_k x_{ki} + u_i$.



:::{.columns}
:::{.column width="50%"}
- estimate the parameters using OLS 
- obtain the standard errors of the OLS estimates
- And imlement them in R

<!-- Now we need to learn how to make inferences about the population parameters based on the *sample estimates and standard errors.* -->

Now, based on *sample estimates and standard errors.* we will answer questions like:

- Whether the estimated coefficient is statistically significant at any confidence level?
  - Means: Whether the true population parameter is different from zero at any confidence level?
:::
:::{.column width="50%"}

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(wooldridge)
wage_data <- wage1
reg1 <- lm(log(wage) ~ educ + exper + tenure, data = wage_data)
summary(reg1)
```
:::
:::

# Refreshers from Econ 160



## Normal Distributions {.smaller}
Random variable: $X \sim N(\mu, \sigma^2)$

PDF: $f(X = x) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}$, where

- $E(X) = \mu$ is the mean and $Var(X) = \sigma^2$ is the variance
- The distirbution is symmetric around $\mu$

Properties:

- $Z = \frac{X - \mu}{\sigma}$ is the standard normal random variable with mean 0 and variance 1
- Transformation of X such as $aX + b \sim N(a\mu + b, a^2\sigma^2)$
- If X & Y are independent normal random variables, then $X + Y \sim N(\mu_X + \mu_Y, \sigma_X^2 + \sigma_Y^2)$
- Any linear combination of normal random variables is normal


## $\chi^2_n$ and $t_n$ {.smaller}
$Z_i \sim N(0,1)$ for $i = 1, 2, \ldots, n$ independent standard normal random variables

- The sum of squared independent standard normal random variables follows a chi-square distribution with $n$ d.o.f..  Let $Q = \sum_{i=1}^n Z_i^2$ $$Q \sim \chi^2_n$$  
  - $E(Q) = n$ and $Var(Q) = 2n$
  <!-- - The sample variance is a random variable and follows a chi-square distribution -->
- The ratio of a standard normal random variable and a chi-square random variable follows a t-distribution. Let $T = \frac{Z}{\sqrt{Q/n}}$ $$T \sim t_n$$
  <!-- - Let $Z \sim N(0,1)$ and $Q \sim \chi^2_n$ independent random variables, then $$T = \frac{Z}{\sqrt{Q/n}} \sim t_n$$ -->
  - $E(T) = 0$ and $Var(T) = n/(n-2)$ for $n > 2$
  - Shape of the t-distribution is similar to the normal distribution but more spread out (heavier tails). Check out Figure B-9 in Math Refresher B 
- As sample size increases, the t-distribution approaches the standard normal distribution

## sampling distributions {.smaller}

<!-- - **Population distribution:** The distribution of the variable of interest in the population -->
The distribution of the sample statistic (e.g., sample mean, sample variance, regression coefficients) over repeated independent sampling

- **Sampling variability:** The variability of the sample statistic across different samples
- **Standard error:** The estimate of the standard deviation of the sampling distribution of a sample statistic
- **Central Limit Theorem (CLT):** Let $\{X_1, X_2, \ldots, X_n\}$ be a random sample of size $n$ from a population with mean $\mu$ and variance $\sigma^2$. Then, as $n$ approaches infinity,  $\bar{X} \sim N(\mu, \sigma^2/n)$ so that $\frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \sim N(0,1)$
  <!-- - The average from a random random sample for any population (with finite variance), when standardized, has a standard normal distribution as sample size tends to infinity.  -->
- Recall from Econ 160:
  - The sampling distribution of the *sample proportion* $\hat{p}$ approaches a normal distribution with mean $p$ and variance $p(1-p)/n$.
  - The sampling distribution of *sample means* follows a $t$-distribution with $n-1$ degrees of freedom 
  

## Hypothesis testing {.smaller}

- **Null hypothesis $H_0$:** A statement about the population parameter that is assumed to be true and test against an alternative hypothesis $H_1$
- **Type I error:** Rejecting the $H_0$ when it is true
- **Type II error:** Failing to reject the $H_0$ when it is false
- **Significance level:** The probability of committing a Type I error, denoted by $\alpha$
- **Confidence interval:** A range of values where we are $1-\alpha$ confident that the true population parameter lies within
  - estimate $\pm$ critical value $\times$ standard error
  - What happens if the CI contains the hypothesized value?
- Tests can only allow us to reject the $H_0$ or fail to reject the $H_0$ but never accept the $H_0$. Why? 

## {.smaller}
- **p-value:** The probability of observing the sample (statistic) if $H_0$ were true
  - What does it mean when p-value is less than $\alpha$?
- **Critical value:** The value that separates the rejection region from the non-rejection region in hypothesis testing
  - What is critical value for a two-tailed test with $\alpha = 0.05$?
- **Critical region:** The range of values that leads to the rejection of the $H_0$
- **One-tailed test:** A hypothesis test that tests the $H_0$ in one direction
- **Two-tailed test:** A hypothesis test that tests the $H_0$ in both directions
<!-- - **Degrees of freedom:** The number of independent observations in a sample that are available to estimate a parameter -->
<!-- - **t-distribution:** A family of distributions that are similar to the normal distribution but with heavier tails, used for hypothesis testing when the population standard deviation is unknown -->
<!-- - **Chi-square distribution:** A family of distributions that are used for hypothesis testing when the population standard deviation  is unknown -->
<!-- - **F-distribution:** A family of distributions that are used for hypothesis testing when comparing the variances of two populations -->














# Hypothesis testing in MLR

## Big picture {.smaller}
- To test hypotheses about regression coeffcients $\hat{\beta}_j$, we need to know:
  - the *sampling distribution* of the $\hat{\beta}_j$'s
  <!-- - the *standard errors* of the $\hat{\beta}_j$'s -->
    <!-- - Recall that the standard errors are the estimates of the standard deviation of the sampling distribution of the $\hat{\beta}_j$'s -->
- Once we have the sampling distribution of the $\hat{\beta}_j$
  - We can get the standard errors of the $\hat{\beta}_j$'s 
    - Estimates of the standard deviation of the sampling distribution of the $\hat{\beta}_j$'s
- We can make inferences about the population parameters $\beta_j$'s' based on 
  - the *estimates*, *standard errors* and the *sampling distribution* of the $\hat{\beta}_j$'s
- Turns out that under some assumptions, the $\hat{\beta}_j$'s follow a $t$ distribution (**in practice**)
  <!-- - when the s.d. of the population error is unknown  (which is the case in practice) -->
  - It requires an assumption on the distribution of the errors $u_i$'s **MLR.6**







## Assumption MLR.6: {.smaller}

::: {.callout-note icon="false"}
### Assumption MLR.6: Normality of Errors

The population error $u_i$ is normally distributed with mean 0 and constant variance $\sigma^2$  and independent of the regressors for all $i = 1, 2, \ldots, n$
$$u_i \sim N(0, \sigma^2)$$
:::

- This directly implies MLR.4 and MLR.5
- The full set of assumptions MLR.1-MLR.6 are called the *classical linear model (CLM) assumptions*. Together they imply: 
$$y \mid x_1, \ldots x_k \sim N(\beta_0 + \beta_1 x_{1i} + \ldots + \beta_k x_{ki}, \sigma^2)$$

- *Hard to defend this very strong assumption*
  - Thankfully not required for large samples (Ch-5)









## {.smaller}

::: {.callout-important icon="false"}
### Theorem 4.1: Normality of OLS Estimators

Under the CLM assumptions MLR. 1 through MLR.6, conditional on the sample values of the independent variables,

$$
\hat{\beta}_j \sim \operatorname{Normal}\left(\beta_j , \operatorname{Var}\left(\hat{\beta}_j\right)\right),
$$

where $\operatorname{Var}\left(\hat{\beta}_{j}\right)$was given in Chapter 3 [equation (3.51)]. Therefore,

$$
\frac{\hat{\beta}_j-\beta_j}{\operatorname{sd}\left(\hat{\beta}_j\right)} \sim \operatorname{Normal}(0,1) \quad \text { for } j=0,1, \ldots, k
$$
:::

::: {.callout-important icon="false"}
### Theorem 4.2: t-distribution of OLS Estimates

Under the CLM assumptions MLR. 1 through MLR.6, conditional on the sample values of the independent variables,

$$
\frac{\hat{\beta}_j-\beta_j}{\operatorname{se}\left(\hat{\beta}_j\right)} \sim t_{n-k-1} \quad \text { for } j=0,1, \ldots, k
$$

where $k+1$ is the number of unknown parameters in the population model and $n-k-1$ is the d.o.f.
:::

Observe the differences!



# Single hypothesis testing 

## Single hypothesis testing {.smaller}

Inference on the $j$th population parameter $\beta_j$:

- **Null hypothesis:** $H_0: \beta_j = b$, where $b$ is a hypothesized value typically 0
- Note that this is a test on the $j$th population parameter while holding all other parameters constant
- One-sided alternative: $H_1: \beta_j > b$ or $H_1: \beta_j < b$
- Two-sided alternative: $H_1: \beta_j \neq b$
- set b= 0. Test statistic: $t_{\hat{\beta}_j} = \frac{\hat{\beta}_j - b}{\operatorname{se}(\hat{\beta}_j)} = \frac{\hat{\beta}_j}{\operatorname{se}(\hat{\beta}_j)}$
- We know that the sampling distribution of $t_{\hat{\beta}_j}$  follows... 









## One-sided test {.smaller}

Fix a level of significance $\alpha$ (e.g., 0.05)


:::{.columns}
:::{.column width="50%"}
- **alternative:** $H_1: \beta_j > 0$ (or $H_1: \beta_j < 0)$
  - Reject $H_0$ if $t_{\hat{\beta}_j} > t_{\alpha, n-k-1}^*$
  - p-value = $P(T > t_{\hat{\beta}_j})$ where $T \sim t_{n-k-1}$
  - Confidence interval: $\hat{\beta}_j \pm t_{\alpha, n-k-1}^* \times \operatorname{se}(\hat{\beta}_j)$
- For large samples, t-distribution $\rightarrow$ standard normal
  - the critical value $\approx  \pm 1.65$ for $\alpha = 0.05$
:::
:::{.column width="50%"}
![One-sided test (right)](onesided.png){height=200}

![One-sided test (left)](onesided_left.png){height=200}
:::
:::





## Examples {.smaller}
:::{.columns}
:::{.column width="50%"}
```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(wooldridge)
nrow(meap93) - 4 - 1 # DOF
summary(lm(math10 ~ totcomp + staff + enroll, data=meap93))
```
:::
:::{.column width="50%"}
```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(wooldridge)
nrow(meap93) - 4 - 1 # DOF
summary(lm(math10 ~ log(totcomp) + log(staff) + log(enroll), data=meap93))
```
:::
:::





## Two-sided test {.smaller}

Fix a level of significance $\alpha$ (e.g., 0.05)

:::{.columns}
:::{.column width="50%"}
- **alternative:** $H_1: \beta_j \neq 0$
  - Reject $H_0$ if $|t_{\hat{\beta}_j}| > t_{\alpha/2, n-k-1}^*$
  - p-value = $P(|T| > |t_{\hat{\beta}_j}|)$
  - Confidence interval: $\hat{\beta}_j \pm t_{\alpha/2, n-k-1}^* \times \operatorname{se}(\hat{\beta}_j)$
- For large samples, t-distribution $\rightarrow$ standard normal
  - the critical value $\approx \pm 1.96$ for $\alpha = 0.05$
:::
:::{.column width="50%"}

![Two-sided test](twosided.png)

:::
:::

## Examples {.smaller}

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(wooldridge)
nrow(meap93) - 4 - 1 # DOF
summary(lm(math10 ~ log(totcomp) + log(staff) + log(enroll), data=meap93))
```





## Computing p-values {.smaller}

- p-value is the probability of observing the sample (statistic) if $H_0$ is true
- Indeed you can do this using a table.
- But R can do this for you

```{r, echo=TRUE, message=FALSE, warning=FALSE}
t_value <- 2.5    #  t-value
df <- 100         #  degrees of freedom

# Two-tailed p-value
p_value <- 2 * (1 - pt(abs(t_value), df))
print(p_value)

# One-tailed p-value (right tail)
p_right <- 1 - pt(t_value, df)
print(p_right)

# One-tailed p-value (left tail)
p_left <- pt(t_value, df)
print(p_left)
```

<!-- ## Computing Confidence Intervals {.smaller}

- Confidence interval is a range of values where we are $1-\alpha$ confident that the true population parameter lies within
- Example: -->

## Testing Linear combinations {.smaller}

$$log(wage_i) = \beta_0 + \beta_1 jc_i + \beta_2 univ_i + \beta_3 exper_i + u_i$$ 

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(wooldridge)
summary(lm(lwage ~ jc + univ + exper, data = twoyear))
```
- Does attending $jc_i$ has lower effect on wages as going to university $univ_i$?
- What are we testing?

## {.smaller}
$$log(wage_i) = \beta_0 + \beta_1 jc_i + \beta_2 univ_i + \beta_3 exper_i + u_i$$ $H_0: \beta_1 = \beta_2$ and $H_1: \beta_1 < \beta_2$



<!-- - $H_0: \beta_1 - \beta_2 = 0$ and $H_1: \beta_1 - \beta_2 < 0$ -->
:::{.incremental}
- Call $\beta_1 - \beta_2 = \theta$ and so $H_0: \theta = 0$ and $H_1: \theta < 0$
- Test statistic: $t_{\hat{\theta}} = \frac{\hat{\theta}}{\operatorname{se}(\hat{\theta})} = \frac{\hat{\beta}_1 - \hat{\beta}_2}{\operatorname{se}(\hat{\beta}_1-\hat{\beta}_2)} = \frac{\hat{\beta}_1 - \hat{\beta}_2}{\sqrt{\operatorname{se}(\hat{\beta}_1)^2 + \operatorname{se}(\hat{\beta}_2)^2 + 2\widehat{\operatorname{cov}(\hat{\beta}_1, \hat{\beta}_2)}}}$
<!-- - Based on d.o.f. we can look up the critical value $t_{\alpha, n-k-1}^*$  -->
  - Reject $H_0$ 
    - if $t_{\hat{\theta}} < t_{\alpha, n-k-1}^*$
    - Or if p-value = $P(T < t_{\hat{\theta}})$ where $T \sim t_{n-k-1}$ is less than $\alpha$
    - Or if CI: $[\hat{\theta} \pm t_{\alpha, n-k-1}^* \times \operatorname{se}(\hat{\theta})]$ does not contain zero
- Easy to calculate  $t_{\hat{\theta}}$ if $\widehat{\operatorname{cov}(\hat{\beta}_1, \hat{\beta}_2)}$ is known 
:::

## Implementation {.smaller}


:::{.incremental}
- $$log(wage_i) = \beta_0 + \beta_1 jc_i + \beta_2 univ_i + \beta_3 exper_i + u_i$$ <!--$H_0: \beta_1 = \beta_2$ and $H_1: \beta_1 < \beta_2$ -->
- $$log(wage_i) = \beta_0 + (\theta + \beta_2) jc_i + \beta_2 univ_i + \beta_3 exper_i + u_i$$
- $$log(wage_i) = \beta_0 + \theta jc_i + \beta_2 (jc_i + univ_i) + \beta_3 exper_i + u_i$$
:::

## R code {.smaller}
$$log(wage_i) = \beta_0 + \theta jc_i + \beta_2 (jc_i + univ_i) + \beta_3 exper_i + u_i$$
```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(wooldridge)
summary(lm(lwage ~ jc + I(jc + univ) + exper, data = twoyear))
```








# Testing Multiple Linear Restrictions 


## Testing multiple restrictions {.smaller}

- The $t$-test is used to test a hypothesis about a single population parameter $\beta_j$
- The $F$-test is used to test multiple hypotheses about multiple population parameters *jointly*
- Example: $\log (\text { salary }_i )=  \beta_0+\beta_1 \text { years }_i +\beta_2 \text { gamesyr }_i +\beta_3 \text { bavg }_i  +\beta_4 \text { hrunsyr }_i +\beta_5 \text { rbisyr }_i +u_i$
  - $\mathrm{H}_0: \beta_3=0, \beta_4=0, \beta_5=0$
  - $\mathrm{H}_1:$ $H_0$ is not true: At least one of $\beta_3, \beta_4, \beta_5$ is not zero


## {.smaller}          
```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(wooldridge)
summary(lm(log(salary) ~ years + gamesyr + bavg + hrunsyr + rbisyr, data = mlb1))
```
- Based on what we have learned so far, what do we think about $\mathrm{H}_0: \beta_3=0, \beta_4=0, \beta_5=0$?

<!-- ## Economic vs Statistical Significance {.smaller} -->

## Let us run both models {.smaller}

:::{.columns}
:::{.column width="50%"}
#### Unrestricted model
```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(wooldridge)
unrestricted <-  lm(log(salary) ~ years + gamesyr + bavg + hrunsyr + rbisyr, data = mlb1)
summary(unrestricted)
sum(unrestricted$residuals^2)
```
:::
:::{.column width="50%"}
#### Restricted model
```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(wooldridge)
restricted <-  lm(log(salary) ~ years + gamesyr, data = mlb1)
summary(restricted)
sum(restricted$residuals^2)
```
:::
:::

- Is the increase in SSR large enough to reject the null hypothesis?
- But what is the test statistic that we can use?

## The F-test {.smaller}

:::{.columns}
:::{.column width="50%"}
:::{.incremental}
- $y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \ldots + \beta_k x_{ki} + u_i$
<!-- - $y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \ldots + \beta_{k-q-1} x_{k-q-1} + \beta_{k-q} x_{k-q} + \beta_{k-q+1} x_{k-q+1} + \ldots + \beta_k x_{ki} + u_i$ -->
- Hypotheses Test of $q$ *exclusion restrictions*
  - $H_0: \beta_{k-q} = \beta_{k-q+1} = \ldots = \beta_k = 0$ 
  - $H_1:$ At least one of $\beta_{k-q}, \beta_{k-q+1}, \ldots, \beta_k$ is not zero
- Test statistic: $F = \frac{(SSR_r - SSR_{ur})/q}{SSR_{ur}/(n-k-1)}$
  <!-- - $SSR_r$ and $SSR_{ur}$ are the sum of squared residuals from the restricted and unrestricted models
  - $q$ is the number of restrictions
  - $n$ is the number of observations and $k$ is the number of parameters in the unrestricted model -->
  - Follows an $F$-distribution with $q$ and $n-k-1$ d.o.f.
  - Note:
    -  $q = dof_r - dof_{ur}$
    -  F > 0
:::
:::
:::{.column width="50%"}
![Example F sampling distribution](f_crit.png)
:::
:::

## Going back to the example {.smaller}

 $\log(salary_i)=  \beta_0+\beta_1 years_i +\beta_2 gamesyr_i +\beta_3 bavg_i  +\beta_4 hrunsyr_i +\beta_5 rbisyr_i +u_i$

- $\mathrm{H}_0: \beta_3=0, \beta_4=0, \beta_5=0$
- $\mathrm{H}_1:$ $H_0$ is not true.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(wooldridge)
unrestricted <-  lm(log(salary) ~ years + gamesyr + bavg + hrunsyr + rbisyr, data = mlb1)
restricted <-  lm(log(salary) ~ years + gamesyr, data = mlb1)
F_stat <- ((sum(restricted$residuals^2) - sum(unrestricted$residuals^2))/3)/(sum(unrestricted$residuals^2)/(nrow(mlb1) - 5-1))
print(F_stat)

critical_F <- qf(0.05, df1 = 3, df2 = nrow(mlb1) - 5-1, lower.tail = F)
print(critical_F)

if (F_stat > critical_F) {
  print("Reject H0")
} else {
  print("Fail to reject H0")
}
```

## {.smaller}
 $\log(salary_i)=  \beta_0+\beta_1 years_i +\beta_2 gamesyr_i +\beta_3 bavg_i  +\beta_4 hrunsyr_i +\beta_5 rbisyr_i +u_i$


:::{.incremental}
- But recall when we ran the unrestricted model, we found that $\beta_3, \beta_4, \beta_5$ were not statistically significant.
- So, whats going on here with the joint F-test?
- $hrunsyr$ and $rbisyr$ are highly correlated. 
  -  multicollinearity $\rightarrow$ large standard errors $\rightarrow$  low t-stats  $\rightarrow$  individual statistical insignificance
- The F-test is a joint test (including bavg) and is not affected by multicollinearity 
- Hence F-tests of joint hypotheses can be useful in the presence of multicollinearity
:::

## Relationship between t and F tests {.smaller}

- For a single restriction $q=1$, the F-test is equivalent to the t-test
  - $F_{1,n-k-1} = t^2_{n-k-1}$
- For single hypothesis testing, the t-test is more powerful
  -  F-tests remain under-powered than t-tests (See Math refresher C)



## Reporting Regression Results {.smaller}

- **Report Estimated Coefficients**  
  - Interpret key variables' estimates in economic or practical terms.

- **Include Standard Errors**  
  - Preferred over just $t$-statistics as they help interpret hypothesis tests and confidence intervals.

- **Report $R^2$ and Other Fit Statistics**  
  - $R^2$ is essential for goodness-of-fit.  
  - Reporting F-statistics helps test exclusion restrictions.

- **Summarize in Tables for Multiple Models**  
  - If multiple equations are estimated, use tables instead of inline equations.  
  - Dependent variable should be clearly indicated.  
  - Independent variables should be listed in the first column.  
  - Standard errors in parentheses below estimates.


## Salary and Benefits Tradeoff example {.smaller}

:::{.incremental}
- **Total compensation** ($totcomp$) consists of salary and benefits:$$totcomp = salary + benefits = salary \left( 1 + \frac{benefits}{salary} \right)$$
- Taking the **log transformation**: $\log(totcomp) = \log(salary) + \log(1 + b/s).$
- For **small** $b/s$, can approximate $\log(1 + b/s) \approx b/s.$
- This leads to the econometric model:$$\log(salary) = \beta_0 + \beta_1 (b/s) + \text{other *controls*}. $$
- **Hypothesis Test**: Testing the **salary-benefits tradeoff**:
  - $H_0: \beta_1 = -1$ (full tradeoff)
  - $H_1: \beta_1 \neq -1$ (partial or no tradeoff)
- Data from **MEAP93** controls for enrollment, staff size, dropout, and graduation rates.
:::

## {.smaller}


```{r, echo=TRUE, message=FALSE, warning=FALSE}

library(stargazer, wooldridge)
model1 <- lm(log(salary) ~ I(benefits/salary), data = meap93)
model2 <- lm(log(salary) ~ I(benefits/salary) + I(log(enroll)) + I(log(staff)), data = meap93)
model3 <- lm(log(salary) ~ I(benefits/salary) + I(log(enroll)) + I(log(staff)) + droprate + gradrate, data = meap93)

stargazer(model1, model2, model3, 
          type = "text", 
          title = "Regression Results",
          omit.stat = c("ser"), # Omit some stats if needed
          dep.var.labels = "Log(salary)",
          column.labels = c("Model 1", "Model 2", "Model 3"),
          covariate.labels = c("benefits to salary ratio", "log of enrollment", "log of staff", "Drop out rate", "Graduation rate", "Intercept"))


```


## `modelsummary` package   {.smaller}

```{r, echo=T, message=FALSE, warning=FALSE}

library(modelsummary)
models <- list(
  "Model 1" = model1,
  "Model 2" = model2,
  "Model 3" = model3
)
modelsummary(models, stars = TRUE, output = "markdown")
```

<!-- ## kableExtra package   {.smaller}

```{r, echo=T, message=FALSE, warning=FALSE}
library(kableExtra)
summary_table <- modelsummary(models, output = "data.frame")
kable(summary_table, format = "html") %>%
  kable_styling(full_width = FALSE)

``` -->